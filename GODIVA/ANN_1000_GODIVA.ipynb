{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Volumes/data/LosAlamosSummer')\n",
    "sys.path.insert(0, '/Volumes/data/LosAlamosSummer/DrOsborneCode')\n",
    "import src.Utilities as ut\n",
    "import importlib\n",
    "import src.models as mod\n",
    "import src.callbacks as cus\n",
    "importlib.reload(ut)\n",
    "importlib.reload(mod)\n",
    "importlib.reload(cus)\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+02 1.e+03 1.e+04 1.e+05 1.e+06 1.e+07]\n"
     ]
    }
   ],
   "source": [
    "print(np.logspace(2, 7, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading In Data\n",
      "Finished Loading Data\n"
     ]
    }
   ],
   "source": [
    "#datapath = '/Users/jessiejo/data/VBUDS/GroupStructurePaper/NeuralNetworks/All_Libraries/NewDataSetFull1.mat'\n",
    "datapath='/Volumes/data/LosAlamosSummer/GODIVA/GODIVA_data_0_12.mat'\n",
    "print('Loading In Data')\n",
    "kinf,GS=ut.LoadData(datapath, 0)\n",
    "#MakeGroupDensity(X, nDecades)\n",
    "Nfeatures = 1000;\n",
    "\n",
    "\n",
    "allData= ut.ProcessData(datapath, 1,Nfeatures,1,np.logspace(2, 7, Nfeatures),0)\n",
    "# allData: (100,000x1,000) y_direct: (100,000x3)\n",
    "print('Finished Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load Data loads raw data from the .mat file\n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]\n",
    "\n",
    "ProcessData is the serialization maker \n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "Percent of data to be used: in most cases full data set will be used but good for analysis [double](0-1)\n",
    "ndecades: Number of decades wanted in equal lethargy serialization. Number is ignored if custom serialization inputted [int]\n",
    "mode: equal lethargy mode (0) or custom serialization mode (1) [boolean]\n",
    "input serial: a custom serialization regime (ignored if mode is not 1) [numpy array]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(50000, 999)\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(kinf.shape)\n",
    "print(allData.shape)\n",
    "print(sum(allData[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999,)\n",
      "(4999,)\n",
      "(50000,)\n",
      "(35000,)\n"
     ]
    }
   ],
   "source": [
    "Nsamples,Ndecades = allData.shape\n",
    "vldF=.1\n",
    "testF=.2\n",
    "normConst=1#np.linalg.norm(kinf)\n",
    "y_norm=np.array(kinf/normConst)\n",
    "\n",
    "X, X_test, y, y_test, vldF_corr = ut.makeFractions(Nsamples, vldF, testF, allData, y_norm, 0)\n",
    "\n",
    "\n",
    "NtrainingSamples = int(Nsamples*(1 - testF))\n",
    "tranValSplit=int(NtrainingSamples*(1-vldF_corr))\n",
    "X_train=X[:tranValSplit,:]\n",
    "y_train=y[:tranValSplit]\n",
    "X_val=X[tranValSplit+1:,:]\n",
    "y_val=y[tranValSplit+1:]\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_norm.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makeFractions splits the data into appropriatly sized sets\n",
    "Nsamples is the number of samples of the data set\n",
    "vldF is the validation fraction\n",
    "testF is the test fraction\n",
    "allData is the set of serialzed group structures\n",
    "y_norm is the kinfs that correspond to the serialized group structures (normalized or otherwise)\n",
    "BU (the last input) is a boolean determining whether the data contains burnup [Boolean] (used in the same manner as previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 832)               832000    \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 554)               461482    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 555       \n",
      "=================================================================\n",
      "Total params: 1,294,037\n",
      "Trainable params: 1,294,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=531\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(832, activation='relu', name='hidden_1', input_dim=999),\n",
    "    layers.Dense(554, activation='relu',  name='hidden_2'),\n",
    "    layers.Dense(1, activation='linear',name='output')])\n",
    "model.compile(loss=\"MAE\",metrics=\"MAE\")\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.1644 - val_loss: 0.0744\n",
      "Epoch 2/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0570 - val_loss: 0.0517\n",
      "Epoch 3/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0417 - val_loss: 0.0450\n",
      "Epoch 4/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0335 - val_loss: 0.0415\n",
      "Epoch 5/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0284 - val_loss: 0.0374\n",
      "Epoch 6/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0262 - val_loss: 0.0367\n",
      "Epoch 7/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0235 - val_loss: 0.0330\n",
      "Epoch 8/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0213 - val_loss: 0.0303\n",
      "Epoch 9/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0189 - val_loss: 0.0283\n",
      "Epoch 10/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0167 - val_loss: 0.0268\n",
      "Epoch 11/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0145 - val_loss: 0.0237\n",
      "Epoch 12/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0132 - val_loss: 0.0225\n",
      "Epoch 13/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0128 - val_loss: 0.0228\n",
      "Epoch 14/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0108 - val_loss: 0.0200\n",
      "Epoch 15/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0098 - val_loss: 0.0214\n",
      "Epoch 16/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0097 - val_loss: 0.0187\n",
      "Epoch 17/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0088 - val_loss: 0.0180\n",
      "Epoch 18/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0084 - val_loss: 0.0189\n",
      "Epoch 19/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0080 - val_loss: 0.0174\n",
      "Epoch 20/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0075 - val_loss: 0.0170\n",
      "Epoch 21/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0077 - val_loss: 0.0170\n",
      "Epoch 22/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0070 - val_loss: 0.0171\n",
      "Epoch 23/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0070 - val_loss: 0.0168\n",
      "Epoch 24/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0068 - val_loss: 0.0160\n",
      "Epoch 25/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0066 - val_loss: 0.0158\n",
      "Epoch 26/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0065 - val_loss: 0.0157\n",
      "Epoch 27/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0064 - val_loss: 0.0155\n",
      "Epoch 28/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0060 - val_loss: 0.0152\n",
      "Epoch 29/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0063 - val_loss: 0.0150\n",
      "Epoch 30/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0057 - val_loss: 0.0150\n",
      "Epoch 31/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0055 - val_loss: 0.0150\n",
      "Epoch 32/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0057 - val_loss: 0.0152\n",
      "Epoch 33/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0057 - val_loss: 0.0147\n",
      "Epoch 34/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0058 - val_loss: 0.0144\n",
      "Epoch 35/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0055 - val_loss: 0.0149\n",
      "Epoch 36/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0053 - val_loss: 0.0140\n",
      "Epoch 37/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0052 - val_loss: 0.0139\n",
      "Epoch 38/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0060 - val_loss: 0.0145\n",
      "Epoch 39/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0053 - val_loss: 0.0137\n",
      "Epoch 40/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0052 - val_loss: 0.0136\n",
      "Epoch 41/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0050 - val_loss: 0.0134\n",
      "Epoch 42/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0050 - val_loss: 0.0133\n",
      "Epoch 43/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0050 - val_loss: 0.0132\n",
      "Epoch 44/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0049 - val_loss: 0.0133\n",
      "Epoch 45/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0051 - val_loss: 0.0131\n",
      "Epoch 46/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0053 - val_loss: 0.0128\n",
      "Epoch 47/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0048 - val_loss: 0.0127\n",
      "Epoch 48/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0049 - val_loss: 0.0125\n",
      "Epoch 49/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0046 - val_loss: 0.0127\n",
      "Epoch 50/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0047 - val_loss: 0.0123\n",
      "Epoch 51/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 52/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0046 - val_loss: 0.0124\n",
      "Epoch 53/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0048 - val_loss: 0.0131\n",
      "Epoch 54/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0047 - val_loss: 0.0119\n",
      "Epoch 55/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0044 - val_loss: 0.0121\n",
      "Epoch 56/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0045 - val_loss: 0.0123\n",
      "Epoch 57/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0046 - val_loss: 0.0116\n",
      "Epoch 58/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0044 - val_loss: 0.0116\n",
      "Epoch 59/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0045 - val_loss: 0.0116\n",
      "Epoch 60/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0044 - val_loss: 0.0116\n",
      "Epoch 61/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0043 - val_loss: 0.0114\n",
      "Epoch 62/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0042 - val_loss: 0.0112\n",
      "Epoch 63/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0041 - val_loss: 0.0114\n",
      "Epoch 64/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0044 - val_loss: 0.0112\n",
      "Epoch 65/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0045 - val_loss: 0.0112\n",
      "Epoch 66/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0041 - val_loss: 0.0113\n",
      "Epoch 67/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0045 - val_loss: 0.0118\n",
      "Epoch 68/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0044 - val_loss: 0.0111\n",
      "Epoch 69/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0042 - val_loss: 0.0109\n",
      "Epoch 70/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0049 - val_loss: 0.0108\n",
      "Epoch 71/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0041 - val_loss: 0.0109\n",
      "Epoch 72/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0042 - val_loss: 0.0105\n",
      "Epoch 73/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0043 - val_loss: 0.0106\n",
      "Epoch 74/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0039 - val_loss: 0.0105\n",
      "Epoch 75/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0038 - val_loss: 0.0103\n",
      "Epoch 76/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0040 - val_loss: 0.0114\n",
      "Epoch 77/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0038 - val_loss: 0.0100\n",
      "Epoch 78/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0038 - val_loss: 0.0102\n",
      "Epoch 79/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0040 - val_loss: 0.0108\n",
      "Epoch 80/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0041 - val_loss: 0.0104\n",
      "Epoch 81/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0040 - val_loss: 0.0101\n",
      "Epoch 82/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0038 - val_loss: 0.0100\n",
      "Epoch 83/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0041 - val_loss: 0.0100\n",
      "Epoch 84/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0039 - val_loss: 0.0101\n",
      "Epoch 85/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0039 - val_loss: 0.0099\n",
      "Epoch 86/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0039 - val_loss: 0.0100\n",
      "Epoch 87/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0038 - val_loss: 0.0098\n",
      "Epoch 88/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0037 - val_loss: 0.0099\n",
      "Epoch 89/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0038 - val_loss: 0.0102\n",
      "Epoch 90/800\n",
      "66/66 [==============================] - 1s 17ms/step - loss: 0.0035 - val_loss: 0.0101\n",
      "Epoch 91/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0036 - val_loss: 0.0098\n",
      "Epoch 92/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0036 - val_loss: 0.0098\n",
      "Epoch 93/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0049 - val_loss: 0.0097\n",
      "Epoch 94/800\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.0038 - val_loss: 0.0098\n",
      "Epoch 95/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0036 - val_loss: 0.0098\n",
      "Epoch 96/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0035 - val_loss: 0.0098\n",
      "Epoch 97/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0037 - val_loss: 0.0096\n",
      "Epoch 98/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0036 - val_loss: 0.0096\n",
      "Epoch 99/800\n",
      "66/66 [==============================] - 1s 19ms/step - loss: 0.0039 - val_loss: 0.0097\n",
      "Epoch 100/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0037 - val_loss: 0.0098\n",
      "Epoch 101/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0049 - val_loss: 0.0108\n",
      "Epoch 102/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0041 - val_loss: 0.0093\n",
      "Epoch 103/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0036 - val_loss: 0.0095\n",
      "Epoch 104/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0035 - val_loss: 0.0095\n",
      "Epoch 105/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0034 - val_loss: 0.0093\n",
      "Epoch 106/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0035 - val_loss: 0.0094\n",
      "Epoch 107/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0035 - val_loss: 0.0092\n",
      "Epoch 108/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0033 - val_loss: 0.0094\n",
      "Epoch 109/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0035 - val_loss: 0.0091\n",
      "Epoch 110/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0032 - val_loss: 0.0092\n",
      "Epoch 111/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0032 - val_loss: 0.0093\n",
      "Epoch 112/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 113/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0031 - val_loss: 0.0093\n",
      "Epoch 114/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0034 - val_loss: 0.0093\n",
      "Epoch 115/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0032 - val_loss: 0.0091\n",
      "Epoch 116/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0031 - val_loss: 0.0093\n",
      "Epoch 117/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0042 - val_loss: 0.0092\n",
      "Epoch 118/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0032 - val_loss: 0.0089\n",
      "Epoch 119/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0033 - val_loss: 0.0089\n",
      "Epoch 120/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0032 - val_loss: 0.0090\n",
      "Epoch 121/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0031 - val_loss: 0.0089\n",
      "Epoch 122/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0033 - val_loss: 0.0090\n",
      "Epoch 123/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0031 - val_loss: 0.0090\n",
      "Epoch 124/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0030 - val_loss: 0.0087\n",
      "Epoch 125/800\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 0.0032 - val_loss: 0.0093\n",
      "Epoch 126/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0031 - val_loss: 0.0089\n",
      "Epoch 127/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0035 - val_loss: 0.0088\n",
      "Epoch 128/800\n",
      "66/66 [==============================] - 2s 26ms/step - loss: 0.0030 - val_loss: 0.0088\n",
      "Epoch 129/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0030 - val_loss: 0.0088\n",
      "Epoch 130/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0032 - val_loss: 0.0087\n",
      "Epoch 131/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0040 - val_loss: 0.0098\n",
      "Epoch 132/800\n",
      "66/66 [==============================] - 2s 26ms/step - loss: 0.0037 - val_loss: 0.0088\n",
      "Epoch 133/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0031 - val_loss: 0.0086\n",
      "Epoch 134/800\n",
      "66/66 [==============================] - 2s 27ms/step - loss: 0.0030 - val_loss: 0.0086\n",
      "Epoch 135/800\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 136/800\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.0031 - val_loss: 0.0088\n",
      "Epoch 137/800\n",
      "66/66 [==============================] - 2s 27ms/step - loss: 0.0030 - val_loss: 0.0089\n",
      "Epoch 138/800\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.0030 - val_loss: 0.0085\n",
      "Epoch 139/800\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.0030 - val_loss: 0.0086\n",
      "Epoch 140/800\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 141/800\n",
      "66/66 [==============================] - 2s 31ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 142/800\n",
      "66/66 [==============================] - 2s 31ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 143/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 144/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0029 - val_loss: 0.0084\n",
      "Epoch 145/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 146/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0030 - val_loss: 0.0085\n",
      "Epoch 147/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0028 - val_loss: 0.0084\n",
      "Epoch 148/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 149/800\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 0.0027 - val_loss: 0.0083\n",
      "Epoch 150/800\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.0030 - val_loss: 0.0084\n",
      "Epoch 151/800\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 152/800\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.0029 - val_loss: 0.0083\n",
      "Epoch 153/800\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.0028 - val_loss: 0.0086\n",
      "Epoch 154/800\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 155/800\n",
      "66/66 [==============================] - 2s 34ms/step - loss: 0.0031 - val_loss: 0.0085\n",
      "Epoch 156/800\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.0029 - val_loss: 0.0083\n",
      "Epoch 157/800\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 158/800\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.0027 - val_loss: 0.0086\n",
      "Epoch 159/800\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.0029 - val_loss: 0.0084\n",
      "Epoch 160/800\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.0028 - val_loss: 0.0082\n",
      "Epoch 161/800\n",
      "66/66 [==============================] - 2s 37ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 162/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 2s 36ms/step - loss: 0.0027 - val_loss: 0.0083\n",
      "Epoch 163/800\n",
      "66/66 [==============================] - 2s 35ms/step - loss: 0.0030 - val_loss: 0.0084\n",
      "Epoch 164/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0032 - val_loss: 0.0085\n",
      "Epoch 165/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 166/800\n",
      "66/66 [==============================] - 2s 33ms/step - loss: 0.0026 - val_loss: 0.0082\n",
      "Epoch 167/800\n",
      "66/66 [==============================] - 2s 31ms/step - loss: 0.0026 - val_loss: 0.0082\n",
      "Epoch 168/800\n",
      "66/66 [==============================] - 2s 31ms/step - loss: 0.0027 - val_loss: 0.0081\n",
      "Epoch 169/800\n",
      "66/66 [==============================] - 2s 32ms/step - loss: 0.0027 - val_loss: 0.0083\n",
      "Epoch 170/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0027 - val_loss: 0.0082\n",
      "Epoch 171/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0027 - val_loss: 0.0082\n",
      "Epoch 172/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0029 - val_loss: 0.0082\n",
      "Epoch 173/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 174/800\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.0026 - val_loss: 0.0082\n",
      "Epoch 175/800\n",
      "66/66 [==============================] - 2s 30ms/step - loss: 0.0026 - val_loss: 0.0080\n",
      "Epoch 176/800\n",
      "66/66 [==============================] - 2s 29ms/step - loss: 0.0027 - val_loss: 0.0081\n",
      "Epoch 177/800\n",
      "66/66 [==============================] - 2s 28ms/step - loss: 0.0026 - val_loss: 0.0080\n",
      "Epoch 178/800\n",
      "66/66 [==============================] - 2s 27ms/step - loss: 0.0027 - val_loss: 0.0081\n",
      "Epoch 179/800\n",
      "66/66 [==============================] - 2s 26ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 180/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0027 - val_loss: 0.0080\n",
      "Epoch 181/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0028 - val_loss: 0.0081\n",
      "Epoch 182/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 183/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0026 - val_loss: 0.0080\n",
      "Epoch 184/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 185/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 186/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 187/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 188/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0026 - val_loss: 0.0081\n",
      "Epoch 189/800\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 190/800\n",
      "66/66 [==============================] - 2s 25ms/step - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 191/800\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 0.0027 - val_loss: 0.0079\n",
      "Epoch 192/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 193/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 194/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0026 - val_loss: 0.0080\n",
      "Epoch 195/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0034 - val_loss: 0.0089\n",
      "Epoch 196/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0042 - val_loss: 0.0088\n",
      "Epoch 197/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 0.0028 - val_loss: 0.0080\n",
      "Epoch 198/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 199/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 200/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 201/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 202/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 203/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 204/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 205/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 206/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 207/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 208/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "Epoch 209/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0079\n",
      "Epoch 210/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "Epoch 211/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 0.0078\n",
      "Epoch 212/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 213/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0079\n",
      "Epoch 214/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0034 - val_loss: 0.0081\n",
      "Epoch 215/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0032 - val_loss: 0.0079\n",
      "Epoch 216/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "Epoch 217/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 218/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0029 - val_loss: 0.0079\n",
      "Epoch 219/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "Epoch 220/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0078\n",
      "Epoch 221/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 0.0078\n",
      "Epoch 222/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 223/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 224/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 225/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 226/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0022 - val_loss: 0.0079\n",
      "Epoch 227/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 228/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 229/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0027 - val_loss: 0.0078\n",
      "Epoch 230/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0032 - val_loss: 0.0078\n",
      "Epoch 231/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 232/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0023 - val_loss: 0.0078\n",
      "Epoch 233/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 0.0077\n",
      "Epoch 234/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0077\n",
      "Epoch 235/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 236/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0076\n",
      "Epoch 237/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 238/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0027 - val_loss: 0.0079\n",
      "Epoch 239/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0027 - val_loss: 0.0077\n",
      "Epoch 240/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 241/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0026 - val_loss: 0.0077\n",
      "Epoch 242/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0025 - val_loss: 0.0077\n",
      "Epoch 243/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 0.0077\n",
      "Epoch 244/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0078\n",
      "Epoch 245/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0077\n",
      "Epoch 246/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 247/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 248/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 249/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 0.0077\n",
      "Epoch 250/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0026 - val_loss: 0.0077\n",
      "Epoch 251/800\n",
      "64/66 [============================>.] - ETA: 0s - loss: 0.0023Restoring model weights from the end of the best epoch.\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 0.0023 - val_loss: 0.0078\n",
      "Epoch 00251: early stopping\n",
      "Epoch 1/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 0.0019 - val_loss: 0.0076\n",
      "Epoch 2/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.0076\n",
      "Epoch 3/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0013 - val_loss: 0.0076\n",
      "Epoch 4/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0012 - val_loss: 0.0076\n",
      "Epoch 5/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 0.0011 - val_loss: 0.0075\n",
      "Epoch 6/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 9.9112e-04 - val_loss: 0.0075\n",
      "Epoch 7/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 9.2055e-04 - val_loss: 0.0075\n",
      "Epoch 8/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.6484e-04 - val_loss: 0.0076\n",
      "Epoch 9/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 8.3687e-04 - val_loss: 0.0075\n",
      "Epoch 10/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.6572e-04 - val_loss: 0.0076\n",
      "Epoch 11/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.7355e-04 - val_loss: 0.0075\n",
      "Epoch 12/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 7.1518e-04 - val_loss: 0.0076\n",
      "Epoch 13/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.0142e-04 - val_loss: 0.0076\n",
      "Epoch 14/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 6.7866e-04 - val_loss: 0.0076\n",
      "Epoch 15/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 6.5498e-04 - val_loss: 0.0075\n",
      "Epoch 16/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 6.4989e-04 - val_loss: 0.0076\n",
      "Epoch 17/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 6.1728e-04 - val_loss: 0.0076\n",
      "Epoch 18/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 5.9798e-04 - val_loss: 0.0076\n",
      "Epoch 19/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 5.9325e-04 - val_loss: 0.0076\n",
      "Epoch 20/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 5.9001e-04 - val_loss: 0.0076\n",
      "Epoch 21/800\n",
      "64/66 [============================>.] - ETA: 0s - loss: 6.0038e-04Restoring model weights from the end of the best epoch.\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 6.0247e-04 - val_loss: 0.0076\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/800\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 8.8088e-04 - val_loss: 0.0075\n",
      "Epoch 2/800\n",
      "66/66 [==============================] - 1s 20ms/step - loss: 8.5092e-04 - val_loss: 0.0075\n",
      "Epoch 3/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 8.3458e-04 - val_loss: 0.0075\n",
      "Epoch 4/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 8.2137e-04 - val_loss: 0.0075\n",
      "Epoch 5/800\n",
      "66/66 [==============================] - 1s 21ms/step - loss: 8.0930e-04 - val_loss: 0.0075\n",
      "Epoch 6/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.9810e-04 - val_loss: 0.0075\n",
      "Epoch 7/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.8956e-04 - val_loss: 0.0075\n",
      "Epoch 8/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.7813e-04 - val_loss: 0.0075\n",
      "Epoch 9/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.6883e-04 - val_loss: 0.0075\n",
      "Epoch 10/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.5973e-04 - val_loss: 0.0075\n",
      "Epoch 11/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.5165e-04 - val_loss: 0.0075\n",
      "Epoch 12/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.4363e-04 - val_loss: 0.0075\n",
      "Epoch 13/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.3555e-04 - val_loss: 0.0075\n",
      "Epoch 14/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.2698e-04 - val_loss: 0.0075\n",
      "Epoch 15/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 7.2648e-04 - val_loss: 0.0075\n",
      "Epoch 16/800\n",
      "64/66 [============================>.] - ETA: 0s - loss: 7.1710e-04Restoring model weights from the end of the best epoch.\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 7.1566e-04 - val_loss: 0.0075\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 8.4860e-04 - val_loss: 0.0075\n",
      "Epoch 2/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.4454e-04 - val_loss: 0.0075\n",
      "Epoch 3/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 8.4209e-04 - val_loss: 0.0075\n",
      "Epoch 4/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.3996e-04 - val_loss: 0.0075\n",
      "Epoch 5/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 8.3797e-04 - val_loss: 0.0075\n",
      "Epoch 6/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.3617e-04 - val_loss: 0.0075\n",
      "Epoch 7/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.3379e-04 - val_loss: 0.0075\n",
      "Epoch 8/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.3176e-04 - val_loss: 0.0075\n",
      "Epoch 9/800\n",
      "66/66 [==============================] - 2s 26ms/step - loss: 8.3028e-04 - val_loss: 0.0075\n",
      "Epoch 10/800\n",
      "66/66 [==============================] - 1s 22ms/step - loss: 8.2812e-04 - val_loss: 0.0075\n",
      "Epoch 11/800\n",
      "66/66 [==============================] - 1s 23ms/step - loss: 8.2654e-04 - val_loss: 0.0075\n",
      "Epoch 12/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 8.2448e-04 - val_loss: 0.0075\n",
      "Epoch 13/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 8.2280e-04 - val_loss: 0.0075\n",
      "Epoch 14/800\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 8.2170e-04 - val_loss: 0.0075\n",
      "Epoch 15/800\n",
      "66/66 [==============================] - 2s 24ms/step - loss: 8.2005e-04 - val_loss: 0.0075\n",
      "Epoch 16/800\n",
      "64/66 [============================>.] - ETA: 0s - loss: 8.1891e-04Restoring model weights from the end of the best epoch.\n",
      "66/66 [==============================] - 2s 23ms/step - loss: 8.1813e-04 - val_loss: 0.0075\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x188b86730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-4))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-5))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-6))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00020991769\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "metric = tf.keras.metrics.MeanSquaredError(name=\"mean_average_error\", dtype=None)\n",
    "metric.update_state(np.array(y_predicted*normConst),np.array(y_test.T*normConst))\n",
    "print(metric.result().numpy())  \n",
    "#print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999,)\n",
      "(9999,)\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted[:,0].shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating dataset\n",
    "a = (y_predicted[:,0]-y_test)\n",
    "print(a.shape)\n",
    "# Creating histogram\n",
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "ax.hist(a,bins=500,label=\"BOL\",color='g')\n",
    "plt.xlabel(\"$Δk_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.title(\"GODIVA $Δk_{\\infty}$, high bias serialization\")\n",
    "#plt.legend(loc='upper right')\n",
    "plt.xlim([-0.5,0.5])\n",
    "plt.ylim([0,1000])\n",
    "\n",
    "plt.savefig(\"../All_Results/GODIVA/GODIVA_deltaK_eqleth12.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06737911895538316\n",
      "0.01440077788766054\n",
      "1.0796179340080996\n",
      "1.0780256\n",
      "9999\n",
      "9999\n",
      "9999\n",
      "[[1.0710818]\n",
      " [1.0409943]\n",
      " [1.0792836]\n",
      " ...\n",
      " [1.0507499]\n",
      " [1.0440333]\n",
      " [1.0840304]]\n"
     ]
    }
   ],
   "source": [
    "print(np.std(y_test.T))\n",
    "print(np.std(a))\n",
    "print(np.mean(y_test.T))\n",
    "print(np.mean(y_predicted))\n",
    "print(len(y_test))\n",
    "print(len(y_predicted))\n",
    "print(len(a))\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    " \n",
    "\n",
    "n_bins = 500\n",
    " \n",
    "\n",
    "# Creating histogram\n",
    "fig, axs = plt.subplots(figsize =(10, 7))\n",
    " \n",
    "axs.hist(y_test.T, bins = n_bins,label=\"k-eigenvalues (ground truth))\")\n",
    "axs.hist(y_predicted, bins = n_bins,label=\"k-eigenvalues predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.title(\"GODIVA $k_{\\infty}$, high bias serialization \")\n",
    "plt.savefig(\"../All_Results/GODIVA/kinfDist_eqleth_12.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "\n",
    "plt.show()\n",
    "# Show plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.540174245834351"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_test*normConst)-np.min(y_test*normConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.02492344379425"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y_test*normConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.565097689628601"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_test*normConst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3279"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.logical_and(kinf > 1.03146,kinf < 1.03546))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(kinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0780256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.05515"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5231909"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16c19f704522b294860ea2fcfcbccb05047b74a4c0e7a99ceb11e0c992f8aef7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tensorflow-groupstruct')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
