{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "1.0.2\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy==1.19.5\n",
    "import sys\n",
    "sys.path.insert(0, '/Volumes/data/LosAlamosSummer')\n",
    "import Utilities\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import kerastuner\n",
    "print(tf.__version__)\n",
    "print(kerastuner.__version__)\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "    #    print(\n",
    "    #        \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "    #    )\n",
    "        return\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "     #   print(\n",
    "     #       \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "     #   )\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and MSE is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_squared_logarithmic_error\"]#logs[\"mean_squared_logarithmic_error\"]\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading In Data\n",
      "Finished Loading Data\n"
     ]
    }
   ],
   "source": [
    "#datapath = '/Users/jessiejo/data/VBUDS/GroupStructurePaper/NeuralNetworks/All_Libraries/NewDataSetFull1.mat'\n",
    "datapath='/Volumes/data/LosAlamosSummer/SFR/DATA/SFR_data_8.mat'\n",
    "print('Loading In Data')\n",
    "kinfBOL,kinfMOL,kinfEOL,GS=Utilities.LoadData(datapath,1)\n",
    "#MakeGroupDensity(X, nDecades)\n",
    "Nfeatures = 1000;\n",
    "allData= Utilities.ProcessData(datapath, 1,1000,0,0,1)\n",
    "# allData: (100,000x1,000) y_direct: (100,000x3)\n",
    "print('Finished Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data loads raw data from the .mat file\n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]\n",
    "\n",
    "ProcessData is the serialization maker \n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "Percent of data to be used: in most cases full data set will be used but good for analysis [double](0-1)\n",
    "ndecades: Number of decades wanted in equal lethargy serialization. Number is ignored if custom serialization inputted [int]\n",
    "mode: equal lethargy mode (0) or custom serialization mode (1) [boolean]\n",
    "input serial: a custom serialization regime (ignored if mode is not 1) [numpy array]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "50000\n",
      "(50000, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install -q -U keras-tuner\n",
    "print(kinfBOL.shape)\n",
    "print(len(kinfBOL))\n",
    "kinf=np.array(np.zeros((len(kinfBOL),3)))\n",
    "kinf[:,0]=kinfBOL\n",
    "kinf[:,1]=kinfMOL\n",
    "kinf[:,2]=kinfEOL#np.concatenate((kinfBOL,kinfMOL,kinfEOL),axis=0)\n",
    "print(kinf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9999)\n",
      "(3, 4999)\n",
      "(50000, 3)\n",
      "(3, 35000)\n"
     ]
    }
   ],
   "source": [
    "Nsamples,Ndecades = allData.shape\n",
    "vldF=.1\n",
    "testF=.2\n",
    "normConst=1#np.linalg.norm(kinf)\n",
    "y_norm=np.array(kinf/normConst)\n",
    "\n",
    "X, X_test, y, y_test, vldF_corr = Utilities.makeFractions(Nsamples, vldF, testF, allData, y_norm, 1)\n",
    "\n",
    "\n",
    "NtrainingSamples = int(Nsamples*(1 - testF))\n",
    "tranValSplit=int(NtrainingSamples*(1-vldF_corr))\n",
    "X_train=X[:tranValSplit,:]\n",
    "y_train=y[:,:tranValSplit]\n",
    "X_val=X[tranValSplit+1:,:]\n",
    "y_val=y[:,tranValSplit+1:]\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_norm.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makeFractions splits the data into appropriatly sized sets\n",
    "Nsamples is the number of samples of the data set\n",
    "vldF is the validation fraction\n",
    "testF is the test fraction\n",
    "allData is the set of serialzed group structures\n",
    "y_norm is the kinfs that correspond to the serialized group structures (normalized or otherwise)\n",
    "BU (the last input) is a boolean determining whether the data contains burnup [Boolean] (used in the same manner as previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (200, 0.0001),\n",
    "    (400, 0.00001),\n",
    "    (500, 0.000001),\n",
    "    (600,0.0000001),\n",
    "]\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 70)                70000     \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 717)               50907     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 2154      \n",
      "=================================================================\n",
      "Total params: 123,061\n",
      "Trainable params: 123,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keff_model = tf.keras.Sequential([\n",
    "    layers.Dense(70, activation='relu', name='hidden_1', input_dim=999),\n",
    "    layers.Dense(717, activation='relu',  name='hidden_2'),\n",
    "    layers.Dense(3, activation='linear',name='output')])\n",
    "keff_model.compile(loss=\"mean_squared_logarithmic_error\",optimizer=tf.keras.optimizers.Adam(1e-03),metrics=\"mean_squared_logarithmic_error\")\n",
    "keff_model.build()\n",
    "keff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.0010.\n",
      "Epoch 1/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 0.0290 - mean_squared_logarithmic_error: 0.0290The average loss for epoch 0 is    0.03 and MSE is    0.03.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 0.0275 - mean_squared_logarithmic_error: 0.0275 - val_loss: 8.9408e-04 - val_mean_squared_logarithmic_error: 8.9408e-04\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0010.\n",
      "Epoch 2/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 7.5214e-04 - mean_squared_logarithmic_error: 7.5214e-04The average loss for epoch 1 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.5077e-04 - mean_squared_logarithmic_error: 7.5077e-04 - val_loss: 7.7732e-04 - val_mean_squared_logarithmic_error: 7.7732e-04\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0010.\n",
      "Epoch 3/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 6.3965e-04 - mean_squared_logarithmic_error: 6.3965e-04The average loss for epoch 2 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.3932e-04 - mean_squared_logarithmic_error: 6.3932e-04 - val_loss: 7.1438e-04 - val_mean_squared_logarithmic_error: 7.1438e-04\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0010.\n",
      "Epoch 4/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 5.7870e-04 - mean_squared_logarithmic_error: 5.7870e-04The average loss for epoch 3 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.7892e-04 - mean_squared_logarithmic_error: 5.7892e-04 - val_loss: 6.8024e-04 - val_mean_squared_logarithmic_error: 6.8024e-04\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0010.\n",
      "Epoch 5/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 5.3477e-04 - mean_squared_logarithmic_error: 5.3477e-04 ETA: 0s - loss: 5.2837e-04 - mean_squared_logarithmic_error: 5.The average loss for epoch 4 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.3590e-04 - mean_squared_logarithmic_error: 5.3590e-04 - val_loss: 6.8375e-04 - val_mean_squared_logarithmic_error: 6.8375e-04\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0010.\n",
      "Epoch 6/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 5.0487e-04 - mean_squared_logarithmic_error: 5.0487e-04The average loss for epoch 5 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.0595e-04 - mean_squared_logarithmic_error: 5.0595e-04 - val_loss: 6.8175e-04 - val_mean_squared_logarithmic_error: 6.8175e-04\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0010.\n",
      "Epoch 7/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 4.5057e-04 - mean_squared_logarithmic_error: 4.5057e-04The average loss for epoch 6 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.5315e-04 - mean_squared_logarithmic_error: 4.5315e-04 - val_loss: 6.8280e-04 - val_mean_squared_logarithmic_error: 6.8280e-04\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0010.\n",
      "Epoch 8/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 3.9239e-04 - mean_squared_logarithmic_error: 3.9239e-04The average loss for epoch 7 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.9313e-04 - mean_squared_logarithmic_error: 3.9313e-04 - val_loss: 6.9262e-04 - val_mean_squared_logarithmic_error: 6.9262e-04\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0010.\n",
      "Epoch 9/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 3.3725e-04 - mean_squared_logarithmic_error: 3.3725e-04The average loss for epoch 8 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.3714e-04 - mean_squared_logarithmic_error: 3.3714e-04 - val_loss: 6.9638e-04 - val_mean_squared_logarithmic_error: 6.9638e-04\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0010.\n",
      "Epoch 10/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 2.6967e-04 - mean_squared_logarithmic_error: 2.6967e-04The average loss for epoch 9 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.7102e-04 - mean_squared_logarithmic_error: 2.7102e-04 - val_loss: 6.6250e-04 - val_mean_squared_logarithmic_error: 6.6250e-04\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0010.\n",
      "Epoch 11/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.1349e-04 - mean_squared_logarithmic_error: 2.1349e-04The average loss for epoch 10 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.1442e-04 - mean_squared_logarithmic_error: 2.1442e-04 - val_loss: 6.3878e-04 - val_mean_squared_logarithmic_error: 6.3878e-04\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0010.\n",
      "Epoch 12/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.6359e-04 - mean_squared_logarithmic_error: 1.6359e-04The average loss for epoch 11 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.6350e-04 - mean_squared_logarithmic_error: 1.6350e-04 - val_loss: 6.0352e-04 - val_mean_squared_logarithmic_error: 6.0352e-04\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0010.\n",
      "Epoch 13/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.2204e-04 - mean_squared_logarithmic_error: 1.2204e-04The average loss for epoch 12 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.2211e-04 - mean_squared_logarithmic_error: 1.2211e-04 - val_loss: 5.6157e-04 - val_mean_squared_logarithmic_error: 5.6157e-04\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0010.\n",
      "Epoch 14/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.0527e-04 - mean_squared_logarithmic_error: 1.0527e-04The average loss for epoch 13 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0550e-04 - mean_squared_logarithmic_error: 1.0550e-04 - val_loss: 5.8848e-04 - val_mean_squared_logarithmic_error: 5.8848e-04\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0010.\n",
      "Epoch 15/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 9.4264e-05 - mean_squared_logarithmic_error: 9.4264e-05The average loss for epoch 14 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.4483e-05 - mean_squared_logarithmic_error: 9.4483e-05 - val_loss: 5.2530e-04 - val_mean_squared_logarithmic_error: 5.2530e-04\n",
      "\n",
      "Epoch 00015: Learning rate is 0.0010.\n",
      "Epoch 16/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 8.7335e-05 - mean_squared_logarithmic_error: 8.7335e-05 ETA: 0s - loss: 8.6727e-05 - mean_squared_logarithmic_error: 8.6727e-The average loss for epoch 15 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.7529e-05 - mean_squared_logarithmic_error: 8.7529e-05 - val_loss: 4.9249e-04 - val_mean_squared_logarithmic_error: 4.9249e-04\n",
      "\n",
      "Epoch 00016: Learning rate is 0.0010.\n",
      "Epoch 17/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 9.1861e-05 - mean_squared_logarithmic_error: 9.1861e-05 ETA: 0s - loss: 9.1426e-05 - mean_squared_logarithmic_errThe average loss for epoch 16 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.1957e-05 - mean_squared_logarithmic_error: 9.1957e-05 - val_loss: 5.2700e-04 - val_mean_squared_logarithmic_error: 5.2700e-04\n",
      "\n",
      "Epoch 00017: Learning rate is 0.0010.\n",
      "Epoch 18/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.0306e-04 - mean_squared_logarithmic_error: 1.0306e-04The average loss for epoch 17 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0274e-04 - mean_squared_logarithmic_error: 1.0274e-04 - val_loss: 5.0323e-04 - val_mean_squared_logarithmic_error: 5.0323e-04\n",
      "\n",
      "Epoch 00018: Learning rate is 0.0010.\n",
      "Epoch 19/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 9.2159e-05 - mean_squared_logarithmic_error: 9.2159e-05The average loss for epoch 18 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.2665e-05 - mean_squared_logarithmic_error: 9.2665e-05 - val_loss: 5.3210e-04 - val_mean_squared_logarithmic_error: 5.3210e-04\n",
      "\n",
      "Epoch 00019: Learning rate is 0.0010.\n",
      "Epoch 20/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/264 [============================>.] - ETA: 0s - loss: 9.1212e-05 - mean_squared_logarithmic_error: 9.1212e-05The average loss for epoch 19 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.1102e-05 - mean_squared_logarithmic_error: 9.1102e-05 - val_loss: 4.6420e-04 - val_mean_squared_logarithmic_error: 4.6420e-04\n",
      "\n",
      "Epoch 00020: Learning rate is 0.0010.\n",
      "Epoch 21/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 8.2490e-05 - mean_squared_logarithmic_error: 8.2490e-05The average loss for epoch 20 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.2612e-05 - mean_squared_logarithmic_error: 8.2612e-05 - val_loss: 5.0608e-04 - val_mean_squared_logarithmic_error: 5.0608e-04\n",
      "\n",
      "Epoch 00021: Learning rate is 0.0010.\n",
      "Epoch 22/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.1415e-04 - mean_squared_logarithmic_error: 1.1415e-04The average loss for epoch 21 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.1248e-04 - mean_squared_logarithmic_error: 1.1248e-04 - val_loss: 4.2793e-04 - val_mean_squared_logarithmic_error: 4.2793e-04\n",
      "\n",
      "Epoch 00022: Learning rate is 0.0010.\n",
      "Epoch 23/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 9.4928e-05 - mean_squared_logarithmic_error: 9.4928e-05The average loss for epoch 22 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 9.4446e-05 - mean_squared_logarithmic_error: 9.4446e-05 - val_loss: 4.4498e-04 - val_mean_squared_logarithmic_error: 4.4498e-04\n",
      "\n",
      "Epoch 00023: Learning rate is 0.0010.\n",
      "Epoch 24/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 7.8808e-05 - mean_squared_logarithmic_error: 7.8808e-05The average loss for epoch 23 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.8337e-05 - mean_squared_logarithmic_error: 7.8337e-05 - val_loss: 4.1829e-04 - val_mean_squared_logarithmic_error: 4.1829e-04\n",
      "\n",
      "Epoch 00024: Learning rate is 0.0010.\n",
      "Epoch 25/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 7.3783e-05 - mean_squared_logarithmic_error: 7.3783e-05The average loss for epoch 24 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.4644e-05 - mean_squared_logarithmic_error: 7.4644e-05 - val_loss: 4.2339e-04 - val_mean_squared_logarithmic_error: 4.2339e-04\n",
      "\n",
      "Epoch 00025: Learning rate is 0.0010.\n",
      "Epoch 26/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 7.2166e-05 - mean_squared_logarithmic_error: 7.2166e-05The average loss for epoch 25 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.2436e-05 - mean_squared_logarithmic_error: 7.2436e-05 - val_loss: 4.0329e-04 - val_mean_squared_logarithmic_error: 4.0329e-04\n",
      "\n",
      "Epoch 00026: Learning rate is 0.0010.\n",
      "Epoch 27/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 7.9204e-05 - mean_squared_logarithmic_error: 7.9204e-05The average loss for epoch 26 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.9329e-05 - mean_squared_logarithmic_error: 7.9329e-05 - val_loss: 4.3106e-04 - val_mean_squared_logarithmic_error: 4.3106e-04\n",
      "\n",
      "Epoch 00027: Learning rate is 0.0010.\n",
      "Epoch 28/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 7.6560e-05 - mean_squared_logarithmic_error: 7.6560e-05The average loss for epoch 27 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.6827e-05 - mean_squared_logarithmic_error: 7.6827e-05 - val_loss: 3.8400e-04 - val_mean_squared_logarithmic_error: 3.8400e-04\n",
      "\n",
      "Epoch 00028: Learning rate is 0.0010.\n",
      "Epoch 29/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 7.7484e-05 - mean_squared_logarithmic_error: 7.7484e-05The average loss for epoch 28 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.7614e-05 - mean_squared_logarithmic_error: 7.7614e-05 - val_loss: 3.8601e-04 - val_mean_squared_logarithmic_error: 3.8601e-04\n",
      "\n",
      "Epoch 00029: Learning rate is 0.0010.\n",
      "Epoch 30/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 6.5383e-05 - mean_squared_logarithmic_error: 6.5383e-05The average loss for epoch 29 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.5418e-05 - mean_squared_logarithmic_error: 6.5418e-05 - val_loss: 4.1186e-04 - val_mean_squared_logarithmic_error: 4.1186e-04\n",
      "\n",
      "Epoch 00030: Learning rate is 0.0010.\n",
      "Epoch 31/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 6.0465e-05 - mean_squared_logarithmic_error: 6.0465e-05The average loss for epoch 30 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.0387e-05 - mean_squared_logarithmic_error: 6.0387e-05 - val_loss: 4.2170e-04 - val_mean_squared_logarithmic_error: 4.2170e-04\n",
      "\n",
      "Epoch 00031: Learning rate is 0.0010.\n",
      "Epoch 32/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 5.9716e-05 - mean_squared_logarithmic_error: 5.9716e-05The average loss for epoch 31 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.9580e-05 - mean_squared_logarithmic_error: 5.9580e-05 - val_loss: 3.6570e-04 - val_mean_squared_logarithmic_error: 3.6570e-04\n",
      "\n",
      "Epoch 00032: Learning rate is 0.0010.\n",
      "Epoch 33/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 6.6512e-05 - mean_squared_logarithmic_error: 6.6512e-05The average loss for epoch 32 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.5936e-05 - mean_squared_logarithmic_error: 6.5936e-05 - val_loss: 3.7748e-04 - val_mean_squared_logarithmic_error: 3.7748e-04\n",
      "\n",
      "Epoch 00033: Learning rate is 0.0010.\n",
      "Epoch 34/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 5.6539e-05 - mean_squared_logarithmic_error: 5.6539e-05The average loss for epoch 33 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.6501e-05 - mean_squared_logarithmic_error: 5.6501e-05 - val_loss: 4.1280e-04 - val_mean_squared_logarithmic_error: 4.1280e-04\n",
      "\n",
      "Epoch 00034: Learning rate is 0.0010.\n",
      "Epoch 35/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 5.3078e-05 - mean_squared_logarithmic_error: 5.3078e-05The average loss for epoch 34 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.2986e-05 - mean_squared_logarithmic_error: 5.2986e-05 - val_loss: 3.6388e-04 - val_mean_squared_logarithmic_error: 3.6388e-04\n",
      "\n",
      "Epoch 00035: Learning rate is 0.0010.\n",
      "Epoch 36/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 5.7006e-05 - mean_squared_logarithmic_error: 5.7006e-05The average loss for epoch 35 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.7858e-05 - mean_squared_logarithmic_error: 5.7858e-05 - val_loss: 3.7847e-04 - val_mean_squared_logarithmic_error: 3.7847e-04\n",
      "\n",
      "Epoch 00036: Learning rate is 0.0010.\n",
      "Epoch 37/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 4.8464e-05 - mean_squared_logarithmic_error: 4.8464e-05The average loss for epoch 36 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.8563e-05 - mean_squared_logarithmic_error: 4.8563e-05 - val_loss: 3.5462e-04 - val_mean_squared_logarithmic_error: 3.5462e-04\n",
      "\n",
      "Epoch 00037: Learning rate is 0.0010.\n",
      "Epoch 38/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 5.0516e-05 - mean_squared_logarithmic_error: 5.0516e-05The average loss for epoch 37 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.0520e-05 - mean_squared_logarithmic_error: 5.0520e-05 - val_loss: 3.5584e-04 - val_mean_squared_logarithmic_error: 3.5584e-04\n",
      "\n",
      "Epoch 00038: Learning rate is 0.0010.\n",
      "Epoch 39/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 5.3253e-05 - mean_squared_logarithmic_error: 5.3253e-05The average loss for epoch 38 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.2183e-05 - mean_squared_logarithmic_error: 5.2183e-05 - val_loss: 3.2858e-04 - val_mean_squared_logarithmic_error: 3.2858e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: Learning rate is 0.0010.\n",
      "Epoch 40/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 4.5535e-05 - mean_squared_logarithmic_error: 4.5535e-05The average loss for epoch 39 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.5722e-05 - mean_squared_logarithmic_error: 4.5722e-05 - val_loss: 3.3618e-04 - val_mean_squared_logarithmic_error: 3.3618e-04\n",
      "\n",
      "Epoch 00040: Learning rate is 0.0010.\n",
      "Epoch 41/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 4.7041e-05 - mean_squared_logarithmic_error: 4.7041e-05The average loss for epoch 40 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.7255e-05 - mean_squared_logarithmic_error: 4.7255e-05 - val_loss: 3.4011e-04 - val_mean_squared_logarithmic_error: 3.4011e-04\n",
      "\n",
      "Epoch 00041: Learning rate is 0.0010.\n",
      "Epoch 42/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 4.8196e-05 - mean_squared_logarithmic_error: 4.8196e-05The average loss for epoch 41 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.7538e-05 - mean_squared_logarithmic_error: 4.7538e-05 - val_loss: 3.3375e-04 - val_mean_squared_logarithmic_error: 3.3375e-04\n",
      "\n",
      "Epoch 00042: Learning rate is 0.0010.\n",
      "Epoch 43/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 4.6786e-05 - mean_squared_logarithmic_error: 4.6786e-05The average loss for epoch 42 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.6469e-05 - mean_squared_logarithmic_error: 4.6469e-05 - val_loss: 3.3021e-04 - val_mean_squared_logarithmic_error: 3.3021e-04\n",
      "\n",
      "Epoch 00043: Learning rate is 0.0010.\n",
      "Epoch 44/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 4.2179e-05 - mean_squared_logarithmic_error: 4.2179e-05The average loss for epoch 43 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.3206e-05 - mean_squared_logarithmic_error: 4.3206e-05 - val_loss: 3.2969e-04 - val_mean_squared_logarithmic_error: 3.2969e-04\n",
      "\n",
      "Epoch 00044: Learning rate is 0.0010.\n",
      "Epoch 45/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 4.3250e-05 - mean_squared_logarithmic_error: 4.3250e-05The average loss for epoch 44 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.3145e-05 - mean_squared_logarithmic_error: 4.3145e-05 - val_loss: 3.4593e-04 - val_mean_squared_logarithmic_error: 3.4593e-04\n",
      "\n",
      "Epoch 00045: Learning rate is 0.0010.\n",
      "Epoch 46/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 4.1081e-05 - mean_squared_logarithmic_error: 4.1081e-05The average loss for epoch 45 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.1084e-05 - mean_squared_logarithmic_error: 4.1084e-05 - val_loss: 3.0259e-04 - val_mean_squared_logarithmic_error: 3.0259e-04\n",
      "\n",
      "Epoch 00046: Learning rate is 0.0010.\n",
      "Epoch 47/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 3.5817e-05 - mean_squared_logarithmic_error: 3.5817e-05The average loss for epoch 46 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.5723e-05 - mean_squared_logarithmic_error: 3.5723e-05 - val_loss: 2.9284e-04 - val_mean_squared_logarithmic_error: 2.9284e-04\n",
      "\n",
      "Epoch 00047: Learning rate is 0.0010.\n",
      "Epoch 48/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 3.9708e-05 - mean_squared_logarithmic_error: 3.9708e-05The average loss for epoch 47 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.9530e-05 - mean_squared_logarithmic_error: 3.9530e-05 - val_loss: 3.0377e-04 - val_mean_squared_logarithmic_error: 3.0377e-04\n",
      "\n",
      "Epoch 00048: Learning rate is 0.0010.\n",
      "Epoch 49/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 3.5615e-05 - mean_squared_logarithmic_error: 3.5615e-05The average loss for epoch 48 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.5467e-05 - mean_squared_logarithmic_error: 3.5467e-05 - val_loss: 3.1600e-04 - val_mean_squared_logarithmic_error: 3.1600e-04\n",
      "\n",
      "Epoch 00049: Learning rate is 0.0010.\n",
      "Epoch 50/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 3.4366e-05 - mean_squared_logarithmic_error: 3.4366e-05The average loss for epoch 49 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.4573e-05 - mean_squared_logarithmic_error: 3.4573e-05 - val_loss: 3.0373e-04 - val_mean_squared_logarithmic_error: 3.0373e-04\n",
      "\n",
      "Epoch 00050: Learning rate is 0.0010.\n",
      "Epoch 51/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 3.5932e-05 - mean_squared_logarithmic_error: 3.5932e-05The average loss for epoch 50 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.5789e-05 - mean_squared_logarithmic_error: 3.5789e-05 - val_loss: 2.8154e-04 - val_mean_squared_logarithmic_error: 2.8154e-04\n",
      "\n",
      "Epoch 00051: Learning rate is 0.0010.\n",
      "Epoch 52/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 3.2339e-05 - mean_squared_logarithmic_error: 3.2339e-05The average loss for epoch 51 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.2345e-05 - mean_squared_logarithmic_error: 3.2345e-05 - val_loss: 3.6198e-04 - val_mean_squared_logarithmic_error: 3.6198e-04\n",
      "\n",
      "Epoch 00052: Learning rate is 0.0010.\n",
      "Epoch 53/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 3.3337e-05 - mean_squared_logarithmic_error: 3.3337e-05The average loss for epoch 52 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.3470e-05 - mean_squared_logarithmic_error: 3.3470e-05 - val_loss: 2.7517e-04 - val_mean_squared_logarithmic_error: 2.7517e-04\n",
      "\n",
      "Epoch 00053: Learning rate is 0.0010.\n",
      "Epoch 54/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 3.5168e-05 - mean_squared_logarithmic_error: 3.5168e-05The average loss for epoch 53 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.5578e-05 - mean_squared_logarithmic_error: 3.5578e-05 - val_loss: 3.0114e-04 - val_mean_squared_logarithmic_error: 3.0114e-04\n",
      "\n",
      "Epoch 00054: Learning rate is 0.0010.\n",
      "Epoch 55/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 2.6799e-05 - mean_squared_logarithmic_error: 2.6799e-05The average loss for epoch 54 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.6681e-05 - mean_squared_logarithmic_error: 2.6681e-05 - val_loss: 2.9620e-04 - val_mean_squared_logarithmic_error: 2.9620e-04\n",
      "\n",
      "Epoch 00055: Learning rate is 0.0010.\n",
      "Epoch 56/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 2.6251e-05 - mean_squared_logarithmic_error: 2.6251e-05 ETA: 0s - loss: 2.6239e-05 - mean_squared_logarithmic_error: 2.6239The average loss for epoch 55 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.6627e-05 - mean_squared_logarithmic_error: 2.6627e-05 - val_loss: 2.9261e-04 - val_mean_squared_logarithmic_error: 2.9261e-04\n",
      "\n",
      "Epoch 00056: Learning rate is 0.0010.\n",
      "Epoch 57/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 2.4639e-05 - mean_squared_logarithmic_error: 2.4639e-05The average loss for epoch 56 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4700e-05 - mean_squared_logarithmic_error: 2.4700e-05 - val_loss: 2.8343e-04 - val_mean_squared_logarithmic_error: 2.8343e-04\n",
      "\n",
      "Epoch 00057: Learning rate is 0.0010.\n",
      "Epoch 58/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.7379e-05 - mean_squared_logarithmic_error: 2.7379e-05The average loss for epoch 57 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.7376e-05 - mean_squared_logarithmic_error: 2.7376e-05 - val_loss: 2.7615e-04 - val_mean_squared_logarithmic_error: 2.7615e-04\n",
      "\n",
      "Epoch 00058: Learning rate is 0.0010.\n",
      "Epoch 59/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.9696e-05 - mean_squared_logarithmic_error: 2.9696e-05The average loss for epoch 58 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.9565e-05 - mean_squared_logarithmic_error: 2.9565e-05 - val_loss: 2.7048e-04 - val_mean_squared_logarithmic_error: 2.7048e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00059: Learning rate is 0.0010.\n",
      "Epoch 60/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 3.4688e-05 - mean_squared_logarithmic_error: 3.4688e-05The average loss for epoch 59 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.4817e-05 - mean_squared_logarithmic_error: 3.4817e-05 - val_loss: 3.0724e-04 - val_mean_squared_logarithmic_error: 3.0724e-04\n",
      "\n",
      "Epoch 00060: Learning rate is 0.0010.\n",
      "Epoch 61/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 2.6098e-05 - mean_squared_logarithmic_error: 2.6098e-05The average loss for epoch 60 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.6037e-05 - mean_squared_logarithmic_error: 2.6037e-05 - val_loss: 2.7351e-04 - val_mean_squared_logarithmic_error: 2.7351e-04\n",
      "\n",
      "Epoch 00061: Learning rate is 0.0010.\n",
      "Epoch 62/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.3072e-05 - mean_squared_logarithmic_error: 2.3072e-05The average loss for epoch 61 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.2902e-05 - mean_squared_logarithmic_error: 2.2902e-05 - val_loss: 2.5720e-04 - val_mean_squared_logarithmic_error: 2.5720e-04\n",
      "\n",
      "Epoch 00062: Learning rate is 0.0010.\n",
      "Epoch 63/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.9029e-05 - mean_squared_logarithmic_error: 1.9029e-05The average loss for epoch 62 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.8975e-05 - mean_squared_logarithmic_error: 1.8975e-05 - val_loss: 2.7055e-04 - val_mean_squared_logarithmic_error: 2.7055e-04\n",
      "\n",
      "Epoch 00063: Learning rate is 0.0010.\n",
      "Epoch 64/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.9142e-05 - mean_squared_logarithmic_error: 1.9142e-05The average loss for epoch 63 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.9232e-05 - mean_squared_logarithmic_error: 1.9232e-05 - val_loss: 2.6748e-04 - val_mean_squared_logarithmic_error: 2.6748e-04\n",
      "\n",
      "Epoch 00064: Learning rate is 0.0010.\n",
      "Epoch 65/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.1922e-05 - mean_squared_logarithmic_error: 2.1922e-05The average loss for epoch 64 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.1920e-05 - mean_squared_logarithmic_error: 2.1920e-05 - val_loss: 2.6440e-04 - val_mean_squared_logarithmic_error: 2.6440e-04\n",
      "\n",
      "Epoch 00065: Learning rate is 0.0010.\n",
      "Epoch 66/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 2.1779e-05 - mean_squared_logarithmic_error: 2.1779e-05The average loss for epoch 65 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.1776e-05 - mean_squared_logarithmic_error: 2.1776e-05 - val_loss: 2.7342e-04 - val_mean_squared_logarithmic_error: 2.7342e-04\n",
      "\n",
      "Epoch 00066: Learning rate is 0.0010.\n",
      "Epoch 67/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.6307e-05 - mean_squared_logarithmic_error: 2.6307e-05The average loss for epoch 66 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.6238e-05 - mean_squared_logarithmic_error: 2.6238e-05 - val_loss: 2.7364e-04 - val_mean_squared_logarithmic_error: 2.7364e-04\n",
      "\n",
      "Epoch 00067: Learning rate is 0.0010.\n",
      "Epoch 68/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.5464e-05 - mean_squared_logarithmic_error: 2.5464e-05The average loss for epoch 67 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5183e-05 - mean_squared_logarithmic_error: 2.5183e-05 - val_loss: 2.7061e-04 - val_mean_squared_logarithmic_error: 2.7061e-04\n",
      "\n",
      "Epoch 00068: Learning rate is 0.0010.\n",
      "Epoch 69/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 2.2356e-05 - mean_squared_logarithmic_error: 2.2356e-05The average loss for epoch 68 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.2364e-05 - mean_squared_logarithmic_error: 2.2364e-05 - val_loss: 2.7259e-04 - val_mean_squared_logarithmic_error: 2.7259e-04\n",
      "\n",
      "Epoch 00069: Learning rate is 0.0010.\n",
      "Epoch 70/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.0337e-05 - mean_squared_logarithmic_error: 2.0337e-05The average loss for epoch 69 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.0131e-05 - mean_squared_logarithmic_error: 2.0131e-05 - val_loss: 2.6792e-04 - val_mean_squared_logarithmic_error: 2.6792e-04\n",
      "\n",
      "Epoch 00070: Learning rate is 0.0010.\n",
      "Epoch 71/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.0347e-05 - mean_squared_logarithmic_error: 2.0347e-05The average loss for epoch 70 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.0238e-05 - mean_squared_logarithmic_error: 2.0238e-05 - val_loss: 2.5065e-04 - val_mean_squared_logarithmic_error: 2.5065e-04\n",
      "\n",
      "Epoch 00071: Learning rate is 0.0010.\n",
      "Epoch 72/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.9017e-05 - mean_squared_logarithmic_error: 1.9017e-05The average loss for epoch 71 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.8831e-05 - mean_squared_logarithmic_error: 1.8831e-05 - val_loss: 2.6590e-04 - val_mean_squared_logarithmic_error: 2.6590e-04\n",
      "\n",
      "Epoch 00072: Learning rate is 0.0010.\n",
      "Epoch 73/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.9058e-05 - mean_squared_logarithmic_error: 1.9058e-05The average loss for epoch 72 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.8966e-05 - mean_squared_logarithmic_error: 1.8966e-05 - val_loss: 2.3966e-04 - val_mean_squared_logarithmic_error: 2.3966e-04\n",
      "\n",
      "Epoch 00073: Learning rate is 0.0010.\n",
      "Epoch 74/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.8121e-05 - mean_squared_logarithmic_error: 1.8121e-05The average loss for epoch 73 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.8170e-05 - mean_squared_logarithmic_error: 1.8170e-05 - val_loss: 2.6562e-04 - val_mean_squared_logarithmic_error: 2.6562e-04\n",
      "\n",
      "Epoch 00074: Learning rate is 0.0010.\n",
      "Epoch 75/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.9703e-05 - mean_squared_logarithmic_error: 1.9703e-05The average loss for epoch 74 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.9535e-05 - mean_squared_logarithmic_error: 1.9535e-05 - val_loss: 2.5923e-04 - val_mean_squared_logarithmic_error: 2.5923e-04\n",
      "\n",
      "Epoch 00075: Learning rate is 0.0010.\n",
      "Epoch 76/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.9967e-05 - mean_squared_logarithmic_error: 1.9967e-05The average loss for epoch 75 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.9996e-05 - mean_squared_logarithmic_error: 1.9996e-05 - val_loss: 2.3744e-04 - val_mean_squared_logarithmic_error: 2.3744e-04\n",
      "\n",
      "Epoch 00076: Learning rate is 0.0010.\n",
      "Epoch 77/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.7028e-05 - mean_squared_logarithmic_error: 1.7028e-05The average loss for epoch 76 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.7153e-05 - mean_squared_logarithmic_error: 1.7153e-05 - val_loss: 2.4979e-04 - val_mean_squared_logarithmic_error: 2.4979e-04\n",
      "\n",
      "Epoch 00077: Learning rate is 0.0010.\n",
      "Epoch 78/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.7239e-05 - mean_squared_logarithmic_error: 1.7239e-05The average loss for epoch 77 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.7096e-05 - mean_squared_logarithmic_error: 1.7096e-05 - val_loss: 2.3033e-04 - val_mean_squared_logarithmic_error: 2.3033e-04\n",
      "\n",
      "Epoch 00078: Learning rate is 0.0010.\n",
      "Epoch 79/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 2.0059e-05 - mean_squared_logarithmic_error: 2.0059e-05The average loss for epoch 78 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.9840e-05 - mean_squared_logarithmic_error: 1.9840e-05 - val_loss: 2.3694e-04 - val_mean_squared_logarithmic_error: 2.3694e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: Learning rate is 0.0010.\n",
      "Epoch 80/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.6698e-05 - mean_squared_logarithmic_error: 1.6698e-05The average loss for epoch 79 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.6661e-05 - mean_squared_logarithmic_error: 1.6661e-05 - val_loss: 2.4029e-04 - val_mean_squared_logarithmic_error: 2.4029e-04\n",
      "\n",
      "Epoch 00080: Learning rate is 0.0010.\n",
      "Epoch 81/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5549e-05 - mean_squared_logarithmic_error: 1.5549e-05The average loss for epoch 80 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.5592e-05 - mean_squared_logarithmic_error: 1.5592e-05 - val_loss: 2.4715e-04 - val_mean_squared_logarithmic_error: 2.4715e-04\n",
      "\n",
      "Epoch 00081: Learning rate is 0.0010.\n",
      "Epoch 82/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.6523e-05 - mean_squared_logarithmic_error: 1.6523e-05The average loss for epoch 81 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.6401e-05 - mean_squared_logarithmic_error: 1.6401e-05 - val_loss: 2.3441e-04 - val_mean_squared_logarithmic_error: 2.3441e-04\n",
      "\n",
      "Epoch 00082: Learning rate is 0.0010.\n",
      "Epoch 83/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 1.6139e-05 - mean_squared_logarithmic_error: 1.6139e-05The average loss for epoch 82 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.5991e-05 - mean_squared_logarithmic_error: 1.5991e-05 - val_loss: 2.2630e-04 - val_mean_squared_logarithmic_error: 2.2630e-04\n",
      "\n",
      "Epoch 00083: Learning rate is 0.0010.\n",
      "Epoch 84/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.6438e-05 - mean_squared_logarithmic_error: 1.6438e-05The average loss for epoch 83 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.6501e-05 - mean_squared_logarithmic_error: 1.6501e-05 - val_loss: 2.3942e-04 - val_mean_squared_logarithmic_error: 2.3942e-04\n",
      "\n",
      "Epoch 00084: Learning rate is 0.0010.\n",
      "Epoch 85/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 1.7013e-05 - mean_squared_logarithmic_error: 1.7013e-05The average loss for epoch 84 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.6830e-05 - mean_squared_logarithmic_error: 1.6830e-05 - val_loss: 2.5324e-04 - val_mean_squared_logarithmic_error: 2.5324e-04\n",
      "\n",
      "Epoch 00085: Learning rate is 0.0010.\n",
      "Epoch 86/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.7068e-05 - mean_squared_logarithmic_error: 1.7068e-05The average loss for epoch 85 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.7056e-05 - mean_squared_logarithmic_error: 1.7056e-05 - val_loss: 2.2815e-04 - val_mean_squared_logarithmic_error: 2.2815e-04\n",
      "\n",
      "Epoch 00086: Learning rate is 0.0010.\n",
      "Epoch 87/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5031e-05 - mean_squared_logarithmic_error: 1.5031e-05The average loss for epoch 86 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.4993e-05 - mean_squared_logarithmic_error: 1.4993e-05 - val_loss: 2.2280e-04 - val_mean_squared_logarithmic_error: 2.2280e-04\n",
      "\n",
      "Epoch 00087: Learning rate is 0.0010.\n",
      "Epoch 88/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.4472e-05 - mean_squared_logarithmic_error: 1.4472e-05The average loss for epoch 87 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.4432e-05 - mean_squared_logarithmic_error: 1.4432e-05 - val_loss: 2.2517e-04 - val_mean_squared_logarithmic_error: 2.2517e-04\n",
      "\n",
      "Epoch 00088: Learning rate is 0.0010.\n",
      "Epoch 89/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5760e-05 - mean_squared_logarithmic_error: 1.5760e-05The average loss for epoch 88 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5708e-05 - mean_squared_logarithmic_error: 1.5708e-05 - val_loss: 2.4079e-04 - val_mean_squared_logarithmic_error: 2.4079e-04\n",
      "\n",
      "Epoch 00089: Learning rate is 0.0010.\n",
      "Epoch 90/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5525e-05 - mean_squared_logarithmic_error: 1.5525e-05The average loss for epoch 89 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.5474e-05 - mean_squared_logarithmic_error: 1.5474e-05 - val_loss: 2.2150e-04 - val_mean_squared_logarithmic_error: 2.2150e-04\n",
      "\n",
      "Epoch 00090: Learning rate is 0.0010.\n",
      "Epoch 91/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.4399e-05 - mean_squared_logarithmic_error: 1.4399e-05The average loss for epoch 90 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.4367e-05 - mean_squared_logarithmic_error: 1.4367e-05 - val_loss: 2.4136e-04 - val_mean_squared_logarithmic_error: 2.4136e-04\n",
      "\n",
      "Epoch 00091: Learning rate is 0.0010.\n",
      "Epoch 92/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.4812e-05 - mean_squared_logarithmic_error: 1.4812e-05The average loss for epoch 91 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.4795e-05 - mean_squared_logarithmic_error: 1.4795e-05 - val_loss: 2.2569e-04 - val_mean_squared_logarithmic_error: 2.2569e-04\n",
      "\n",
      "Epoch 00092: Learning rate is 0.0010.\n",
      "Epoch 93/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.2979e-05 - mean_squared_logarithmic_error: 1.2979e-05The average loss for epoch 92 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.2978e-05 - mean_squared_logarithmic_error: 1.2978e-05 - val_loss: 2.2986e-04 - val_mean_squared_logarithmic_error: 2.2986e-04\n",
      "\n",
      "Epoch 00093: Learning rate is 0.0010.\n",
      "Epoch 94/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5616e-05 - mean_squared_logarithmic_error: 1.5616e-05The average loss for epoch 93 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5594e-05 - mean_squared_logarithmic_error: 1.5594e-05 - val_loss: 2.1933e-04 - val_mean_squared_logarithmic_error: 2.1933e-04\n",
      "\n",
      "Epoch 00094: Learning rate is 0.0010.\n",
      "Epoch 95/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.4634e-05 - mean_squared_logarithmic_error: 1.4634e-05The average loss for epoch 94 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.4484e-05 - mean_squared_logarithmic_error: 1.4484e-05 - val_loss: 2.2501e-04 - val_mean_squared_logarithmic_error: 2.2501e-04\n",
      "\n",
      "Epoch 00095: Learning rate is 0.0010.\n",
      "Epoch 96/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5442e-05 - mean_squared_logarithmic_error: 1.5442e-05The average loss for epoch 95 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.5349e-05 - mean_squared_logarithmic_error: 1.5349e-05 - val_loss: 2.2899e-04 - val_mean_squared_logarithmic_error: 2.2899e-04\n",
      "\n",
      "Epoch 00096: Learning rate is 0.0010.\n",
      "Epoch 97/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.4980e-05 - mean_squared_logarithmic_error: 1.4980e-05The average loss for epoch 96 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.5012e-05 - mean_squared_logarithmic_error: 1.5012e-05 - val_loss: 2.1107e-04 - val_mean_squared_logarithmic_error: 2.1107e-04\n",
      "\n",
      "Epoch 00097: Learning rate is 0.0010.\n",
      "Epoch 98/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.1569e-05 - mean_squared_logarithmic_error: 1.1569e-05The average loss for epoch 97 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.1555e-05 - mean_squared_logarithmic_error: 1.1555e-05 - val_loss: 2.3408e-04 - val_mean_squared_logarithmic_error: 2.3408e-04\n",
      "\n",
      "Epoch 00098: Learning rate is 0.0010.\n",
      "Epoch 99/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.0585e-05 - mean_squared_logarithmic_error: 1.0585e-05The average loss for epoch 98 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0513e-05 - mean_squared_logarithmic_error: 1.0513e-05 - val_loss: 2.1987e-04 - val_mean_squared_logarithmic_error: 2.1987e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: Learning rate is 0.0010.\n",
      "Epoch 100/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.0445e-05 - mean_squared_logarithmic_error: 1.0445e-05The average loss for epoch 99 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0459e-05 - mean_squared_logarithmic_error: 1.0459e-05 - val_loss: 2.0317e-04 - val_mean_squared_logarithmic_error: 2.0317e-04\n",
      "\n",
      "Epoch 00100: Learning rate is 0.0010.\n",
      "Epoch 101/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 1.1273e-05 - mean_squared_logarithmic_error: 1.1273e-05The average loss for epoch 100 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.1280e-05 - mean_squared_logarithmic_error: 1.1280e-05 - val_loss: 2.0761e-04 - val_mean_squared_logarithmic_error: 2.0761e-04\n",
      "\n",
      "Epoch 00101: Learning rate is 0.0010.\n",
      "Epoch 102/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.3939e-05 - mean_squared_logarithmic_error: 1.3939e-05The average loss for epoch 101 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.3891e-05 - mean_squared_logarithmic_error: 1.3891e-05 - val_loss: 2.3229e-04 - val_mean_squared_logarithmic_error: 2.3229e-04\n",
      "\n",
      "Epoch 00102: Learning rate is 0.0010.\n",
      "Epoch 103/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.4385e-05 - mean_squared_logarithmic_error: 1.4385e-05The average loss for epoch 102 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.4310e-05 - mean_squared_logarithmic_error: 1.4310e-05 - val_loss: 2.0077e-04 - val_mean_squared_logarithmic_error: 2.0077e-04\n",
      "\n",
      "Epoch 00103: Learning rate is 0.0010.\n",
      "Epoch 104/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.4551e-05 - mean_squared_logarithmic_error: 1.4551e-05The average loss for epoch 103 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.4551e-05 - mean_squared_logarithmic_error: 1.4551e-05 - val_loss: 2.0764e-04 - val_mean_squared_logarithmic_error: 2.0764e-04\n",
      "\n",
      "Epoch 00104: Learning rate is 0.0010.\n",
      "Epoch 105/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.4548e-05 - mean_squared_logarithmic_error: 1.4548e-05The average loss for epoch 104 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.4547e-05 - mean_squared_logarithmic_error: 1.4547e-05 - val_loss: 2.1155e-04 - val_mean_squared_logarithmic_error: 2.1155e-04\n",
      "\n",
      "Epoch 00105: Learning rate is 0.0010.\n",
      "Epoch 106/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.1783e-05 - mean_squared_logarithmic_error: 1.1783e-05 ETA: 0s - loss: 1.1998e-05 - mean_squared_logarithmic_error: 1.19The average loss for epoch 105 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.1713e-05 - mean_squared_logarithmic_error: 1.1713e-05 - val_loss: 2.0153e-04 - val_mean_squared_logarithmic_error: 2.0153e-04\n",
      "\n",
      "Epoch 00106: Learning rate is 0.0010.\n",
      "Epoch 107/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.0226e-05 - mean_squared_logarithmic_error: 1.0226e-05The average loss for epoch 106 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0167e-05 - mean_squared_logarithmic_error: 1.0167e-05 - val_loss: 2.1294e-04 - val_mean_squared_logarithmic_error: 2.1294e-04\n",
      "\n",
      "Epoch 00107: Learning rate is 0.0010.\n",
      "Epoch 108/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 8.9291e-06 - mean_squared_logarithmic_error: 8.9291e-06The average loss for epoch 107 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.8950e-06 - mean_squared_logarithmic_error: 8.8950e-06 - val_loss: 1.9686e-04 - val_mean_squared_logarithmic_error: 1.9686e-04\n",
      "\n",
      "Epoch 00108: Learning rate is 0.0010.\n",
      "Epoch 109/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.0465e-05 - mean_squared_logarithmic_error: 1.0465e-05The average loss for epoch 108 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0465e-05 - mean_squared_logarithmic_error: 1.0465e-05 - val_loss: 2.1947e-04 - val_mean_squared_logarithmic_error: 2.1947e-04\n",
      "\n",
      "Epoch 00109: Learning rate is 0.0010.\n",
      "Epoch 110/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 9.7690e-06 - mean_squared_logarithmic_error: 9.7690e- - ETA: 0s - loss: 1.0014e-05 - mean_squared_logarithmic_error: 1.0014e-05The average loss for epoch 109 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0014e-05 - mean_squared_logarithmic_error: 1.0014e-05 - val_loss: 1.9859e-04 - val_mean_squared_logarithmic_error: 1.9859e-04\n",
      "\n",
      "Epoch 00110: Learning rate is 0.0010.\n",
      "Epoch 111/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 1.0427e-05 - mean_squared_logarithmic_error: 1.0427e-05The average loss for epoch 110 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0587e-05 - mean_squared_logarithmic_error: 1.0587e-05 - val_loss: 2.0252e-04 - val_mean_squared_logarithmic_error: 2.0252e-04\n",
      "\n",
      "Epoch 00111: Learning rate is 0.0010.\n",
      "Epoch 112/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.1693e-05 - mean_squared_logarithmic_error: 1.1693e-05The average loss for epoch 111 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.1720e-05 - mean_squared_logarithmic_error: 1.1720e-05 - val_loss: 2.0330e-04 - val_mean_squared_logarithmic_error: 2.0330e-04\n",
      "\n",
      "Epoch 00112: Learning rate is 0.0010.\n",
      "Epoch 113/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.3075e-05 - mean_squared_logarithmic_error: 1.3075e-05The average loss for epoch 112 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.2936e-05 - mean_squared_logarithmic_error: 1.2936e-05 - val_loss: 1.9445e-04 - val_mean_squared_logarithmic_error: 1.9445e-04\n",
      "\n",
      "Epoch 00113: Learning rate is 0.0010.\n",
      "Epoch 114/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.2258e-05 - mean_squared_logarithmic_error: 1.2258e-05The average loss for epoch 113 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.2124e-05 - mean_squared_logarithmic_error: 1.2124e-05 - val_loss: 1.9742e-04 - val_mean_squared_logarithmic_error: 1.9742e-04\n",
      "\n",
      "Epoch 00114: Learning rate is 0.0010.\n",
      "Epoch 115/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.0195e-05 - mean_squared_logarithmic_error: 1.0195e-05The average loss for epoch 114 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0195e-05 - mean_squared_logarithmic_error: 1.0195e-05 - val_loss: 1.9761e-04 - val_mean_squared_logarithmic_error: 1.9761e-04\n",
      "\n",
      "Epoch 00115: Learning rate is 0.0010.\n",
      "Epoch 116/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 8.5254e-06 - mean_squared_logarithmic_error: 8.5254e-06The average loss for epoch 115 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.4979e-06 - mean_squared_logarithmic_error: 8.4979e-06 - val_loss: 2.0334e-04 - val_mean_squared_logarithmic_error: 2.0334e-04\n",
      "\n",
      "Epoch 00116: Learning rate is 0.0010.\n",
      "Epoch 117/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 1.0112e-05 - mean_squared_logarithmic_error: 1.0112e-05The average loss for epoch 116 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0200e-05 - mean_squared_logarithmic_error: 1.0200e-05 - val_loss: 1.9593e-04 - val_mean_squared_logarithmic_error: 1.9593e-04\n",
      "\n",
      "Epoch 00117: Learning rate is 0.0010.\n",
      "Epoch 118/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.0100e-05 - mean_squared_logarithmic_error: 1.0100e-05The average loss for epoch 117 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0057e-05 - mean_squared_logarithmic_error: 1.0057e-05 - val_loss: 1.9450e-04 - val_mean_squared_logarithmic_error: 1.9450e-04\n",
      "\n",
      "Epoch 00118: Learning rate is 0.0010.\n",
      "Epoch 119/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/264 [===========================>..] - ETA: 0s - loss: 9.6106e-06 - mean_squared_logarithmic_error: 9.6106e-06The average loss for epoch 118 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 9.6727e-06 - mean_squared_logarithmic_error: 9.6727e-06 - val_loss: 1.9497e-04 - val_mean_squared_logarithmic_error: 1.9497e-04\n",
      "\n",
      "Epoch 00119: Learning rate is 0.0010.\n",
      "Epoch 120/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.0826e-05 - mean_squared_logarithmic_error: 1.0826e-05The average loss for epoch 119 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0829e-05 - mean_squared_logarithmic_error: 1.0829e-05 - val_loss: 1.9466e-04 - val_mean_squared_logarithmic_error: 1.9466e-04\n",
      "\n",
      "Epoch 00120: Learning rate is 0.0010.\n",
      "Epoch 121/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.2468e-05 - mean_squared_logarithmic_error: 1.2468e-05The average loss for epoch 120 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.2329e-05 - mean_squared_logarithmic_error: 1.2329e-05 - val_loss: 1.9532e-04 - val_mean_squared_logarithmic_error: 1.9532e-04\n",
      "\n",
      "Epoch 00121: Learning rate is 0.0010.\n",
      "Epoch 122/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.0571e-05 - mean_squared_logarithmic_error: 1.0571e-05The average loss for epoch 121 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0632e-05 - mean_squared_logarithmic_error: 1.0632e-05 - val_loss: 2.0417e-04 - val_mean_squared_logarithmic_error: 2.0417e-04\n",
      "\n",
      "Epoch 00122: Learning rate is 0.0010.\n",
      "Epoch 123/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 9.7453e-06 - mean_squared_logarithmic_error: 9.7453e-06The average loss for epoch 122 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.7079e-06 - mean_squared_logarithmic_error: 9.7079e-06 - val_loss: 1.9329e-04 - val_mean_squared_logarithmic_error: 1.9329e-04\n",
      "\n",
      "Epoch 00123: Learning rate is 0.0010.\n",
      "Epoch 124/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.0757e-05 - mean_squared_logarithmic_error: 1.0757e-05The average loss for epoch 123 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0735e-05 - mean_squared_logarithmic_error: 1.0735e-05 - val_loss: 2.0791e-04 - val_mean_squared_logarithmic_error: 2.0791e-04\n",
      "\n",
      "Epoch 00124: Learning rate is 0.0010.\n",
      "Epoch 125/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 8.5027e-06 - mean_squared_logarithmic_error: 8.5027e-06The average loss for epoch 124 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.4672e-06 - mean_squared_logarithmic_error: 8.4672e-06 - val_loss: 1.9230e-04 - val_mean_squared_logarithmic_error: 1.9230e-04\n",
      "\n",
      "Epoch 00125: Learning rate is 0.0010.\n",
      "Epoch 126/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 8.0800e-06 - mean_squared_logarithmic_error: 8.0800e-06The average loss for epoch 125 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.0324e-06 - mean_squared_logarithmic_error: 8.0324e-06 - val_loss: 1.8924e-04 - val_mean_squared_logarithmic_error: 1.8924e-04\n",
      "\n",
      "Epoch 00126: Learning rate is 0.0010.\n",
      "Epoch 127/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 9.0029e-06 - mean_squared_logarithmic_error: 9.0029e-06The average loss for epoch 126 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.9787e-06 - mean_squared_logarithmic_error: 8.9787e-06 - val_loss: 1.8777e-04 - val_mean_squared_logarithmic_error: 1.8777e-04\n",
      "\n",
      "Epoch 00127: Learning rate is 0.0010.\n",
      "Epoch 128/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.0911e-05 - mean_squared_logarithmic_error: 1.0911e-05The average loss for epoch 127 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0737e-05 - mean_squared_logarithmic_error: 1.0737e-05 - val_loss: 2.0124e-04 - val_mean_squared_logarithmic_error: 2.0124e-04\n",
      "\n",
      "Epoch 00128: Learning rate is 0.0010.\n",
      "Epoch 129/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 9.8478e-06 - mean_squared_logarithmic_error: 9.8478e-06The average loss for epoch 128 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.8287e-06 - mean_squared_logarithmic_error: 9.8287e-06 - val_loss: 1.9220e-04 - val_mean_squared_logarithmic_error: 1.9220e-04\n",
      "\n",
      "Epoch 00129: Learning rate is 0.0010.\n",
      "Epoch 130/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 8.4621e-06 - mean_squared_logarithmic_error: 8.4621e-06The average loss for epoch 129 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.6243e-06 - mean_squared_logarithmic_error: 8.6243e-06 - val_loss: 1.8629e-04 - val_mean_squared_logarithmic_error: 1.8629e-04\n",
      "\n",
      "Epoch 00130: Learning rate is 0.0010.\n",
      "Epoch 131/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 9.3433e-06 - mean_squared_logarithmic_error: 9.3433e-06The average loss for epoch 130 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.5438e-06 - mean_squared_logarithmic_error: 9.5438e-06 - val_loss: 2.0455e-04 - val_mean_squared_logarithmic_error: 2.0455e-04\n",
      "\n",
      "Epoch 00131: Learning rate is 0.0010.\n",
      "Epoch 132/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.0219e-05 - mean_squared_logarithmic_error: 1.0219e-05The average loss for epoch 131 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0246e-05 - mean_squared_logarithmic_error: 1.0246e-05 - val_loss: 1.8398e-04 - val_mean_squared_logarithmic_error: 1.8398e-04\n",
      "\n",
      "Epoch 00132: Learning rate is 0.0010.\n",
      "Epoch 133/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.0524e-05 - mean_squared_logarithmic_error: 1.0524e-05The average loss for epoch 132 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0505e-05 - mean_squared_logarithmic_error: 1.0505e-05 - val_loss: 1.8768e-04 - val_mean_squared_logarithmic_error: 1.8768e-04\n",
      "\n",
      "Epoch 00133: Learning rate is 0.0010.\n",
      "Epoch 134/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 8.7189e-06 - mean_squared_logarithmic_error: 8.7189e-06The average loss for epoch 133 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.6689e-06 - mean_squared_logarithmic_error: 8.6689e-06 - val_loss: 1.7409e-04 - val_mean_squared_logarithmic_error: 1.7409e-04\n",
      "\n",
      "Epoch 00134: Learning rate is 0.0010.\n",
      "Epoch 135/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 9.6982e-06 - mean_squared_logarithmic_error: 9.6982e-06The average loss for epoch 134 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.6887e-06 - mean_squared_logarithmic_error: 9.6887e-06 - val_loss: 1.8493e-04 - val_mean_squared_logarithmic_error: 1.8493e-04\n",
      "\n",
      "Epoch 00135: Learning rate is 0.0010.\n",
      "Epoch 136/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 9.1687e-06 - mean_squared_logarithmic_error: 9.1687e-06The average loss for epoch 135 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.1653e-06 - mean_squared_logarithmic_error: 9.1653e-06 - val_loss: 1.8323e-04 - val_mean_squared_logarithmic_error: 1.8323e-04\n",
      "\n",
      "Epoch 00136: Learning rate is 0.0010.\n",
      "Epoch 137/800\n",
      "242/264 [==========================>...] - ETA: 0s - loss: 8.5445e-06 - mean_squared_logarithmic_error: 8.5445e-06The average loss for epoch 136 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.5061e-06 - mean_squared_logarithmic_error: 8.5061e-06 - val_loss: 1.8358e-04 - val_mean_squared_logarithmic_error: 1.8358e-04\n",
      "\n",
      "Epoch 00137: Learning rate is 0.0010.\n",
      "Epoch 138/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 8.9774e-06 - mean_squared_logarithmic_error: 8.9774e-06The average loss for epoch 137 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.9726e-06 - mean_squared_logarithmic_error: 8.9726e-06 - val_loss: 1.8791e-04 - val_mean_squared_logarithmic_error: 1.8791e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: Learning rate is 0.0010.\n",
      "Epoch 139/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 9.0818e-06 - mean_squared_logarithmic_error: 9.0818e-06The average loss for epoch 138 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.0554e-06 - mean_squared_logarithmic_error: 9.0554e-06 - val_loss: 1.8802e-04 - val_mean_squared_logarithmic_error: 1.8802e-04\n",
      "\n",
      "Epoch 00139: Learning rate is 0.0010.\n",
      "Epoch 140/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 7.7727e-06 - mean_squared_logarithmic_error: 7.7727e-06The average loss for epoch 139 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.7664e-06 - mean_squared_logarithmic_error: 7.7664e-06 - val_loss: 1.8874e-04 - val_mean_squared_logarithmic_error: 1.8874e-04\n",
      "\n",
      "Epoch 00140: Learning rate is 0.0010.\n",
      "Epoch 141/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 6.9891e-06 - mean_squared_logarithmic_error: 6.9891e-06 ETA: 0s - loss: 7.1450e-06 - mean_squared_logarithmic_error: 7. - ETA: 0s - loss: 6.9876e-06 - mean_squared_logarithmic_error: 6.9876e-06The average loss for epoch 140 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.9911e-06 - mean_squared_logarithmic_error: 6.9911e-06 - val_loss: 1.9429e-04 - val_mean_squared_logarithmic_error: 1.9429e-04\n",
      "\n",
      "Epoch 00141: Learning rate is 0.0010.\n",
      "Epoch 142/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 8.1918e-06 - mean_squared_logarithmic_error: 8.1918e-06The average loss for epoch 141 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.2182e-06 - mean_squared_logarithmic_error: 8.2182e-06 - val_loss: 1.7778e-04 - val_mean_squared_logarithmic_error: 1.7778e-04\n",
      "\n",
      "Epoch 00142: Learning rate is 0.0010.\n",
      "Epoch 143/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 7.7092e-06 - mean_squared_logarithmic_error: 7.7092e-06 ETA: 0s - loss: 7.5972e-06 - mean_squared_logarithmic_error: 7.5972e-The average loss for epoch 142 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.6920e-06 - mean_squared_logarithmic_error: 7.6920e-06 - val_loss: 1.8341e-04 - val_mean_squared_logarithmic_error: 1.8341e-04\n",
      "\n",
      "Epoch 00143: Learning rate is 0.0010.\n",
      "Epoch 144/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 9.0376e-06 - mean_squared_logarithmic_error: 9.0376e-06The average loss for epoch 143 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.8735e-06 - mean_squared_logarithmic_error: 8.8735e-06 - val_loss: 1.7734e-04 - val_mean_squared_logarithmic_error: 1.7734e-04\n",
      "\n",
      "Epoch 00144: Learning rate is 0.0010.\n",
      "Epoch 145/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 9.5684e-06 - mean_squared_logarithmic_error: 9.5684e-06The average loss for epoch 144 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.7918e-06 - mean_squared_logarithmic_error: 9.7918e-06 - val_loss: 1.9218e-04 - val_mean_squared_logarithmic_error: 1.9218e-04\n",
      "\n",
      "Epoch 00145: Learning rate is 0.0010.\n",
      "Epoch 146/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 8.5516e-06 - mean_squared_logarithmic_error: 8.5516e-06The average loss for epoch 145 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.5714e-06 - mean_squared_logarithmic_error: 8.5714e-06 - val_loss: 1.7451e-04 - val_mean_squared_logarithmic_error: 1.7451e-04\n",
      "\n",
      "Epoch 00146: Learning rate is 0.0010.\n",
      "Epoch 147/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 8.8179e-06 - mean_squared_logarithmic_error: 8.8179e-06The average loss for epoch 146 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.8007e-06 - mean_squared_logarithmic_error: 8.8007e-06 - val_loss: 1.7938e-04 - val_mean_squared_logarithmic_error: 1.7938e-04\n",
      "\n",
      "Epoch 00147: Learning rate is 0.0010.\n",
      "Epoch 148/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 7.9782e-06 - mean_squared_logarithmic_error: 7.9782e-06The average loss for epoch 147 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.9503e-06 - mean_squared_logarithmic_error: 7.9503e-06 - val_loss: 1.8515e-04 - val_mean_squared_logarithmic_error: 1.8515e-04\n",
      "\n",
      "Epoch 00148: Learning rate is 0.0010.\n",
      "Epoch 149/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 6.4488e-06 - mean_squared_logarithmic_error: 6.4488e-06The average loss for epoch 148 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.4516e-06 - mean_squared_logarithmic_error: 6.4516e-06 - val_loss: 1.7546e-04 - val_mean_squared_logarithmic_error: 1.7546e-04\n",
      "\n",
      "Epoch 00149: Learning rate is 0.0010.\n",
      "Epoch 150/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 6.0449e-06 - mean_squared_logarithmic_error: 6.0449e-06The average loss for epoch 149 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.0333e-06 - mean_squared_logarithmic_error: 6.0333e-06 - val_loss: 1.8593e-04 - val_mean_squared_logarithmic_error: 1.8593e-04\n",
      "\n",
      "Epoch 00150: Learning rate is 0.0010.\n",
      "Epoch 151/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 6.4641e-06 - mean_squared_logarithmic_error: 6.4641e-06The average loss for epoch 150 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.4081e-06 - mean_squared_logarithmic_error: 6.4081e-06 - val_loss: 1.7257e-04 - val_mean_squared_logarithmic_error: 1.7257e-04\n",
      "\n",
      "Epoch 00151: Learning rate is 0.0010.\n",
      "Epoch 152/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 8.8020e-06 - mean_squared_logarithmic_error: 8.8020e-06The average loss for epoch 151 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.8279e-06 - mean_squared_logarithmic_error: 8.8279e-06 - val_loss: 1.8911e-04 - val_mean_squared_logarithmic_error: 1.8911e-04\n",
      "\n",
      "Epoch 00152: Learning rate is 0.0010.\n",
      "Epoch 153/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 1.0666e-05 - mean_squared_logarithmic_error: 1.0666e-05The average loss for epoch 152 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0465e-05 - mean_squared_logarithmic_error: 1.0465e-05 - val_loss: 1.8486e-04 - val_mean_squared_logarithmic_error: 1.8486e-04\n",
      "\n",
      "Epoch 00153: Learning rate is 0.0010.\n",
      "Epoch 154/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 8.3521e-06 - mean_squared_logarithmic_error: 8.3521e-06The average loss for epoch 153 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.5271e-06 - mean_squared_logarithmic_error: 8.5271e-06 - val_loss: 1.9154e-04 - val_mean_squared_logarithmic_error: 1.9154e-04\n",
      "\n",
      "Epoch 00154: Learning rate is 0.0010.\n",
      "Epoch 155/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.0946e-05 - mean_squared_logarithmic_error: 1.0946e-05The average loss for epoch 154 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 1.0895e-05 - mean_squared_logarithmic_error: 1.0895e-05 - val_loss: 1.7114e-04 - val_mean_squared_logarithmic_error: 1.7114e-04\n",
      "\n",
      "Epoch 00155: Learning rate is 0.0010.\n",
      "Epoch 156/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 7.8049e-06 - mean_squared_logarithmic_error: 7.8049e-06The average loss for epoch 155 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.8915e-06 - mean_squared_logarithmic_error: 7.8915e-06 - val_loss: 1.6488e-04 - val_mean_squared_logarithmic_error: 1.6488e-04\n",
      "\n",
      "Epoch 00156: Learning rate is 0.0010.\n",
      "Epoch 157/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 7.1962e-06 - mean_squared_logarithmic_error: 7.1962e-06The average loss for epoch 156 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.1918e-06 - mean_squared_logarithmic_error: 7.1918e-06 - val_loss: 1.6993e-04 - val_mean_squared_logarithmic_error: 1.6993e-04\n",
      "\n",
      "Epoch 00157: Learning rate is 0.0010.\n",
      "Epoch 158/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/264 [==========================>...] - ETA: 0s - loss: 6.2936e-06 - mean_squared_logarithmic_error: 6.2936e-06The average loss for epoch 157 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.3171e-06 - mean_squared_logarithmic_error: 6.3171e-06 - val_loss: 1.6991e-04 - val_mean_squared_logarithmic_error: 1.6991e-04\n",
      "\n",
      "Epoch 00158: Learning rate is 0.0010.\n",
      "Epoch 159/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 5.1240e-06 - mean_squared_logarithmic_error: 5.1240e-06The average loss for epoch 158 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.1736e-06 - mean_squared_logarithmic_error: 5.1736e-06 - val_loss: 1.7601e-04 - val_mean_squared_logarithmic_error: 1.7601e-04\n",
      "\n",
      "Epoch 00159: Learning rate is 0.0010.\n",
      "Epoch 160/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 5.4589e-06 - mean_squared_logarithmic_error: 5.4589e-06The average loss for epoch 159 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.4749e-06 - mean_squared_logarithmic_error: 5.4749e-06 - val_loss: 1.6526e-04 - val_mean_squared_logarithmic_error: 1.6526e-04\n",
      "\n",
      "Epoch 00160: Learning rate is 0.0010.\n",
      "Epoch 161/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 7.1205e-06 - mean_squared_logarithmic_error: 7.1205e-06The average loss for epoch 160 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.0166e-06 - mean_squared_logarithmic_error: 7.0166e-06 - val_loss: 1.7137e-04 - val_mean_squared_logarithmic_error: 1.7137e-04\n",
      "\n",
      "Epoch 00161: Learning rate is 0.0010.\n",
      "Epoch 162/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 8.2146e-06 - mean_squared_logarithmic_error: 8.2146e-06The average loss for epoch 161 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.2257e-06 - mean_squared_logarithmic_error: 8.2257e-06 - val_loss: 1.7048e-04 - val_mean_squared_logarithmic_error: 1.7048e-04\n",
      "\n",
      "Epoch 00162: Learning rate is 0.0010.\n",
      "Epoch 163/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 8.6585e-06 - mean_squared_logarithmic_error: 8.6585e-06The average loss for epoch 162 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.5406e-06 - mean_squared_logarithmic_error: 8.5406e-06 - val_loss: 1.7786e-04 - val_mean_squared_logarithmic_error: 1.7786e-04\n",
      "\n",
      "Epoch 00163: Learning rate is 0.0010.\n",
      "Epoch 164/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 8.6106e-06 - mean_squared_logarithmic_error: 8.6106e-06The average loss for epoch 163 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.5443e-06 - mean_squared_logarithmic_error: 8.5443e-06 - val_loss: 1.8157e-04 - val_mean_squared_logarithmic_error: 1.8157e-04\n",
      "\n",
      "Epoch 00164: Learning rate is 0.0010.\n",
      "Epoch 165/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 8.3259e-06 - mean_squared_logarithmic_error: 8.3259e-06The average loss for epoch 164 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.2253e-06 - mean_squared_logarithmic_error: 8.2253e-06 - val_loss: 1.6643e-04 - val_mean_squared_logarithmic_error: 1.6643e-04\n",
      "\n",
      "Epoch 00165: Learning rate is 0.0010.\n",
      "Epoch 166/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 7.1975e-06 - mean_squared_logarithmic_error: 7.1975e-06The average loss for epoch 165 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.1169e-06 - mean_squared_logarithmic_error: 7.1169e-06 - val_loss: 1.6297e-04 - val_mean_squared_logarithmic_error: 1.6297e-04\n",
      "\n",
      "Epoch 00166: Learning rate is 0.0010.\n",
      "Epoch 167/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 5.5982e-06 - mean_squared_logarithmic_error: 5.5982e-06The average loss for epoch 166 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.5795e-06 - mean_squared_logarithmic_error: 5.5795e-06 - val_loss: 1.5960e-04 - val_mean_squared_logarithmic_error: 1.5960e-04\n",
      "\n",
      "Epoch 00167: Learning rate is 0.0010.\n",
      "Epoch 168/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 5.6534e-06 - mean_squared_logarithmic_error: 5.6534e-06The average loss for epoch 167 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.6170e-06 - mean_squared_logarithmic_error: 5.6170e-06 - val_loss: 1.6657e-04 - val_mean_squared_logarithmic_error: 1.6657e-04\n",
      "\n",
      "Epoch 00168: Learning rate is 0.0010.\n",
      "Epoch 169/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 7.1088e-06 - mean_squared_logarithmic_error: 7.1088e-06The average loss for epoch 168 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.0806e-06 - mean_squared_logarithmic_error: 7.0806e-06 - val_loss: 1.7716e-04 - val_mean_squared_logarithmic_error: 1.7716e-04\n",
      "\n",
      "Epoch 00169: Learning rate is 0.0010.\n",
      "Epoch 170/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 7.1158e-06 - mean_squared_logarithmic_error: 7.1158e-06The average loss for epoch 169 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.0879e-06 - mean_squared_logarithmic_error: 7.0879e-06 - val_loss: 1.6698e-04 - val_mean_squared_logarithmic_error: 1.6698e-04\n",
      "\n",
      "Epoch 00170: Learning rate is 0.0010.\n",
      "Epoch 171/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 9.3593e-06 - mean_squared_logarithmic_error: 9.3593e-06The average loss for epoch 170 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.1222e-06 - mean_squared_logarithmic_error: 9.1222e-06 - val_loss: 1.7002e-04 - val_mean_squared_logarithmic_error: 1.7002e-04\n",
      "\n",
      "Epoch 00171: Learning rate is 0.0010.\n",
      "Epoch 172/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 8.9080e-06 - mean_squared_logarithmic_error: 8.9080e-06The average loss for epoch 171 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.8841e-06 - mean_squared_logarithmic_error: 8.8841e-06 - val_loss: 1.6789e-04 - val_mean_squared_logarithmic_error: 1.6789e-04\n",
      "\n",
      "Epoch 00172: Learning rate is 0.0010.\n",
      "Epoch 173/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 6.6764e-06 - mean_squared_logarithmic_error: 6.6764e-06The average loss for epoch 172 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.7161e-06 - mean_squared_logarithmic_error: 6.7161e-06 - val_loss: 1.6447e-04 - val_mean_squared_logarithmic_error: 1.6447e-04\n",
      "\n",
      "Epoch 00173: Learning rate is 0.0010.\n",
      "Epoch 174/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 7.0565e-06 - mean_squared_logarithmic_error: 7.0565e-06The average loss for epoch 173 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.9845e-06 - mean_squared_logarithmic_error: 6.9845e-06 - val_loss: 1.6840e-04 - val_mean_squared_logarithmic_error: 1.6840e-04\n",
      "\n",
      "Epoch 00174: Learning rate is 0.0010.\n",
      "Epoch 175/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 6.6617e-06 - mean_squared_logarithmic_error: 6.6617e-06The average loss for epoch 174 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.5909e-06 - mean_squared_logarithmic_error: 6.5909e-06 - val_loss: 1.6559e-04 - val_mean_squared_logarithmic_error: 1.6559e-04\n",
      "\n",
      "Epoch 00175: Learning rate is 0.0010.\n",
      "Epoch 176/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 5.6977e-06 - mean_squared_logarithmic_error: 5.6977e-06The average loss for epoch 175 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.6432e-06 - mean_squared_logarithmic_error: 5.6432e-06 - val_loss: 1.5688e-04 - val_mean_squared_logarithmic_error: 1.5688e-04\n",
      "\n",
      "Epoch 00176: Learning rate is 0.0010.\n",
      "Epoch 177/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 4.8722e-06 - mean_squared_logarithmic_error: 4.8722e-06The average loss for epoch 176 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.8331e-06 - mean_squared_logarithmic_error: 4.8331e-06 - val_loss: 1.5710e-04 - val_mean_squared_logarithmic_error: 1.5710e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00177: Learning rate is 0.0010.\n",
      "Epoch 178/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 4.5011e-06 - mean_squared_logarithmic_error: 4.5011e-06The average loss for epoch 177 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.4817e-06 - mean_squared_logarithmic_error: 4.4817e-06 - val_loss: 1.5774e-04 - val_mean_squared_logarithmic_error: 1.5774e-04\n",
      "\n",
      "Epoch 00178: Learning rate is 0.0010.\n",
      "Epoch 179/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 6.1553e-06 - mean_squared_logarithmic_error: 6.1553e-06The average loss for epoch 178 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 6.0966e-06 - mean_squared_logarithmic_error: 6.0966e-06 - val_loss: 1.6103e-04 - val_mean_squared_logarithmic_error: 1.6103e-04\n",
      "\n",
      "Epoch 00179: Learning rate is 0.0010.\n",
      "Epoch 180/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 8.4585e-06 - mean_squared_logarithmic_error: 8.4585e-06The average loss for epoch 179 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.4066e-06 - mean_squared_logarithmic_error: 8.4066e-06 - val_loss: 1.6177e-04 - val_mean_squared_logarithmic_error: 1.6177e-04\n",
      "\n",
      "Epoch 00180: Learning rate is 0.0010.\n",
      "Epoch 181/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.0800e-05 - mean_squared_logarithmic_error: 1.0800e-05The average loss for epoch 180 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.0763e-05 - mean_squared_logarithmic_error: 1.0763e-05 - val_loss: 1.5616e-04 - val_mean_squared_logarithmic_error: 1.5616e-04\n",
      "\n",
      "Epoch 00181: Learning rate is 0.0010.\n",
      "Epoch 182/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 9.8572e-06 - mean_squared_logarithmic_error: 9.8572e-06The average loss for epoch 181 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 9.6244e-06 - mean_squared_logarithmic_error: 9.6244e-06 - val_loss: 1.6003e-04 - val_mean_squared_logarithmic_error: 1.6003e-04\n",
      "\n",
      "Epoch 00182: Learning rate is 0.0010.\n",
      "Epoch 183/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 7.6722e-06 - mean_squared_logarithmic_error: 7.6722e-06The average loss for epoch 182 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.5963e-06 - mean_squared_logarithmic_error: 7.5963e-06 - val_loss: 1.5645e-04 - val_mean_squared_logarithmic_error: 1.5645e-04\n",
      "\n",
      "Epoch 00183: Learning rate is 0.0010.\n",
      "Epoch 184/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 5.2673e-06 - mean_squared_logarithmic_error: 5.2673e-06The average loss for epoch 183 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.2400e-06 - mean_squared_logarithmic_error: 5.2400e-06 - val_loss: 1.5843e-04 - val_mean_squared_logarithmic_error: 1.5843e-04\n",
      "\n",
      "Epoch 00184: Learning rate is 0.0010.\n",
      "Epoch 185/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 4.2016e-06 - mean_squared_logarithmic_error: 4.2016e-06The average loss for epoch 184 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.2162e-06 - mean_squared_logarithmic_error: 4.2162e-06 - val_loss: 1.5230e-04 - val_mean_squared_logarithmic_error: 1.5230e-04\n",
      "\n",
      "Epoch 00185: Learning rate is 0.0010.\n",
      "Epoch 186/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 3.9286e-06 - mean_squared_logarithmic_error: 3.9286e-06The average loss for epoch 185 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.9416e-06 - mean_squared_logarithmic_error: 3.9416e-06 - val_loss: 1.6282e-04 - val_mean_squared_logarithmic_error: 1.6282e-04\n",
      "\n",
      "Epoch 00186: Learning rate is 0.0010.\n",
      "Epoch 187/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 4.1795e-06 - mean_squared_logarithmic_error: 4.1795e-06The average loss for epoch 186 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.1805e-06 - mean_squared_logarithmic_error: 4.1805e-06 - val_loss: 1.7275e-04 - val_mean_squared_logarithmic_error: 1.7275e-04\n",
      "\n",
      "Epoch 00187: Learning rate is 0.0010.\n",
      "Epoch 188/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 5.3004e-06 - mean_squared_logarithmic_error: 5.3004e-06The average loss for epoch 187 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 5.3159e-06 - mean_squared_logarithmic_error: 5.3159e-06 - val_loss: 1.5521e-04 - val_mean_squared_logarithmic_error: 1.5521e-04\n",
      "\n",
      "Epoch 00188: Learning rate is 0.0010.\n",
      "Epoch 189/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 6.3402e-06 - mean_squared_logarithmic_error: 6.3402e-06The average loss for epoch 188 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.3084e-06 - mean_squared_logarithmic_error: 6.3084e-06 - val_loss: 1.5177e-04 - val_mean_squared_logarithmic_error: 1.5177e-04\n",
      "\n",
      "Epoch 00189: Learning rate is 0.0010.\n",
      "Epoch 190/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 6.7202e-06 - mean_squared_logarithmic_error: 6.7202e-06The average loss for epoch 189 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.7407e-06 - mean_squared_logarithmic_error: 6.7407e-06 - val_loss: 1.6281e-04 - val_mean_squared_logarithmic_error: 1.6281e-04\n",
      "\n",
      "Epoch 00190: Learning rate is 0.0010.\n",
      "Epoch 191/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 8.1272e-06 - mean_squared_logarithmic_error: 8.1272e-06The average loss for epoch 190 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 8.1098e-06 - mean_squared_logarithmic_error: 8.1098e-06 - val_loss: 1.5611e-04 - val_mean_squared_logarithmic_error: 1.5611e-04\n",
      "\n",
      "Epoch 00191: Learning rate is 0.0010.\n",
      "Epoch 192/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 7.6346e-06 - mean_squared_logarithmic_error: 7.6346e-06The average loss for epoch 191 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.5948e-06 - mean_squared_logarithmic_error: 7.5948e-06 - val_loss: 1.6087e-04 - val_mean_squared_logarithmic_error: 1.6087e-04\n",
      "\n",
      "Epoch 00192: Learning rate is 0.0010.\n",
      "Epoch 193/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 7.1570e-06 - mean_squared_logarithmic_error: 7.1570e-06The average loss for epoch 192 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 7.1563e-06 - mean_squared_logarithmic_error: 7.1563e-06 - val_loss: 1.6005e-04 - val_mean_squared_logarithmic_error: 1.6005e-04\n",
      "\n",
      "Epoch 00193: Learning rate is 0.0010.\n",
      "Epoch 194/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 6.6798e-06 - mean_squared_logarithmic_error: 6.6798e-06The average loss for epoch 193 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.6623e-06 - mean_squared_logarithmic_error: 6.6623e-06 - val_loss: 1.6262e-04 - val_mean_squared_logarithmic_error: 1.6262e-04\n",
      "\n",
      "Epoch 00194: Learning rate is 0.0010.\n",
      "Epoch 195/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 5.3527e-06 - mean_squared_logarithmic_error: 5.3527e-06The average loss for epoch 194 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.3096e-06 - mean_squared_logarithmic_error: 5.3096e-06 - val_loss: 1.5229e-04 - val_mean_squared_logarithmic_error: 1.5229e-04\n",
      "\n",
      "Epoch 00195: Learning rate is 0.0010.\n",
      "Epoch 196/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 4.8284e-06 - mean_squared_logarithmic_error: 4.8284e-06The average loss for epoch 195 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.8378e-06 - mean_squared_logarithmic_error: 4.8378e-06 - val_loss: 1.4941e-04 - val_mean_squared_logarithmic_error: 1.4941e-04\n",
      "\n",
      "Epoch 00196: Learning rate is 0.0010.\n",
      "Epoch 197/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 4.4229e-06 - mean_squared_logarithmic_error: 4.4229e-06The average loss for epoch 196 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.3760e-06 - mean_squared_logarithmic_error: 4.3760e-06 - val_loss: 1.5198e-04 - val_mean_squared_logarithmic_error: 1.5198e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00197: Learning rate is 0.0010.\n",
      "Epoch 198/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 5.3169e-06 - mean_squared_logarithmic_error: 5.3169e-06The average loss for epoch 197 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.2363e-06 - mean_squared_logarithmic_error: 5.2363e-06 - val_loss: 1.5615e-04 - val_mean_squared_logarithmic_error: 1.5615e-04\n",
      "\n",
      "Epoch 00198: Learning rate is 0.0010.\n",
      "Epoch 199/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 4.7139e-06 - mean_squared_logarithmic_error: 4.7139e-06The average loss for epoch 198 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 4.7059e-06 - mean_squared_logarithmic_error: 4.7059e-06 - val_loss: 1.6134e-04 - val_mean_squared_logarithmic_error: 1.6134e-04\n",
      "\n",
      "Epoch 00199: Learning rate is 0.0010.\n",
      "Epoch 200/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 6.3147e-06 - mean_squared_logarithmic_error: 6.3147e-06The average loss for epoch 199 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 6.2877e-06 - mean_squared_logarithmic_error: 6.2877e-06 - val_loss: 1.5344e-04 - val_mean_squared_logarithmic_error: 1.5344e-04\n",
      "\n",
      "Epoch 00200: Learning rate is 0.0001.\n",
      "Epoch 201/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 3.1022e-06 - mean_squared_logarithmic_error: 3.1022e-06The average loss for epoch 200 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.0590e-06 - mean_squared_logarithmic_error: 3.0590e-06 - val_loss: 1.5219e-04 - val_mean_squared_logarithmic_error: 1.5219e-04\n",
      "\n",
      "Epoch 00201: Learning rate is 0.0001.\n",
      "Epoch 202/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.0054e-06 - mean_squared_logarithmic_error: 2.0054e-06The average loss for epoch 201 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.0068e-06 - mean_squared_logarithmic_error: 2.0068e-06 - val_loss: 1.5095e-04 - val_mean_squared_logarithmic_error: 1.5095e-04\n",
      "\n",
      "Epoch 00202: Learning rate is 0.0001.\n",
      "Epoch 203/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5682e-06 - mean_squared_logarithmic_error: 1.5682e-06 ETA: 0s - loss: 1.5891e-06 - mean_squared_logarithmic_error: The average loss for epoch 202 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5642e-06 - mean_squared_logarithmic_error: 1.5642e-06 - val_loss: 1.5023e-04 - val_mean_squared_logarithmic_error: 1.5023e-04\n",
      "\n",
      "Epoch 00203: Learning rate is 0.0001.\n",
      "Epoch 204/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.2892e-06 - mean_squared_logarithmic_error: 1.2892e-06The average loss for epoch 203 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.2926e-06 - mean_squared_logarithmic_error: 1.2926e-06 - val_loss: 1.4846e-04 - val_mean_squared_logarithmic_error: 1.4846e-04\n",
      "\n",
      "Epoch 00204: Learning rate is 0.0001.\n",
      "Epoch 205/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.1035e-06 - mean_squared_logarithmic_error: 1.1035e-06The average loss for epoch 204 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.1035e-06 - mean_squared_logarithmic_error: 1.1035e-06 - val_loss: 1.5130e-04 - val_mean_squared_logarithmic_error: 1.5130e-04\n",
      "\n",
      "Epoch 00205: Learning rate is 0.0001.\n",
      "Epoch 206/800\n",
      "243/264 [==========================>...] - ETA: 0s - loss: 9.7432e-07 - mean_squared_logarithmic_error: 9.7432e-07The average loss for epoch 205 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 9.7033e-07 - mean_squared_logarithmic_error: 9.7033e-07 - val_loss: 1.5007e-04 - val_mean_squared_logarithmic_error: 1.5007e-04\n",
      "\n",
      "Epoch 00206: Learning rate is 0.0001.\n",
      "Epoch 207/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 8.4214e-07 - mean_squared_logarithmic_error: 8.4214e-07The average loss for epoch 206 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 8.5893e-07 - mean_squared_logarithmic_error: 8.5893e-07 - val_loss: 1.4980e-04 - val_mean_squared_logarithmic_error: 1.4980e-04\n",
      "\n",
      "Epoch 00207: Learning rate is 0.0001.\n",
      "Epoch 208/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 7.7242e-07 - mean_squared_logarithmic_error: 7.7242e-07The average loss for epoch 207 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.7016e-07 - mean_squared_logarithmic_error: 7.7016e-07 - val_loss: 1.5025e-04 - val_mean_squared_logarithmic_error: 1.5025e-04\n",
      "\n",
      "Epoch 00208: Learning rate is 0.0001.\n",
      "Epoch 209/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 6.9831e-07 - mean_squared_logarithmic_error: 6.9831e-07The average loss for epoch 208 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 7.0033e-07 - mean_squared_logarithmic_error: 7.0033e-07 - val_loss: 1.5069e-04 - val_mean_squared_logarithmic_error: 1.5069e-04\n",
      "\n",
      "Epoch 00209: Learning rate is 0.0001.\n",
      "Epoch 210/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 6.4154e-07 - mean_squared_logarithmic_error: 6.4154e-07The average loss for epoch 209 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 6.4209e-07 - mean_squared_logarithmic_error: 6.4209e-07 - val_loss: 1.4954e-04 - val_mean_squared_logarithmic_error: 1.4954e-04\n",
      "\n",
      "Epoch 00210: Learning rate is 0.0001.\n",
      "Epoch 211/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 5.9734e-07 - mean_squared_logarithmic_error: 5.9734e-07The average loss for epoch 210 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.9813e-07 - mean_squared_logarithmic_error: 5.9813e-07 - val_loss: 1.5035e-04 - val_mean_squared_logarithmic_error: 1.5035e-04\n",
      "\n",
      "Epoch 00211: Learning rate is 0.0001.\n",
      "Epoch 212/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 5.5787e-07 - mean_squared_logarithmic_error: 5.5787e-07The average loss for epoch 211 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.5802e-07 - mean_squared_logarithmic_error: 5.5802e-07 - val_loss: 1.4954e-04 - val_mean_squared_logarithmic_error: 1.4954e-04\n",
      "\n",
      "Epoch 00212: Learning rate is 0.0001.\n",
      "Epoch 213/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 5.2866e-07 - mean_squared_logarithmic_error: 5.2866e-07The average loss for epoch 212 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.2670e-07 - mean_squared_logarithmic_error: 5.2670e-07 - val_loss: 1.5050e-04 - val_mean_squared_logarithmic_error: 1.5050e-04\n",
      "\n",
      "Epoch 00213: Learning rate is 0.0001.\n",
      "Epoch 214/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 5.0303e-07 - mean_squared_logarithmic_error: 5.0303e-07The average loss for epoch 213 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 5.0238e-07 - mean_squared_logarithmic_error: 5.0238e-07 - val_loss: 1.5084e-04 - val_mean_squared_logarithmic_error: 1.5084e-04\n",
      "\n",
      "Epoch 00214: Learning rate is 0.0001.\n",
      "Epoch 215/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 4.7331e-07 - mean_squared_logarithmic_error: 4.7331e-07The average loss for epoch 214 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.7390e-07 - mean_squared_logarithmic_error: 4.7390e-07 - val_loss: 1.4998e-04 - val_mean_squared_logarithmic_error: 1.4998e-04\n",
      "\n",
      "Epoch 00215: Learning rate is 0.0001.\n",
      "Epoch 216/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 4.5736e-07 - mean_squared_logarithmic_error: 4.5736e-07The average loss for epoch 215 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.5565e-07 - mean_squared_logarithmic_error: 4.5565e-07 - val_loss: 1.5075e-04 - val_mean_squared_logarithmic_error: 1.5075e-04\n",
      "\n",
      "Epoch 00216: Learning rate is 0.0001.\n",
      "Epoch 217/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/264 [===========================>..] - ETA: 0s - loss: 4.3817e-07 - mean_squared_logarithmic_error: 4.3817e-07The average loss for epoch 216 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.3867e-07 - mean_squared_logarithmic_error: 4.3867e-07 - val_loss: 1.5117e-04 - val_mean_squared_logarithmic_error: 1.5117e-04\n",
      "\n",
      "Epoch 00217: Learning rate is 0.0001.\n",
      "Epoch 218/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 4.3645e-07 - mean_squared_logarithmic_error: 4.3645e-07The average loss for epoch 217 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.3538e-07 - mean_squared_logarithmic_error: 4.3538e-07 - val_loss: 1.5082e-04 - val_mean_squared_logarithmic_error: 1.5082e-04\n",
      "\n",
      "Epoch 00218: Learning rate is 0.0001.\n",
      "Epoch 219/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 4.0820e-07 - mean_squared_logarithmic_error: 4.0820e-07The average loss for epoch 218 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.0859e-07 - mean_squared_logarithmic_error: 4.0859e-07 - val_loss: 1.5115e-04 - val_mean_squared_logarithmic_error: 1.5115e-04\n",
      "\n",
      "Epoch 00219: Learning rate is 0.0001.\n",
      "Epoch 220/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 4.0954e-07 - mean_squared_logarithmic_error: 4.0954e-07The average loss for epoch 219 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 4.0880e-07 - mean_squared_logarithmic_error: 4.0880e-07 - val_loss: 1.5117e-04 - val_mean_squared_logarithmic_error: 1.5117e-04\n",
      "\n",
      "Epoch 00220: Learning rate is 0.0001.\n",
      "Epoch 221/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 3.9160e-07 - mean_squared_logarithmic_error: 3.9160e-07The average loss for epoch 220 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.9165e-07 - mean_squared_logarithmic_error: 3.9165e-07 - val_loss: 1.5073e-04 - val_mean_squared_logarithmic_error: 1.5073e-04\n",
      "\n",
      "Epoch 00221: Learning rate is 0.0001.\n",
      "Epoch 222/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 3.6946e-07 - mean_squared_logarithmic_error: 3.6946e-07The average loss for epoch 221 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.8648e-07 - mean_squared_logarithmic_error: 3.8648e-07 - val_loss: 1.5107e-04 - val_mean_squared_logarithmic_error: 1.5107e-04\n",
      "\n",
      "Epoch 00222: Learning rate is 0.0001.\n",
      "Epoch 223/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 3.8167e-07 - mean_squared_logarithmic_error: 3.8167e-07The average loss for epoch 222 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.8127e-07 - mean_squared_logarithmic_error: 3.8127e-07 - val_loss: 1.5068e-04 - val_mean_squared_logarithmic_error: 1.5068e-04\n",
      "\n",
      "Epoch 00223: Learning rate is 0.0001.\n",
      "Epoch 224/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 3.7097e-07 - mean_squared_logarithmic_error: 3.7097e-07The average loss for epoch 223 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.7097e-07 - mean_squared_logarithmic_error: 3.7097e-07 - val_loss: 1.5147e-04 - val_mean_squared_logarithmic_error: 1.5147e-04\n",
      "\n",
      "Epoch 00224: Learning rate is 0.0001.\n",
      "Epoch 225/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 3.5825e-07 - mean_squared_logarithmic_error: 3.5825e-07The average loss for epoch 224 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.5828e-07 - mean_squared_logarithmic_error: 3.5828e-07 - val_loss: 1.5082e-04 - val_mean_squared_logarithmic_error: 1.5082e-04\n",
      "\n",
      "Epoch 00225: Learning rate is 0.0001.\n",
      "Epoch 226/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 3.6254e-07 - mean_squared_logarithmic_error: 3.6254e-07The average loss for epoch 225 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.6312e-07 - mean_squared_logarithmic_error: 3.6312e-07 - val_loss: 1.5110e-04 - val_mean_squared_logarithmic_error: 1.5110e-04\n",
      "\n",
      "Epoch 00226: Learning rate is 0.0001.\n",
      "Epoch 227/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 3.4955e-07 - mean_squared_logarithmic_error: 3.4955e-07The average loss for epoch 226 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.4955e-07 - mean_squared_logarithmic_error: 3.4955e-07 - val_loss: 1.5169e-04 - val_mean_squared_logarithmic_error: 1.5169e-04\n",
      "\n",
      "Epoch 00227: Learning rate is 0.0001.\n",
      "Epoch 228/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 3.5508e-07 - mean_squared_logarithmic_error: 3.5508e-07The average loss for epoch 227 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.5508e-07 - mean_squared_logarithmic_error: 3.5508e-07 - val_loss: 1.5136e-04 - val_mean_squared_logarithmic_error: 1.5136e-04\n",
      "\n",
      "Epoch 00228: Learning rate is 0.0001.\n",
      "Epoch 229/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 3.4749e-07 - mean_squared_logarithmic_error: 3.4749e-07The average loss for epoch 228 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.4866e-07 - mean_squared_logarithmic_error: 3.4866e-07 - val_loss: 1.5138e-04 - val_mean_squared_logarithmic_error: 1.5138e-04\n",
      "\n",
      "Epoch 00229: Learning rate is 0.0001.\n",
      "Epoch 230/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 3.3480e-07 - mean_squared_logarithmic_error: 3.3480e-07The average loss for epoch 229 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.3631e-07 - mean_squared_logarithmic_error: 3.3631e-07 - val_loss: 1.5204e-04 - val_mean_squared_logarithmic_error: 1.5204e-04\n",
      "\n",
      "Epoch 00230: Learning rate is 0.0001.\n",
      "Epoch 231/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 3.3360e-07 - mean_squared_logarithmic_error: 3.3360e-07The average loss for epoch 230 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.3360e-07 - mean_squared_logarithmic_error: 3.3360e-07 - val_loss: 1.5175e-04 - val_mean_squared_logarithmic_error: 1.5175e-04\n",
      "\n",
      "Epoch 00231: Learning rate is 0.0001.\n",
      "Epoch 232/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 3.3793e-07 - mean_squared_logarithmic_error: 3.3793e-07The average loss for epoch 231 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.3754e-07 - mean_squared_logarithmic_error: 3.3754e-07 - val_loss: 1.5162e-04 - val_mean_squared_logarithmic_error: 1.5162e-04\n",
      "\n",
      "Epoch 00232: Learning rate is 0.0001.\n",
      "Epoch 233/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 3.2875e-07 - mean_squared_logarithmic_error: 3.2875e-07The average loss for epoch 232 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.2967e-07 - mean_squared_logarithmic_error: 3.2967e-07 - val_loss: 1.5100e-04 - val_mean_squared_logarithmic_error: 1.5100e-04\n",
      "\n",
      "Epoch 00233: Learning rate is 0.0001.\n",
      "Epoch 234/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 3.2410e-07 - mean_squared_logarithmic_error: 3.2410e-07The average loss for epoch 233 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.2410e-07 - mean_squared_logarithmic_error: 3.2410e-07 - val_loss: 1.5127e-04 - val_mean_squared_logarithmic_error: 1.5127e-04\n",
      "\n",
      "Epoch 00234: Learning rate is 0.0001.\n",
      "Epoch 235/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 3.1530e-07 - mean_squared_logarithmic_error: 3.1530e-07 ETA: 0s - loss: 3.3050e-07 - mean_squared_logarithmic_error: The average loss for epoch 234 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.1391e-07 - mean_squared_logarithmic_error: 3.1391e-07 - val_loss: 1.5137e-04 - val_mean_squared_logarithmic_error: 1.5137e-04\n",
      "\n",
      "Epoch 00235: Learning rate is 0.0001.\n",
      "Epoch 236/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 3.1014e-07 - mean_squared_logarithmic_error: 3.1014e-07The average loss for epoch 235 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.1020e-07 - mean_squared_logarithmic_error: 3.1020e-07 - val_loss: 1.5095e-04 - val_mean_squared_logarithmic_error: 1.5095e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00236: Learning rate is 0.0001.\n",
      "Epoch 237/800\n",
      "241/264 [==========================>...] - ETA: 0s - loss: 3.0816e-07 - mean_squared_logarithmic_error: 3.0816e-07The average loss for epoch 236 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.0727e-07 - mean_squared_logarithmic_error: 3.0727e-07 - val_loss: 1.5171e-04 - val_mean_squared_logarithmic_error: 1.5171e-04\n",
      "\n",
      "Epoch 00237: Learning rate is 0.0001.\n",
      "Epoch 238/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 3.1804e-07 - mean_squared_logarithmic_error: 3.1804e-07The average loss for epoch 237 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 3.1667e-07 - mean_squared_logarithmic_error: 3.1667e-07 - val_loss: 1.5201e-04 - val_mean_squared_logarithmic_error: 1.5201e-04\n",
      "\n",
      "Epoch 00238: Learning rate is 0.0001.\n",
      "Epoch 239/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 3.0620e-07 - mean_squared_logarithmic_error: 3.0620e-07The average loss for epoch 238 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.0683e-07 - mean_squared_logarithmic_error: 3.0683e-07 - val_loss: 1.5088e-04 - val_mean_squared_logarithmic_error: 1.5088e-04\n",
      "\n",
      "Epoch 00239: Learning rate is 0.0001.\n",
      "Epoch 240/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 3.0507e-07 - mean_squared_logarithmic_error: 3.0507e-07The average loss for epoch 239 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.0519e-07 - mean_squared_logarithmic_error: 3.0519e-07 - val_loss: 1.5106e-04 - val_mean_squared_logarithmic_error: 1.5106e-04\n",
      "\n",
      "Epoch 00240: Learning rate is 0.0001.\n",
      "Epoch 241/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 3.0552e-07 - mean_squared_logarithmic_error: 3.0552e-07The average loss for epoch 240 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 3.0521e-07 - mean_squared_logarithmic_error: 3.0521e-07 - val_loss: 1.5147e-04 - val_mean_squared_logarithmic_error: 1.5147e-04\n",
      "\n",
      "Epoch 00241: Learning rate is 0.0001.\n",
      "Epoch 242/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.9743e-07 - mean_squared_logarithmic_error: 2.9743e-07The average loss for epoch 241 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9725e-07 - mean_squared_logarithmic_error: 2.9725e-07 - val_loss: 1.5164e-04 - val_mean_squared_logarithmic_error: 1.5164e-04\n",
      "\n",
      "Epoch 00242: Learning rate is 0.0001.\n",
      "Epoch 243/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.9862e-07 - mean_squared_logarithmic_error: 2.9862e-07The average loss for epoch 242 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9856e-07 - mean_squared_logarithmic_error: 2.9856e-07 - val_loss: 1.5163e-04 - val_mean_squared_logarithmic_error: 1.5163e-04\n",
      "\n",
      "Epoch 00243: Learning rate is 0.0001.\n",
      "Epoch 244/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.9387e-07 - mean_squared_logarithmic_error: 2.9387e-07The average loss for epoch 243 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9469e-07 - mean_squared_logarithmic_error: 2.9469e-07 - val_loss: 1.5193e-04 - val_mean_squared_logarithmic_error: 1.5193e-04\n",
      "\n",
      "Epoch 00244: Learning rate is 0.0001.\n",
      "Epoch 245/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.9276e-07 - mean_squared_logarithmic_error: 2.9276e-07The average loss for epoch 244 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9273e-07 - mean_squared_logarithmic_error: 2.9273e-07 - val_loss: 1.5107e-04 - val_mean_squared_logarithmic_error: 1.5107e-04\n",
      "\n",
      "Epoch 00245: Learning rate is 0.0001.\n",
      "Epoch 246/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.9623e-07 - mean_squared_logarithmic_error: 2.9623e-07The average loss for epoch 245 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9617e-07 - mean_squared_logarithmic_error: 2.9617e-07 - val_loss: 1.5123e-04 - val_mean_squared_logarithmic_error: 1.5123e-04\n",
      "\n",
      "Epoch 00246: Learning rate is 0.0001.\n",
      "Epoch 247/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.9233e-07 - mean_squared_logarithmic_error: 2.9233e-07The average loss for epoch 246 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.9271e-07 - mean_squared_logarithmic_error: 2.9271e-07 - val_loss: 1.5173e-04 - val_mean_squared_logarithmic_error: 1.5173e-04\n",
      "\n",
      "Epoch 00247: Learning rate is 0.0001.\n",
      "Epoch 248/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 2.9272e-07 - mean_squared_logarithmic_error: 2.9272e-07The average loss for epoch 247 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.9143e-07 - mean_squared_logarithmic_error: 2.9143e-07 - val_loss: 1.5209e-04 - val_mean_squared_logarithmic_error: 1.5209e-04\n",
      "\n",
      "Epoch 00248: Learning rate is 0.0001.\n",
      "Epoch 249/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 2.8887e-07 - mean_squared_logarithmic_error: 2.8887e-07The average loss for epoch 248 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.8623e-07 - mean_squared_logarithmic_error: 2.8623e-07 - val_loss: 1.5207e-04 - val_mean_squared_logarithmic_error: 1.5207e-04\n",
      "\n",
      "Epoch 00249: Learning rate is 0.0001.\n",
      "Epoch 250/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.8347e-07 - mean_squared_logarithmic_error: 2.8347e-07The average loss for epoch 249 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.8292e-07 - mean_squared_logarithmic_error: 2.8292e-07 - val_loss: 1.5156e-04 - val_mean_squared_logarithmic_error: 1.5156e-04\n",
      "\n",
      "Epoch 00250: Learning rate is 0.0001.\n",
      "Epoch 251/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.7493e-07 - mean_squared_logarithmic_error: 2.7493e-07The average loss for epoch 250 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.7439e-07 - mean_squared_logarithmic_error: 2.7439e-07 - val_loss: 1.5172e-04 - val_mean_squared_logarithmic_error: 1.5172e-04\n",
      "\n",
      "Epoch 00251: Learning rate is 0.0001.\n",
      "Epoch 252/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.7426e-07 - mean_squared_logarithmic_error: 2.7426e-07The average loss for epoch 251 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.7504e-07 - mean_squared_logarithmic_error: 2.7504e-07 - val_loss: 1.5154e-04 - val_mean_squared_logarithmic_error: 1.5154e-04\n",
      "\n",
      "Epoch 00252: Learning rate is 0.0001.\n",
      "Epoch 253/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.7488e-07 - mean_squared_logarithmic_error: 2.7488e-07The average loss for epoch 252 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.7588e-07 - mean_squared_logarithmic_error: 2.7588e-07 - val_loss: 1.5175e-04 - val_mean_squared_logarithmic_error: 1.5175e-04\n",
      "\n",
      "Epoch 00253: Learning rate is 0.0001.\n",
      "Epoch 254/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 2.8119e-07 - mean_squared_logarithmic_error: 2.8119e-07The average loss for epoch 253 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.8107e-07 - mean_squared_logarithmic_error: 2.8107e-07 - val_loss: 1.5177e-04 - val_mean_squared_logarithmic_error: 1.5177e-04\n",
      "\n",
      "Epoch 00254: Learning rate is 0.0001.\n",
      "Epoch 255/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.7366e-07 - mean_squared_logarithmic_error: 2.7366e-07The average loss for epoch 254 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.7238e-07 - mean_squared_logarithmic_error: 2.7238e-07 - val_loss: 1.5161e-04 - val_mean_squared_logarithmic_error: 1.5161e-04\n",
      "\n",
      "Epoch 00255: Learning rate is 0.0001.\n",
      "Epoch 256/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.7541e-07 - mean_squared_logarithmic_error: 2.7541e-07The average loss for epoch 255 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.7422e-07 - mean_squared_logarithmic_error: 2.7422e-07 - val_loss: 1.5146e-04 - val_mean_squared_logarithmic_error: 1.5146e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00256: Learning rate is 0.0001.\n",
      "Epoch 257/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.6839e-07 - mean_squared_logarithmic_error: 2.6839e-07The average loss for epoch 256 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.6792e-07 - mean_squared_logarithmic_error: 2.6792e-07 - val_loss: 1.5155e-04 - val_mean_squared_logarithmic_error: 1.5155e-04\n",
      "\n",
      "Epoch 00257: Learning rate is 0.0001.\n",
      "Epoch 258/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.7442e-07 - mean_squared_logarithmic_error: 2.7442e-07The average loss for epoch 257 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.7435e-07 - mean_squared_logarithmic_error: 2.7435e-07 - val_loss: 1.5152e-04 - val_mean_squared_logarithmic_error: 1.5152e-04\n",
      "\n",
      "Epoch 00258: Learning rate is 0.0001.\n",
      "Epoch 259/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.7886e-07 - mean_squared_logarithmic_error: 2.7886e-07The average loss for epoch 258 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.7790e-07 - mean_squared_logarithmic_error: 2.7790e-07 - val_loss: 1.5158e-04 - val_mean_squared_logarithmic_error: 1.5158e-04\n",
      "\n",
      "Epoch 00259: Learning rate is 0.0001.\n",
      "Epoch 260/800\n",
      "245/264 [==========================>...] - ETA: 0s - loss: 2.6098e-07 - mean_squared_logarithmic_error: 2.6098e-07The average loss for epoch 259 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5862e-07 - mean_squared_logarithmic_error: 2.5862e-07 - val_loss: 1.5170e-04 - val_mean_squared_logarithmic_error: 1.5170e-04\n",
      "\n",
      "Epoch 00260: Learning rate is 0.0001.\n",
      "Epoch 261/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.5916e-07 - mean_squared_logarithmic_error: 2.5916e-07The average loss for epoch 260 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5897e-07 - mean_squared_logarithmic_error: 2.5897e-07 - val_loss: 1.5197e-04 - val_mean_squared_logarithmic_error: 1.5197e-04\n",
      "\n",
      "Epoch 00261: Learning rate is 0.0001.\n",
      "Epoch 262/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.6612e-07 - mean_squared_logarithmic_error: 2.6612e-07The average loss for epoch 261 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.6532e-07 - mean_squared_logarithmic_error: 2.6532e-07 - val_loss: 1.5156e-04 - val_mean_squared_logarithmic_error: 1.5156e-04\n",
      "\n",
      "Epoch 00262: Learning rate is 0.0001.\n",
      "Epoch 263/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.6493e-07 - mean_squared_logarithmic_error: 2.6493e-07The average loss for epoch 262 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.6308e-07 - mean_squared_logarithmic_error: 2.6308e-07 - val_loss: 1.5203e-04 - val_mean_squared_logarithmic_error: 1.5203e-04\n",
      "\n",
      "Epoch 00263: Learning rate is 0.0001.\n",
      "Epoch 264/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.5741e-07 - mean_squared_logarithmic_error: 2.5741e-07The average loss for epoch 263 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5679e-07 - mean_squared_logarithmic_error: 2.5679e-07 - val_loss: 1.5196e-04 - val_mean_squared_logarithmic_error: 1.5196e-04\n",
      "\n",
      "Epoch 00264: Learning rate is 0.0001.\n",
      "Epoch 265/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 2.6251e-07 - mean_squared_logarithmic_error: 2.6251e-07The average loss for epoch 264 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.6221e-07 - mean_squared_logarithmic_error: 2.6221e-07 - val_loss: 1.5162e-04 - val_mean_squared_logarithmic_error: 1.5162e-04\n",
      "\n",
      "Epoch 00265: Learning rate is 0.0001.\n",
      "Epoch 266/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.5985e-07 - mean_squared_logarithmic_error: 2.5985e-07The average loss for epoch 265 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5852e-07 - mean_squared_logarithmic_error: 2.5852e-07 - val_loss: 1.5218e-04 - val_mean_squared_logarithmic_error: 1.5218e-04\n",
      "\n",
      "Epoch 00266: Learning rate is 0.0001.\n",
      "Epoch 267/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.5723e-07 - mean_squared_logarithmic_error: 2.5723e-07The average loss for epoch 266 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5693e-07 - mean_squared_logarithmic_error: 2.5693e-07 - val_loss: 1.5189e-04 - val_mean_squared_logarithmic_error: 1.5189e-04\n",
      "\n",
      "Epoch 00267: Learning rate is 0.0001.\n",
      "Epoch 268/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.5484e-07 - mean_squared_logarithmic_error: 2.5484e-07The average loss for epoch 267 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5482e-07 - mean_squared_logarithmic_error: 2.5482e-07 - val_loss: 1.5179e-04 - val_mean_squared_logarithmic_error: 1.5179e-04\n",
      "\n",
      "Epoch 00268: Learning rate is 0.0001.\n",
      "Epoch 269/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 2.5420e-07 - mean_squared_logarithmic_error: 2.5420e-07The average loss for epoch 268 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5272e-07 - mean_squared_logarithmic_error: 2.5272e-07 - val_loss: 1.5222e-04 - val_mean_squared_logarithmic_error: 1.5222e-04\n",
      "\n",
      "Epoch 00269: Learning rate is 0.0001.\n",
      "Epoch 270/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.5248e-07 - mean_squared_logarithmic_error: 2.5248e-07The average loss for epoch 269 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 2ms/step - loss: 2.5442e-07 - mean_squared_logarithmic_error: 2.5442e-07 - val_loss: 1.5191e-04 - val_mean_squared_logarithmic_error: 1.5191e-04\n",
      "\n",
      "Epoch 00270: Learning rate is 0.0001.\n",
      "Epoch 271/800\n",
      "244/264 [==========================>...] - ETA: 0s - loss: 2.5868e-07 - mean_squared_logarithmic_error: 2.5868e-07The average loss for epoch 270 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5808e-07 - mean_squared_logarithmic_error: 2.5808e-07 - val_loss: 1.5194e-04 - val_mean_squared_logarithmic_error: 1.5194e-04\n",
      "\n",
      "Epoch 00271: Learning rate is 0.0001.\n",
      "Epoch 272/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 2.5066e-07 - mean_squared_logarithmic_error: 2.5066e-07The average loss for epoch 271 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5063e-07 - mean_squared_logarithmic_error: 2.5063e-07 - val_loss: 1.5188e-04 - val_mean_squared_logarithmic_error: 1.5188e-04\n",
      "\n",
      "Epoch 00272: Learning rate is 0.0001.\n",
      "Epoch 273/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.5184e-07 - mean_squared_logarithmic_error: 2.5184e-07The average loss for epoch 272 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5078e-07 - mean_squared_logarithmic_error: 2.5078e-07 - val_loss: 1.5210e-04 - val_mean_squared_logarithmic_error: 1.5210e-04\n",
      "\n",
      "Epoch 00273: Learning rate is 0.0001.\n",
      "Epoch 274/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.4795e-07 - mean_squared_logarithmic_error: 2.4795e-07The average loss for epoch 273 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4768e-07 - mean_squared_logarithmic_error: 2.4768e-07 - val_loss: 1.5205e-04 - val_mean_squared_logarithmic_error: 1.5205e-04\n",
      "\n",
      "Epoch 00274: Learning rate is 0.0001.\n",
      "Epoch 275/800\n",
      "247/264 [===========================>..] - ETA: 0s - loss: 2.4999e-07 - mean_squared_logarithmic_error: 2.4999e-07The average loss for epoch 274 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4746e-07 - mean_squared_logarithmic_error: 2.4746e-07 - val_loss: 1.5210e-04 - val_mean_squared_logarithmic_error: 1.5210e-04\n",
      "\n",
      "Epoch 00275: Learning rate is 0.0001.\n",
      "Epoch 276/800\n",
      "246/264 [==========================>...] - ETA: 0s - loss: 2.4753e-07 - mean_squared_logarithmic_error: 2.4753e-07The average loss for epoch 275 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4660e-07 - mean_squared_logarithmic_error: 2.4660e-07 - val_loss: 1.5211e-04 - val_mean_squared_logarithmic_error: 1.5211e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00276: Learning rate is 0.0001.\n",
      "Epoch 277/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.5326e-07 - mean_squared_logarithmic_error: 2.5326e-07The average loss for epoch 276 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5240e-07 - mean_squared_logarithmic_error: 2.5240e-07 - val_loss: 1.5206e-04 - val_mean_squared_logarithmic_error: 1.5206e-04\n",
      "\n",
      "Epoch 00277: Learning rate is 0.0001.\n",
      "Epoch 278/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 2.5221e-07 - mean_squared_logarithmic_error: 2.5221e-07The average loss for epoch 277 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.5146e-07 - mean_squared_logarithmic_error: 2.5146e-07 - val_loss: 1.5215e-04 - val_mean_squared_logarithmic_error: 1.5215e-04\n",
      "\n",
      "Epoch 00278: Learning rate is 0.0001.\n",
      "Epoch 279/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.4276e-07 - mean_squared_logarithmic_error: 2.4276e-07The average loss for epoch 278 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4180e-07 - mean_squared_logarithmic_error: 2.4180e-07 - val_loss: 1.5163e-04 - val_mean_squared_logarithmic_error: 1.5163e-04\n",
      "\n",
      "Epoch 00279: Learning rate is 0.0001.\n",
      "Epoch 280/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.4509e-07 - mean_squared_logarithmic_error: 2.4509e-07The average loss for epoch 279 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4449e-07 - mean_squared_logarithmic_error: 2.4449e-07 - val_loss: 1.5239e-04 - val_mean_squared_logarithmic_error: 1.5239e-04\n",
      "\n",
      "Epoch 00280: Learning rate is 0.0001.\n",
      "Epoch 281/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.3950e-07 - mean_squared_logarithmic_error: 2.3950e-07The average loss for epoch 280 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3948e-07 - mean_squared_logarithmic_error: 2.3948e-07 - val_loss: 1.5205e-04 - val_mean_squared_logarithmic_error: 1.5205e-04\n",
      "\n",
      "Epoch 00281: Learning rate is 0.0001.\n",
      "Epoch 282/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 2.4092e-07 - mean_squared_logarithmic_error: 2.4092e-07The average loss for epoch 281 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3929e-07 - mean_squared_logarithmic_error: 2.3929e-07 - val_loss: 1.5241e-04 - val_mean_squared_logarithmic_error: 1.5241e-04\n",
      "\n",
      "Epoch 00282: Learning rate is 0.0001.\n",
      "Epoch 283/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.3879e-07 - mean_squared_logarithmic_error: 2.3879e-07The average loss for epoch 282 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3827e-07 - mean_squared_logarithmic_error: 2.3827e-07 - val_loss: 1.5240e-04 - val_mean_squared_logarithmic_error: 1.5240e-04\n",
      "\n",
      "Epoch 00283: Learning rate is 0.0001.\n",
      "Epoch 284/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.4688e-07 - mean_squared_logarithmic_error: 2.4688e-07The average loss for epoch 283 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.4612e-07 - mean_squared_logarithmic_error: 2.4612e-07 - val_loss: 1.5234e-04 - val_mean_squared_logarithmic_error: 1.5234e-04\n",
      "\n",
      "Epoch 00284: Learning rate is 0.0001.\n",
      "Epoch 285/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.3904e-07 - mean_squared_logarithmic_error: 2.3904e-07The average loss for epoch 284 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3875e-07 - mean_squared_logarithmic_error: 2.3875e-07 - val_loss: 1.5210e-04 - val_mean_squared_logarithmic_error: 1.5210e-04\n",
      "\n",
      "Epoch 00285: Learning rate is 0.0001.\n",
      "Epoch 286/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.3436e-07 - mean_squared_logarithmic_error: 2.3436e-07The average loss for epoch 285 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3503e-07 - mean_squared_logarithmic_error: 2.3503e-07 - val_loss: 1.5198e-04 - val_mean_squared_logarithmic_error: 1.5198e-04\n",
      "\n",
      "Epoch 00286: Learning rate is 0.0001.\n",
      "Epoch 287/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.4018e-07 - mean_squared_logarithmic_error: 2.4018e-07The average loss for epoch 286 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 2.3898e-07 - mean_squared_logarithmic_error: 2.3898e-07 - val_loss: 1.5232e-04 - val_mean_squared_logarithmic_error: 1.5232e-04\n",
      "\n",
      "Epoch 00287: Learning rate is 0.0001.\n",
      "Epoch 288/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.3265e-07 - mean_squared_logarithmic_error: 2.3265e-07The average loss for epoch 287 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 2.3195e-07 - mean_squared_logarithmic_error: 2.3195e-07 - val_loss: 1.5212e-04 - val_mean_squared_logarithmic_error: 1.5212e-04\n",
      "\n",
      "Epoch 00288: Learning rate is 0.0001.\n",
      "Epoch 289/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.4120e-07 - mean_squared_logarithmic_error: 2.4120e-07The average loss for epoch 288 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.3985e-07 - mean_squared_logarithmic_error: 2.3985e-07 - val_loss: 1.5243e-04 - val_mean_squared_logarithmic_error: 1.5243e-04\n",
      "\n",
      "Epoch 00289: Learning rate is 0.0001.\n",
      "Epoch 290/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.3763e-07 - mean_squared_logarithmic_error: 2.3763e-07The average loss for epoch 289 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 2.3700e-07 - mean_squared_logarithmic_error: 2.3700e-07 - val_loss: 1.5181e-04 - val_mean_squared_logarithmic_error: 1.5181e-04\n",
      "\n",
      "Epoch 00290: Learning rate is 0.0001.\n",
      "Epoch 291/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.3142e-07 - mean_squared_logarithmic_error: 2.3142e-07 ETA: 0s - loss: 2.4279e-07 - mean_squared_logarithmicThe average loss for epoch 290 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.3109e-07 - mean_squared_logarithmic_error: 2.3109e-07 - val_loss: 1.5215e-04 - val_mean_squared_logarithmic_error: 1.5215e-04\n",
      "\n",
      "Epoch 00291: Learning rate is 0.0001.\n",
      "Epoch 292/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.2854e-07 - mean_squared_logarithmic_error: 2.2854e-07The average loss for epoch 291 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.2817e-07 - mean_squared_logarithmic_error: 2.2817e-07 - val_loss: 1.5236e-04 - val_mean_squared_logarithmic_error: 1.5236e-04\n",
      "\n",
      "Epoch 00292: Learning rate is 0.0001.\n",
      "Epoch 293/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.3311e-07 - mean_squared_logarithmic_error: 2.3311e-07The average loss for epoch 292 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.3327e-07 - mean_squared_logarithmic_error: 2.3327e-07 - val_loss: 1.5220e-04 - val_mean_squared_logarithmic_error: 1.5220e-04\n",
      "\n",
      "Epoch 00293: Learning rate is 0.0001.\n",
      "Epoch 294/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.3303e-07 - mean_squared_logarithmic_error: 2.3303e-07The average loss for epoch 293 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.3321e-07 - mean_squared_logarithmic_error: 2.3321e-07 - val_loss: 1.5241e-04 - val_mean_squared_logarithmic_error: 1.5241e-04\n",
      "\n",
      "Epoch 00294: Learning rate is 0.0001.\n",
      "Epoch 295/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 2.3057e-07 - mean_squared_logarithmic_error: 2.3057e-07The average loss for epoch 294 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.3061e-07 - mean_squared_logarithmic_error: 2.3061e-07 - val_loss: 1.5154e-04 - val_mean_squared_logarithmic_error: 1.5154e-04\n",
      "\n",
      "Epoch 00295: Learning rate is 0.0001.\n",
      "Epoch 296/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/264 [============================>.] - ETA: 0s - loss: 2.3844e-07 - mean_squared_logarithmic_error: 2.3844e-07The average loss for epoch 295 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.3802e-07 - mean_squared_logarithmic_error: 2.3802e-07 - val_loss: 1.5230e-04 - val_mean_squared_logarithmic_error: 1.5230e-04\n",
      "\n",
      "Epoch 00296: Learning rate is 0.0001.\n",
      "Epoch 297/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.2753e-07 - mean_squared_logarithmic_error: 2.2753e-07The average loss for epoch 296 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.2674e-07 - mean_squared_logarithmic_error: 2.2674e-07 - val_loss: 1.5199e-04 - val_mean_squared_logarithmic_error: 1.5199e-04\n",
      "\n",
      "Epoch 00297: Learning rate is 0.0001.\n",
      "Epoch 298/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.2391e-07 - mean_squared_logarithmic_error: 2.2391e-07The average loss for epoch 297 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.2355e-07 - mean_squared_logarithmic_error: 2.2355e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00298: Learning rate is 0.0001.\n",
      "Epoch 299/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.2940e-07 - mean_squared_logarithmic_error: 2.2940e-07The average loss for epoch 298 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.2882e-07 - mean_squared_logarithmic_error: 2.2882e-07 - val_loss: 1.5189e-04 - val_mean_squared_logarithmic_error: 1.5189e-04\n",
      "\n",
      "Epoch 00299: Learning rate is 0.0001.\n",
      "Epoch 300/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.2339e-07 - mean_squared_logarithmic_error: 2.2339e-07The average loss for epoch 299 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.2357e-07 - mean_squared_logarithmic_error: 2.2357e-07 - val_loss: 1.5207e-04 - val_mean_squared_logarithmic_error: 1.5207e-04\n",
      "\n",
      "Epoch 00300: Learning rate is 0.0001.\n",
      "Epoch 301/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.2435e-07 - mean_squared_logarithmic_error: 2.2435e-07The average loss for epoch 300 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.2443e-07 - mean_squared_logarithmic_error: 2.2443e-07 - val_loss: 1.5219e-04 - val_mean_squared_logarithmic_error: 1.5219e-04\n",
      "\n",
      "Epoch 00301: Learning rate is 0.0001.\n",
      "Epoch 302/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.2821e-07 - mean_squared_logarithmic_error: 2.2821e-07The average loss for epoch 301 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.2775e-07 - mean_squared_logarithmic_error: 2.2775e-07 - val_loss: 1.5231e-04 - val_mean_squared_logarithmic_error: 1.5231e-04\n",
      "\n",
      "Epoch 00302: Learning rate is 0.0001.\n",
      "Epoch 303/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.1824e-07 - mean_squared_logarithmic_error: 2.1824e-07The average loss for epoch 302 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 9ms/step - loss: 2.1797e-07 - mean_squared_logarithmic_error: 2.1797e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00303: Learning rate is 0.0001.\n",
      "Epoch 304/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 2.2062e-07 - mean_squared_logarithmic_error: 2.2062e-07The average loss for epoch 303 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 9ms/step - loss: 2.2062e-07 - mean_squared_logarithmic_error: 2.2062e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00304: Learning rate is 0.0001.\n",
      "Epoch 305/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.2443e-07 - mean_squared_logarithmic_error: 2.2443e-07The average loss for epoch 304 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 9ms/step - loss: 2.2418e-07 - mean_squared_logarithmic_error: 2.2418e-07 - val_loss: 1.5225e-04 - val_mean_squared_logarithmic_error: 1.5225e-04\n",
      "\n",
      "Epoch 00305: Learning rate is 0.0001.\n",
      "Epoch 306/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.1853e-07 - mean_squared_logarithmic_error: 2.1853e-07The average loss for epoch 305 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.1850e-07 - mean_squared_logarithmic_error: 2.1850e-07 - val_loss: 1.5230e-04 - val_mean_squared_logarithmic_error: 1.5230e-04\n",
      "\n",
      "Epoch 00306: Learning rate is 0.0001.\n",
      "Epoch 307/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 2.2103e-07 - mean_squared_logarithmic_error: 2.2103e-07The average loss for epoch 306 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.2103e-07 - mean_squared_logarithmic_error: 2.2103e-07 - val_loss: 1.5203e-04 - val_mean_squared_logarithmic_error: 1.5203e-04\n",
      "\n",
      "Epoch 00307: Learning rate is 0.0001.\n",
      "Epoch 308/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.2153e-07 - mean_squared_logarithmic_error: 2.2153e-07The average loss for epoch 307 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.2192e-07 - mean_squared_logarithmic_error: 2.2192e-07 - val_loss: 1.5216e-04 - val_mean_squared_logarithmic_error: 1.5216e-04\n",
      "\n",
      "Epoch 00308: Learning rate is 0.0001.\n",
      "Epoch 309/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 2.2007e-07 - mean_squared_logarithmic_error: 2.2007e-07The average loss for epoch 308 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.2007e-07 - mean_squared_logarithmic_error: 2.2007e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00309: Learning rate is 0.0001.\n",
      "Epoch 310/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1927e-07 - mean_squared_logarithmic_error: 2.1927e-07The average loss for epoch 309 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 12ms/step - loss: 2.1958e-07 - mean_squared_logarithmic_error: 2.1958e-07 - val_loss: 1.5226e-04 - val_mean_squared_logarithmic_error: 1.5226e-04\n",
      "\n",
      "Epoch 00310: Learning rate is 0.0001.\n",
      "Epoch 311/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.1882e-07 - mean_squared_logarithmic_error: 2.1882e-07The average loss for epoch 310 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.1840e-07 - mean_squared_logarithmic_error: 2.1840e-07 - val_loss: 1.5234e-04 - val_mean_squared_logarithmic_error: 1.5234e-04\n",
      "\n",
      "Epoch 00311: Learning rate is 0.0001.\n",
      "Epoch 312/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.1390e-07 - mean_squared_logarithmic_error: 2.1390e-07The average loss for epoch 311 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 12ms/step - loss: 2.1389e-07 - mean_squared_logarithmic_error: 2.1389e-07 - val_loss: 1.5242e-04 - val_mean_squared_logarithmic_error: 1.5242e-04\n",
      "\n",
      "Epoch 00312: Learning rate is 0.0001.\n",
      "Epoch 313/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.2196e-07 - mean_squared_logarithmic_error: 2.2196e-07The average loss for epoch 312 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 12ms/step - loss: 2.2192e-07 - mean_squared_logarithmic_error: 2.2192e-07 - val_loss: 1.5183e-04 - val_mean_squared_logarithmic_error: 1.5183e-04\n",
      "\n",
      "Epoch 00313: Learning rate is 0.0001.\n",
      "Epoch 314/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 2.1492e-07 - mean_squared_logarithmic_error: 2.1492e-07The average loss for epoch 313 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 11ms/step - loss: 2.1492e-07 - mean_squared_logarithmic_error: 2.1492e-07 - val_loss: 1.5201e-04 - val_mean_squared_logarithmic_error: 1.5201e-04\n",
      "\n",
      "Epoch 00314: Learning rate is 0.0001.\n",
      "Epoch 315/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1414e-07 - mean_squared_logarithmic_error: 2.1414e-07The average loss for epoch 314 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 11ms/step - loss: 2.1393e-07 - mean_squared_logarithmic_error: 2.1393e-07 - val_loss: 1.5249e-04 - val_mean_squared_logarithmic_error: 1.5249e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00315: Learning rate is 0.0001.\n",
      "Epoch 316/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.1540e-07 - mean_squared_logarithmic_error: 2.1540e-07The average loss for epoch 315 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 12ms/step - loss: 2.1513e-07 - mean_squared_logarithmic_error: 2.1513e-07 - val_loss: 1.5240e-04 - val_mean_squared_logarithmic_error: 1.5240e-04\n",
      "\n",
      "Epoch 00316: Learning rate is 0.0001.\n",
      "Epoch 317/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1399e-07 - mean_squared_logarithmic_error: 2.1399e-07The average loss for epoch 316 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 11ms/step - loss: 2.1436e-07 - mean_squared_logarithmic_error: 2.1436e-07 - val_loss: 1.5219e-04 - val_mean_squared_logarithmic_error: 1.5219e-04\n",
      "\n",
      "Epoch 00317: Learning rate is 0.0001.\n",
      "Epoch 318/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.1466e-07 - mean_squared_logarithmic_error: 2.1466e-07The average loss for epoch 317 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 12ms/step - loss: 2.1645e-07 - mean_squared_logarithmic_error: 2.1645e-07 - val_loss: 1.5238e-04 - val_mean_squared_logarithmic_error: 1.5238e-04\n",
      "\n",
      "Epoch 00318: Learning rate is 0.0001.\n",
      "Epoch 319/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1612e-07 - mean_squared_logarithmic_error: 2.1612e-07The average loss for epoch 318 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.1607e-07 - mean_squared_logarithmic_error: 2.1607e-07 - val_loss: 1.5279e-04 - val_mean_squared_logarithmic_error: 1.5279e-04\n",
      "\n",
      "Epoch 00319: Learning rate is 0.0001.\n",
      "Epoch 320/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 2.1467e-07 - mean_squared_logarithmic_error: 2.1467e-07The average loss for epoch 319 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 11ms/step - loss: 2.1455e-07 - mean_squared_logarithmic_error: 2.1455e-07 - val_loss: 1.5230e-04 - val_mean_squared_logarithmic_error: 1.5230e-04\n",
      "\n",
      "Epoch 00320: Learning rate is 0.0001.\n",
      "Epoch 321/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.1163e-07 - mean_squared_logarithmic_error: 2.1163e-07The average loss for epoch 320 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 3s 10ms/step - loss: 2.1116e-07 - mean_squared_logarithmic_error: 2.1116e-07 - val_loss: 1.5244e-04 - val_mean_squared_logarithmic_error: 1.5244e-04\n",
      "\n",
      "Epoch 00321: Learning rate is 0.0001.\n",
      "Epoch 322/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1416e-07 - mean_squared_logarithmic_error: 2.1416e-07The average loss for epoch 321 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 9ms/step - loss: 2.1386e-07 - mean_squared_logarithmic_error: 2.1386e-07 - val_loss: 1.5241e-04 - val_mean_squared_logarithmic_error: 1.5241e-04\n",
      "\n",
      "Epoch 00322: Learning rate is 0.0001.\n",
      "Epoch 323/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 2.0621e-07 - mean_squared_logarithmic_error: 2.0621e-07The average loss for epoch 322 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.0609e-07 - mean_squared_logarithmic_error: 2.0609e-07 - val_loss: 1.5232e-04 - val_mean_squared_logarithmic_error: 1.5232e-04\n",
      "\n",
      "Epoch 00323: Learning rate is 0.0001.\n",
      "Epoch 324/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1412e-07 - mean_squared_logarithmic_error: 2.1412e-07The average loss for epoch 323 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.1357e-07 - mean_squared_logarithmic_error: 2.1357e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00324: Learning rate is 0.0001.\n",
      "Epoch 325/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.0636e-07 - mean_squared_logarithmic_error: 2.0636e-07The average loss for epoch 324 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.0640e-07 - mean_squared_logarithmic_error: 2.0640e-07 - val_loss: 1.5237e-04 - val_mean_squared_logarithmic_error: 1.5237e-04\n",
      "\n",
      "Epoch 00325: Learning rate is 0.0001.\n",
      "Epoch 326/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 2.1234e-07 - mean_squared_logarithmic_error: 2.1234e-07The average loss for epoch 325 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 2.1234e-07 - mean_squared_logarithmic_error: 2.1234e-07 - val_loss: 1.5235e-04 - val_mean_squared_logarithmic_error: 1.5235e-04\n",
      "\n",
      "Epoch 00326: Learning rate is 0.0001.\n",
      "Epoch 327/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.1022e-07 - mean_squared_logarithmic_error: 2.1022e-07The average loss for epoch 326 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.0986e-07 - mean_squared_logarithmic_error: 2.0986e-07 - val_loss: 1.5273e-04 - val_mean_squared_logarithmic_error: 1.5273e-04\n",
      "\n",
      "Epoch 00327: Learning rate is 0.0001.\n",
      "Epoch 328/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 2.0603e-07 - mean_squared_logarithmic_error: 2.0603e-07The average loss for epoch 327 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.0565e-07 - mean_squared_logarithmic_error: 2.0565e-07 - val_loss: 1.5209e-04 - val_mean_squared_logarithmic_error: 1.5209e-04\n",
      "\n",
      "Epoch 00328: Learning rate is 0.0001.\n",
      "Epoch 329/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.0897e-07 - mean_squared_logarithmic_error: 2.0897e-07The average loss for epoch 328 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 2.0874e-07 - mean_squared_logarithmic_error: 2.0874e-07 - val_loss: 1.5216e-04 - val_mean_squared_logarithmic_error: 1.5216e-04\n",
      "\n",
      "Epoch 00329: Learning rate is 0.0001.\n",
      "Epoch 330/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.1102e-07 - mean_squared_logarithmic_error: 2.1102e-07The average loss for epoch 329 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.1120e-07 - mean_squared_logarithmic_error: 2.1120e-07 - val_loss: 1.5218e-04 - val_mean_squared_logarithmic_error: 1.5218e-04\n",
      "\n",
      "Epoch 00330: Learning rate is 0.0001.\n",
      "Epoch 331/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 2.1493e-07 - mean_squared_logarithmic_error: 2.1493e-07The average loss for epoch 330 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.1413e-07 - mean_squared_logarithmic_error: 2.1413e-07 - val_loss: 1.5219e-04 - val_mean_squared_logarithmic_error: 1.5219e-04\n",
      "\n",
      "Epoch 00331: Learning rate is 0.0001.\n",
      "Epoch 332/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.1039e-07 - mean_squared_logarithmic_error: 2.1039e-07The average loss for epoch 331 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.0973e-07 - mean_squared_logarithmic_error: 2.0973e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00332: Learning rate is 0.0001.\n",
      "Epoch 333/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.0322e-07 - mean_squared_logarithmic_error: 2.0322e-07The average loss for epoch 332 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.0325e-07 - mean_squared_logarithmic_error: 2.0325e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00333: Learning rate is 0.0001.\n",
      "Epoch 334/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.0766e-07 - mean_squared_logarithmic_error: 2.0766e-07The average loss for epoch 333 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.0632e-07 - mean_squared_logarithmic_error: 2.0632e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00334: Learning rate is 0.0001.\n",
      "Epoch 335/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.0479e-07 - mean_squared_logarithmic_error: 2.0479e-07The average loss for epoch 334 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 2.0476e-07 - mean_squared_logarithmic_error: 2.0476e-07 - val_loss: 1.5244e-04 - val_mean_squared_logarithmic_error: 1.5244e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00335: Learning rate is 0.0001.\n",
      "Epoch 336/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.0977e-07 - mean_squared_logarithmic_error: 2.0977e-07The average loss for epoch 335 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0964e-07 - mean_squared_logarithmic_error: 2.0964e-07 - val_loss: 1.5297e-04 - val_mean_squared_logarithmic_error: 1.5297e-04\n",
      "\n",
      "Epoch 00336: Learning rate is 0.0001.\n",
      "Epoch 337/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.0860e-07 - mean_squared_logarithmic_error: 2.0860e-07The average loss for epoch 336 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0784e-07 - mean_squared_logarithmic_error: 2.0784e-07 - val_loss: 1.5223e-04 - val_mean_squared_logarithmic_error: 1.5223e-04\n",
      "\n",
      "Epoch 00337: Learning rate is 0.0001.\n",
      "Epoch 338/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 2.0346e-07 - mean_squared_logarithmic_error: 2.0346e-07The average loss for epoch 337 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0355e-07 - mean_squared_logarithmic_error: 2.0355e-07 - val_loss: 1.5251e-04 - val_mean_squared_logarithmic_error: 1.5251e-04\n",
      "\n",
      "Epoch 00338: Learning rate is 0.0001.\n",
      "Epoch 339/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.0158e-07 - mean_squared_logarithmic_error: 2.0158e-07The average loss for epoch 338 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0046e-07 - mean_squared_logarithmic_error: 2.0046e-07 - val_loss: 1.5292e-04 - val_mean_squared_logarithmic_error: 1.5292e-04\n",
      "\n",
      "Epoch 00339: Learning rate is 0.0001.\n",
      "Epoch 340/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.0191e-07 - mean_squared_logarithmic_error: 2.0191e-07The average loss for epoch 339 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0171e-07 - mean_squared_logarithmic_error: 2.0171e-07 - val_loss: 1.5324e-04 - val_mean_squared_logarithmic_error: 1.5324e-04\n",
      "\n",
      "Epoch 00340: Learning rate is 0.0001.\n",
      "Epoch 341/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 2.1284e-07 - mean_squared_logarithmic_error: 2.1284e-07The average loss for epoch 340 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.1193e-07 - mean_squared_logarithmic_error: 2.1193e-07 - val_loss: 1.5241e-04 - val_mean_squared_logarithmic_error: 1.5241e-04\n",
      "\n",
      "Epoch 00341: Learning rate is 0.0001.\n",
      "Epoch 342/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.9965e-07 - mean_squared_logarithmic_error: 1.9965e-07 ETA: 0s - loss: 2.0199e-07 - mean_squared_logarithmic_error: 2.0199The average loss for epoch 341 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.9917e-07 - mean_squared_logarithmic_error: 1.9917e-07 - val_loss: 1.5272e-04 - val_mean_squared_logarithmic_error: 1.5272e-04\n",
      "\n",
      "Epoch 00342: Learning rate is 0.0001.\n",
      "Epoch 343/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.9848e-07 - mean_squared_logarithmic_error: 1.9848e-07The average loss for epoch 342 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.9749e-07 - mean_squared_logarithmic_error: 1.9749e-07 - val_loss: 1.5225e-04 - val_mean_squared_logarithmic_error: 1.5225e-04\n",
      "\n",
      "Epoch 00343: Learning rate is 0.0001.\n",
      "Epoch 344/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 2.0348e-07 - mean_squared_logarithmic_error: 2.0348e-07The average loss for epoch 343 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0308e-07 - mean_squared_logarithmic_error: 2.0308e-07 - val_loss: 1.5247e-04 - val_mean_squared_logarithmic_error: 1.5247e-04\n",
      "\n",
      "Epoch 00344: Learning rate is 0.0001.\n",
      "Epoch 345/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.0741e-07 - mean_squared_logarithmic_error: 2.0741e-07The average loss for epoch 344 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0587e-07 - mean_squared_logarithmic_error: 2.0587e-07 - val_loss: 1.5240e-04 - val_mean_squared_logarithmic_error: 1.5240e-04\n",
      "\n",
      "Epoch 00345: Learning rate is 0.0001.\n",
      "Epoch 346/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 2.0242e-07 - mean_squared_logarithmic_error: 2.0242e-07The average loss for epoch 345 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0212e-07 - mean_squared_logarithmic_error: 2.0212e-07 - val_loss: 1.5240e-04 - val_mean_squared_logarithmic_error: 1.5240e-04\n",
      "\n",
      "Epoch 00346: Learning rate is 0.0001.\n",
      "Epoch 347/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 2.0265e-07 - mean_squared_logarithmic_error: 2.0265e-07The average loss for epoch 346 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 2.0279e-07 - mean_squared_logarithmic_error: 2.0279e-07 - val_loss: 1.5176e-04 - val_mean_squared_logarithmic_error: 1.5176e-04\n",
      "\n",
      "Epoch 00347: Learning rate is 0.0001.\n",
      "Epoch 348/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.9970e-07 - mean_squared_logarithmic_error: 1.9970e-07The average loss for epoch 347 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.9968e-07 - mean_squared_logarithmic_error: 1.9968e-07 - val_loss: 1.5287e-04 - val_mean_squared_logarithmic_error: 1.5287e-04\n",
      "\n",
      "Epoch 00348: Learning rate is 0.0001.\n",
      "Epoch 349/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.9875e-07 - mean_squared_logarithmic_error: 1.9875e-07The average loss for epoch 348 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9881e-07 - mean_squared_logarithmic_error: 1.9881e-07 - val_loss: 1.5246e-04 - val_mean_squared_logarithmic_error: 1.5246e-04\n",
      "\n",
      "Epoch 00349: Learning rate is 0.0001.\n",
      "Epoch 350/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.9720e-07 - mean_squared_logarithmic_error: 1.9720e-07The average loss for epoch 349 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9689e-07 - mean_squared_logarithmic_error: 1.9689e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00350: Learning rate is 0.0001.\n",
      "Epoch 351/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 2.0044e-07 - mean_squared_logarithmic_error: 2.0044e-07The average loss for epoch 350 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 2.0170e-07 - mean_squared_logarithmic_error: 2.0170e-07 - val_loss: 1.5217e-04 - val_mean_squared_logarithmic_error: 1.5217e-04\n",
      "\n",
      "Epoch 00351: Learning rate is 0.0001.\n",
      "Epoch 352/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 2.0517e-07 - mean_squared_logarithmic_error: 2.0517e-07The average loss for epoch 351 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 2.0318e-07 - mean_squared_logarithmic_error: 2.0318e-07 - val_loss: 1.5273e-04 - val_mean_squared_logarithmic_error: 1.5273e-04\n",
      "\n",
      "Epoch 00352: Learning rate is 0.0001.\n",
      "Epoch 353/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9975e-07 - mean_squared_logarithmic_error: 1.9975e-07The average loss for epoch 352 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.9914e-07 - mean_squared_logarithmic_error: 1.9914e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00353: Learning rate is 0.0001.\n",
      "Epoch 354/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.9298e-07 - mean_squared_logarithmic_error: 1.9298e-07The average loss for epoch 353 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9299e-07 - mean_squared_logarithmic_error: 1.9299e-07 - val_loss: 1.5242e-04 - val_mean_squared_logarithmic_error: 1.5242e-04\n",
      "\n",
      "Epoch 00354: Learning rate is 0.0001.\n",
      "Epoch 355/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9803e-07 - mean_squared_logarithmic_error: 1.9803e-07The average loss for epoch 354 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9717e-07 - mean_squared_logarithmic_error: 1.9717e-07 - val_loss: 1.5274e-04 - val_mean_squared_logarithmic_error: 1.5274e-04\n",
      "\n",
      "Epoch 00355: Learning rate is 0.0001.\n",
      "Epoch 356/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.9896e-07 - mean_squared_logarithmic_error: 1.9896e-07The average loss for epoch 355 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9864e-07 - mean_squared_logarithmic_error: 1.9864e-07 - val_loss: 1.5280e-04 - val_mean_squared_logarithmic_error: 1.5280e-04\n",
      "\n",
      "Epoch 00356: Learning rate is 0.0001.\n",
      "Epoch 357/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.9343e-07 - mean_squared_logarithmic_error: 1.9343e-07The average loss for epoch 356 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9343e-07 - mean_squared_logarithmic_error: 1.9343e-07 - val_loss: 1.5219e-04 - val_mean_squared_logarithmic_error: 1.5219e-04\n",
      "\n",
      "Epoch 00357: Learning rate is 0.0001.\n",
      "Epoch 358/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 2.0237e-07 - mean_squared_logarithmic_error: 2.0237e-07The average loss for epoch 357 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 2.0185e-07 - mean_squared_logarithmic_error: 2.0185e-07 - val_loss: 1.5277e-04 - val_mean_squared_logarithmic_error: 1.5277e-04\n",
      "\n",
      "Epoch 00358: Learning rate is 0.0001.\n",
      "Epoch 359/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.9365e-07 - mean_squared_logarithmic_error: 1.9365e-07The average loss for epoch 358 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9290e-07 - mean_squared_logarithmic_error: 1.9290e-07 - val_loss: 1.5287e-04 - val_mean_squared_logarithmic_error: 1.5287e-04\n",
      "\n",
      "Epoch 00359: Learning rate is 0.0001.\n",
      "Epoch 360/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.9617e-07 - mean_squared_logarithmic_error: 1.9617e-07The average loss for epoch 359 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9565e-07 - mean_squared_logarithmic_error: 1.9565e-07 - val_loss: 1.5259e-04 - val_mean_squared_logarithmic_error: 1.5259e-04\n",
      "\n",
      "Epoch 00360: Learning rate is 0.0001.\n",
      "Epoch 361/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9466e-07 - mean_squared_logarithmic_error: 1.9466e-07The average loss for epoch 360 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9309e-07 - mean_squared_logarithmic_error: 1.9309e-07 - val_loss: 1.5281e-04 - val_mean_squared_logarithmic_error: 1.5281e-04\n",
      "\n",
      "Epoch 00361: Learning rate is 0.0001.\n",
      "Epoch 362/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.9609e-07 - mean_squared_logarithmic_error: 1.9609e-07The average loss for epoch 361 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9557e-07 - mean_squared_logarithmic_error: 1.9557e-07 - val_loss: 1.5255e-04 - val_mean_squared_logarithmic_error: 1.5255e-04\n",
      "\n",
      "Epoch 00362: Learning rate is 0.0001.\n",
      "Epoch 363/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.8995e-07 - mean_squared_logarithmic_error: 1.8995e-07The average loss for epoch 362 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8883e-07 - mean_squared_logarithmic_error: 1.8883e-07 - val_loss: 1.5246e-04 - val_mean_squared_logarithmic_error: 1.5246e-04\n",
      "\n",
      "Epoch 00363: Learning rate is 0.0001.\n",
      "Epoch 364/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9338e-07 - mean_squared_logarithmic_error: 1.9338e-07The average loss for epoch 363 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9263e-07 - mean_squared_logarithmic_error: 1.9263e-07 - val_loss: 1.5305e-04 - val_mean_squared_logarithmic_error: 1.5305e-04\n",
      "\n",
      "Epoch 00364: Learning rate is 0.0001.\n",
      "Epoch 365/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9079e-07 - mean_squared_logarithmic_error: 1.9079e-07The average loss for epoch 364 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8956e-07 - mean_squared_logarithmic_error: 1.8956e-07 - val_loss: 1.5249e-04 - val_mean_squared_logarithmic_error: 1.5249e-04\n",
      "\n",
      "Epoch 00365: Learning rate is 0.0001.\n",
      "Epoch 366/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.9094e-07 - mean_squared_logarithmic_error: 1.9094e-07The average loss for epoch 365 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8944e-07 - mean_squared_logarithmic_error: 1.8944e-07 - val_loss: 1.5259e-04 - val_mean_squared_logarithmic_error: 1.5259e-04\n",
      "\n",
      "Epoch 00366: Learning rate is 0.0001.\n",
      "Epoch 367/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9340e-07 - mean_squared_logarithmic_error: 1.9340e-07The average loss for epoch 366 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9460e-07 - mean_squared_logarithmic_error: 1.9460e-07 - val_loss: 1.5218e-04 - val_mean_squared_logarithmic_error: 1.5218e-04\n",
      "\n",
      "Epoch 00367: Learning rate is 0.0001.\n",
      "Epoch 368/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.9170e-07 - mean_squared_logarithmic_error: 1.9170e-07The average loss for epoch 367 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9116e-07 - mean_squared_logarithmic_error: 1.9116e-07 - val_loss: 1.5214e-04 - val_mean_squared_logarithmic_error: 1.5214e-04\n",
      "\n",
      "Epoch 00368: Learning rate is 0.0001.\n",
      "Epoch 369/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9293e-07 - mean_squared_logarithmic_error: 1.9293e-07The average loss for epoch 368 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9227e-07 - mean_squared_logarithmic_error: 1.9227e-07 - val_loss: 1.5291e-04 - val_mean_squared_logarithmic_error: 1.5291e-04\n",
      "\n",
      "Epoch 00369: Learning rate is 0.0001.\n",
      "Epoch 370/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.9581e-07 - mean_squared_logarithmic_error: 1.9581e-07The average loss for epoch 369 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9504e-07 - mean_squared_logarithmic_error: 1.9504e-07 - val_loss: 1.5281e-04 - val_mean_squared_logarithmic_error: 1.5281e-04\n",
      "\n",
      "Epoch 00370: Learning rate is 0.0001.\n",
      "Epoch 371/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.9110e-07 - mean_squared_logarithmic_error: 1.9110e-07The average loss for epoch 370 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8956e-07 - mean_squared_logarithmic_error: 1.8956e-07 - val_loss: 1.5274e-04 - val_mean_squared_logarithmic_error: 1.5274e-04\n",
      "\n",
      "Epoch 00371: Learning rate is 0.0001.\n",
      "Epoch 372/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.9239e-07 - mean_squared_logarithmic_error: 1.9239e-07The average loss for epoch 371 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9117e-07 - mean_squared_logarithmic_error: 1.9117e-07 - val_loss: 1.5238e-04 - val_mean_squared_logarithmic_error: 1.5238e-04\n",
      "\n",
      "Epoch 00372: Learning rate is 0.0001.\n",
      "Epoch 373/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.7220e-07 - mean_squared_logarithmic_error: 1.7220e-07The average loss for epoch 372 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8840e-07 - mean_squared_logarithmic_error: 1.8840e-07 - val_loss: 1.5295e-04 - val_mean_squared_logarithmic_error: 1.5295e-04\n",
      "\n",
      "Epoch 00373: Learning rate is 0.0001.\n",
      "Epoch 374/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.8999e-07 - mean_squared_logarithmic_error: 1.8999e-07The average loss for epoch 373 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8996e-07 - mean_squared_logarithmic_error: 1.8996e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00374: Learning rate is 0.0001.\n",
      "Epoch 375/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.8869e-07 - mean_squared_logarithmic_error: 1.8869e-07The average loss for epoch 374 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8877e-07 - mean_squared_logarithmic_error: 1.8877e-07 - val_loss: 1.5300e-04 - val_mean_squared_logarithmic_error: 1.5300e-04\n",
      "\n",
      "Epoch 00375: Learning rate is 0.0001.\n",
      "Epoch 376/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.8951e-07 - mean_squared_logarithmic_error: 1.8951e-07The average loss for epoch 375 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8876e-07 - mean_squared_logarithmic_error: 1.8876e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00376: Learning rate is 0.0001.\n",
      "Epoch 377/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.8570e-07 - mean_squared_logarithmic_error: 1.8570e-07The average loss for epoch 376 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8539e-07 - mean_squared_logarithmic_error: 1.8539e-07 - val_loss: 1.5290e-04 - val_mean_squared_logarithmic_error: 1.5290e-04\n",
      "\n",
      "Epoch 00377: Learning rate is 0.0001.\n",
      "Epoch 378/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.9076e-07 - mean_squared_logarithmic_error: 1.9076e-07The average loss for epoch 377 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9006e-07 - mean_squared_logarithmic_error: 1.9006e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00378: Learning rate is 0.0001.\n",
      "Epoch 379/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.8794e-07 - mean_squared_logarithmic_error: 1.8794e-07The average loss for epoch 378 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8726e-07 - mean_squared_logarithmic_error: 1.8726e-07 - val_loss: 1.5193e-04 - val_mean_squared_logarithmic_error: 1.5193e-04\n",
      "\n",
      "Epoch 00379: Learning rate is 0.0001.\n",
      "Epoch 380/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.9171e-07 - mean_squared_logarithmic_error: 1.9171e-07The average loss for epoch 379 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9054e-07 - mean_squared_logarithmic_error: 1.9054e-07 - val_loss: 1.5297e-04 - val_mean_squared_logarithmic_error: 1.5297e-04\n",
      "\n",
      "Epoch 00380: Learning rate is 0.0001.\n",
      "Epoch 381/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.8380e-07 - mean_squared_logarithmic_error: 1.8380e-07The average loss for epoch 380 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8348e-07 - mean_squared_logarithmic_error: 1.8348e-07 - val_loss: 1.5236e-04 - val_mean_squared_logarithmic_error: 1.5236e-04\n",
      "\n",
      "Epoch 00381: Learning rate is 0.0001.\n",
      "Epoch 382/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.9248e-07 - mean_squared_logarithmic_error: 1.9248e-07The average loss for epoch 381 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.9162e-07 - mean_squared_logarithmic_error: 1.9162e-07 - val_loss: 1.5295e-04 - val_mean_squared_logarithmic_error: 1.5295e-04\n",
      "\n",
      "Epoch 00382: Learning rate is 0.0001.\n",
      "Epoch 383/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.8682e-07 - mean_squared_logarithmic_error: 1.8682e-07 ETA: 0s - loss: 1.9902e-07 - mean_squared_logarithmic_eThe average loss for epoch 382 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8581e-07 - mean_squared_logarithmic_error: 1.8581e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00383: Learning rate is 0.0001.\n",
      "Epoch 384/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.8366e-07 - mean_squared_logarithmic_error: 1.8366e-07The average loss for epoch 383 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8376e-07 - mean_squared_logarithmic_error: 1.8376e-07 - val_loss: 1.5273e-04 - val_mean_squared_logarithmic_error: 1.5273e-04\n",
      "\n",
      "Epoch 00384: Learning rate is 0.0001.\n",
      "Epoch 385/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.9040e-07 - mean_squared_logarithmic_error: 1.9040e-07The average loss for epoch 384 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8873e-07 - mean_squared_logarithmic_error: 1.8873e-07 - val_loss: 1.5314e-04 - val_mean_squared_logarithmic_error: 1.5314e-04\n",
      "\n",
      "Epoch 00385: Learning rate is 0.0001.\n",
      "Epoch 386/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.8728e-07 - mean_squared_logarithmic_error: 1.8728e-07The average loss for epoch 385 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8664e-07 - mean_squared_logarithmic_error: 1.8664e-07 - val_loss: 1.5311e-04 - val_mean_squared_logarithmic_error: 1.5311e-04\n",
      "\n",
      "Epoch 00386: Learning rate is 0.0001.\n",
      "Epoch 387/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.8637e-07 - mean_squared_logarithmic_error: 1.8637e-07The average loss for epoch 386 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8539e-07 - mean_squared_logarithmic_error: 1.8539e-07 - val_loss: 1.5313e-04 - val_mean_squared_logarithmic_error: 1.5313e-04\n",
      "\n",
      "Epoch 00387: Learning rate is 0.0001.\n",
      "Epoch 388/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.8491e-07 - mean_squared_logarithmic_error: 1.8491e-07The average loss for epoch 387 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8417e-07 - mean_squared_logarithmic_error: 1.8417e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00388: Learning rate is 0.0001.\n",
      "Epoch 389/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.8104e-07 - mean_squared_logarithmic_error: 1.8104e-07The average loss for epoch 388 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8068e-07 - mean_squared_logarithmic_error: 1.8068e-07 - val_loss: 1.5245e-04 - val_mean_squared_logarithmic_error: 1.5245e-04\n",
      "\n",
      "Epoch 00389: Learning rate is 0.0001.\n",
      "Epoch 390/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.8633e-07 - mean_squared_logarithmic_error: 1.8633e-07The average loss for epoch 389 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8414e-07 - mean_squared_logarithmic_error: 1.8414e-07 - val_loss: 1.5254e-04 - val_mean_squared_logarithmic_error: 1.5254e-04\n",
      "\n",
      "Epoch 00390: Learning rate is 0.0001.\n",
      "Epoch 391/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.8480e-07 - mean_squared_logarithmic_error: 1.8480e-07The average loss for epoch 390 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8391e-07 - mean_squared_logarithmic_error: 1.8391e-07 - val_loss: 1.5293e-04 - val_mean_squared_logarithmic_error: 1.5293e-04\n",
      "\n",
      "Epoch 00391: Learning rate is 0.0001.\n",
      "Epoch 392/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.8369e-07 - mean_squared_logarithmic_error: 1.8369e-07The average loss for epoch 391 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8289e-07 - mean_squared_logarithmic_error: 1.8289e-07 - val_loss: 1.5299e-04 - val_mean_squared_logarithmic_error: 1.5299e-04\n",
      "\n",
      "Epoch 00392: Learning rate is 0.0001.\n",
      "Epoch 393/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.8139e-07 - mean_squared_logarithmic_error: 1.8139e-07The average loss for epoch 392 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8104e-07 - mean_squared_logarithmic_error: 1.8104e-07 - val_loss: 1.5299e-04 - val_mean_squared_logarithmic_error: 1.5299e-04\n",
      "\n",
      "Epoch 00393: Learning rate is 0.0001.\n",
      "Epoch 394/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/264 [============================>.] - ETA: 0s - loss: 1.7961e-07 - mean_squared_logarithmic_error: 1.7961e-07The average loss for epoch 393 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.7961e-07 - mean_squared_logarithmic_error: 1.7961e-07 - val_loss: 1.5281e-04 - val_mean_squared_logarithmic_error: 1.5281e-04\n",
      "\n",
      "Epoch 00394: Learning rate is 0.0001.\n",
      "Epoch 395/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.8174e-07 - mean_squared_logarithmic_error: 1.8174e-07The average loss for epoch 394 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8172e-07 - mean_squared_logarithmic_error: 1.8172e-07 - val_loss: 1.5252e-04 - val_mean_squared_logarithmic_error: 1.5252e-04\n",
      "\n",
      "Epoch 00395: Learning rate is 0.0001.\n",
      "Epoch 396/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.8131e-07 - mean_squared_logarithmic_error: 1.8131e-07The average loss for epoch 395 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8071e-07 - mean_squared_logarithmic_error: 1.8071e-07 - val_loss: 1.5255e-04 - val_mean_squared_logarithmic_error: 1.5255e-04\n",
      "\n",
      "Epoch 00396: Learning rate is 0.0001.\n",
      "Epoch 397/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.8556e-07 - mean_squared_logarithmic_error: 1.8556e-07The average loss for epoch 396 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8420e-07 - mean_squared_logarithmic_error: 1.8420e-07 - val_loss: 1.5256e-04 - val_mean_squared_logarithmic_error: 1.5256e-04\n",
      "\n",
      "Epoch 00397: Learning rate is 0.0001.\n",
      "Epoch 398/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.7694e-07 - mean_squared_logarithmic_error: 1.7694e-07The average loss for epoch 397 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.7700e-07 - mean_squared_logarithmic_error: 1.7700e-07 - val_loss: 1.5283e-04 - val_mean_squared_logarithmic_error: 1.5283e-04\n",
      "\n",
      "Epoch 00398: Learning rate is 0.0001.\n",
      "Epoch 399/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.8393e-07 - mean_squared_logarithmic_error: 1.8393e-07The average loss for epoch 398 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.8349e-07 - mean_squared_logarithmic_error: 1.8349e-07 - val_loss: 1.5222e-04 - val_mean_squared_logarithmic_error: 1.5222e-04\n",
      "\n",
      "Epoch 00399: Learning rate is 0.0001.\n",
      "Epoch 400/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.8174e-07 - mean_squared_logarithmic_error: 1.8174e-07The average loss for epoch 399 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.8178e-07 - mean_squared_logarithmic_error: 1.8178e-07 - val_loss: 1.5214e-04 - val_mean_squared_logarithmic_error: 1.5214e-04\n",
      "\n",
      "Epoch 00400: Learning rate is 0.0000.\n",
      "Epoch 401/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.6380e-07 - mean_squared_logarithmic_error: 1.6380e-07The average loss for epoch 400 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6278e-07 - mean_squared_logarithmic_error: 1.6278e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00401: Learning rate is 0.0000.\n",
      "Epoch 402/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.6190e-07 - mean_squared_logarithmic_error: 1.6190e-07The average loss for epoch 401 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6066e-07 - mean_squared_logarithmic_error: 1.6066e-07 - val_loss: 1.5259e-04 - val_mean_squared_logarithmic_error: 1.5259e-04\n",
      "\n",
      "Epoch 00402: Learning rate is 0.0000.\n",
      "Epoch 403/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.6172e-07 - mean_squared_logarithmic_error: 1.6172e-07The average loss for epoch 402 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6037e-07 - mean_squared_logarithmic_error: 1.6037e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00403: Learning rate is 0.0000.\n",
      "Epoch 404/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.6023e-07 - mean_squared_logarithmic_error: 1.6023e-07The average loss for epoch 403 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5999e-07 - mean_squared_logarithmic_error: 1.5999e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00404: Learning rate is 0.0000.\n",
      "Epoch 405/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.6148e-07 - mean_squared_logarithmic_error: 1.6148e-07The average loss for epoch 404 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6025e-07 - mean_squared_logarithmic_error: 1.6025e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00405: Learning rate is 0.0000.\n",
      "Epoch 406/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.6069e-07 - mean_squared_logarithmic_error: 1.6069e-07The average loss for epoch 405 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6002e-07 - mean_squared_logarithmic_error: 1.6002e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00406: Learning rate is 0.0000.\n",
      "Epoch 407/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.6048e-07 - mean_squared_logarithmic_error: 1.6048e-07The average loss for epoch 406 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6019e-07 - mean_squared_logarithmic_error: 1.6019e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00407: Learning rate is 0.0000.\n",
      "Epoch 408/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.6098e-07 - mean_squared_logarithmic_error: 1.6098e-07The average loss for epoch 407 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6093e-07 - mean_squared_logarithmic_error: 1.6093e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00408: Learning rate is 0.0000.\n",
      "Epoch 409/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5952e-07 - mean_squared_logarithmic_error: 1.5952e-07The average loss for epoch 408 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6002e-07 - mean_squared_logarithmic_error: 1.6002e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00409: Learning rate is 0.0000.\n",
      "Epoch 410/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.6103e-07 - mean_squared_logarithmic_error: 1.6103e-07 ETA: 0s - loss: 1.6128e-07 - mean_squared_logarithmic_error: 1.6128The average loss for epoch 409 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6018e-07 - mean_squared_logarithmic_error: 1.6018e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00410: Learning rate is 0.0000.\n",
      "Epoch 411/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6012e-07 - mean_squared_logarithmic_error: 1.6012e-07The average loss for epoch 410 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6017e-07 - mean_squared_logarithmic_error: 1.6017e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n",
      "\n",
      "Epoch 00411: Learning rate is 0.0000.\n",
      "Epoch 412/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6139e-07 - mean_squared_logarithmic_error: 1.6139e-07The average loss for epoch 411 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6057e-07 - mean_squared_logarithmic_error: 1.6057e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00412: Learning rate is 0.0000.\n",
      "Epoch 413/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.6069e-07 - mean_squared_logarithmic_error: 1.6069e-07The average loss for epoch 412 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6059e-07 - mean_squared_logarithmic_error: 1.6059e-07 - val_loss: 1.5259e-04 - val_mean_squared_logarithmic_error: 1.5259e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00413: Learning rate is 0.0000.\n",
      "Epoch 414/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6235e-07 - mean_squared_logarithmic_error: 1.6235e-07The average loss for epoch 413 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6121e-07 - mean_squared_logarithmic_error: 1.6121e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00414: Learning rate is 0.0000.\n",
      "Epoch 415/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5793e-07 - mean_squared_logarithmic_error: 1.5793e-07The average loss for epoch 414 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5988e-07 - mean_squared_logarithmic_error: 1.5988e-07 - val_loss: 1.5253e-04 - val_mean_squared_logarithmic_error: 1.5253e-04\n",
      "\n",
      "Epoch 00415: Learning rate is 0.0000.\n",
      "Epoch 416/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5965e-07 - mean_squared_logarithmic_error: 1.5965e-07The average loss for epoch 415 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5963e-07 - mean_squared_logarithmic_error: 1.5963e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00416: Learning rate is 0.0000.\n",
      "Epoch 417/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6103e-07 - mean_squared_logarithmic_error: 1.6103e-07The average loss for epoch 416 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.6048e-07 - mean_squared_logarithmic_error: 1.6048e-07 - val_loss: 1.5271e-04 - val_mean_squared_logarithmic_error: 1.5271e-04\n",
      "\n",
      "Epoch 00417: Learning rate is 0.0000.\n",
      "Epoch 418/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5990e-07 - mean_squared_logarithmic_error: 1.5990e-07The average loss for epoch 417 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5983e-07 - mean_squared_logarithmic_error: 1.5983e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00418: Learning rate is 0.0000.\n",
      "Epoch 419/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.6071e-07 - mean_squared_logarithmic_error: 1.6071e-07The average loss for epoch 418 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5952e-07 - mean_squared_logarithmic_error: 1.5952e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00419: Learning rate is 0.0000.\n",
      "Epoch 420/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.6077e-07 - mean_squared_logarithmic_error: 1.6077e-07The average loss for epoch 419 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.6017e-07 - mean_squared_logarithmic_error: 1.6017e-07 - val_loss: 1.5273e-04 - val_mean_squared_logarithmic_error: 1.5273e-04\n",
      "\n",
      "Epoch 00420: Learning rate is 0.0000.\n",
      "Epoch 421/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6056e-07 - mean_squared_logarithmic_error: 1.6056e-07The average loss for epoch 420 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5970e-07 - mean_squared_logarithmic_error: 1.5970e-07 - val_loss: 1.5276e-04 - val_mean_squared_logarithmic_error: 1.5276e-04\n",
      "\n",
      "Epoch 00421: Learning rate is 0.0000.\n",
      "Epoch 422/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.6048e-07 - mean_squared_logarithmic_error: 1.6048e-07The average loss for epoch 421 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5976e-07 - mean_squared_logarithmic_error: 1.5976e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00422: Learning rate is 0.0000.\n",
      "Epoch 423/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.6092e-07 - mean_squared_logarithmic_error: 1.6092e-07The average loss for epoch 422 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5943e-07 - mean_squared_logarithmic_error: 1.5943e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n",
      "\n",
      "Epoch 00423: Learning rate is 0.0000.\n",
      "Epoch 424/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.6012e-07 - mean_squared_logarithmic_error: 1.6012e-07The average loss for epoch 423 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5994e-07 - mean_squared_logarithmic_error: 1.5994e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00424: Learning rate is 0.0000.\n",
      "Epoch 425/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5987e-07 - mean_squared_logarithmic_error: 1.5987e-07The average loss for epoch 424 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5930e-07 - mean_squared_logarithmic_error: 1.5930e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00425: Learning rate is 0.0000.\n",
      "Epoch 426/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.4299e-07 - mean_squared_logarithmic_error: 1.4299e-07The average loss for epoch 425 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5946e-07 - mean_squared_logarithmic_error: 1.5946e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00426: Learning rate is 0.0000.\n",
      "Epoch 427/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5945e-07 - mean_squared_logarithmic_error: 1.5945e-07The average loss for epoch 426 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5937e-07 - mean_squared_logarithmic_error: 1.5937e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00427: Learning rate is 0.0000.\n",
      "Epoch 428/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.6117e-07 - mean_squared_logarithmic_error: 1.6117e-07The average loss for epoch 427 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5992e-07 - mean_squared_logarithmic_error: 1.5992e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00428: Learning rate is 0.0000.\n",
      "Epoch 429/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5757e-07 - mean_squared_logarithmic_error: 1.5757e-07The average loss for epoch 428 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5992e-07 - mean_squared_logarithmic_error: 1.5992e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00429: Learning rate is 0.0000.\n",
      "Epoch 430/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.6054e-07 - mean_squared_logarithmic_error: 1.6054e-07The average loss for epoch 429 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5952e-07 - mean_squared_logarithmic_error: 1.5952e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00430: Learning rate is 0.0000.\n",
      "Epoch 431/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5992e-07 - mean_squared_logarithmic_error: 1.5992e-07The average loss for epoch 430 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5945e-07 - mean_squared_logarithmic_error: 1.5945e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00431: Learning rate is 0.0000.\n",
      "Epoch 432/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5959e-07 - mean_squared_logarithmic_error: 1.5959e-07The average loss for epoch 431 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5933e-07 - mean_squared_logarithmic_error: 1.5933e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n",
      "\n",
      "Epoch 00432: Learning rate is 0.0000.\n",
      "Epoch 433/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5938e-07 - mean_squared_logarithmic_error: 1.5938e-07The average loss for epoch 432 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5904e-07 - mean_squared_logarithmic_error: 1.5904e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00433: Learning rate is 0.0000.\n",
      "Epoch 434/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.6059e-07 - mean_squared_logarithmic_error: 1.6059e-07The average loss for epoch 433 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5988e-07 - mean_squared_logarithmic_error: 1.5988e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00434: Learning rate is 0.0000.\n",
      "Epoch 435/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.4264e-07 - mean_squared_logarithmic_error: 1.4264e-07The average loss for epoch 434 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5899e-07 - mean_squared_logarithmic_error: 1.5899e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00435: Learning rate is 0.0000.\n",
      "Epoch 436/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6010e-07 - mean_squared_logarithmic_error: 1.6010e-07The average loss for epoch 435 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5936e-07 - mean_squared_logarithmic_error: 1.5936e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00436: Learning rate is 0.0000.\n",
      "Epoch 437/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.6057e-07 - mean_squared_logarithmic_error: 1.6057e-07 ETA: 0s - loss: 1.6683e-07 - mean_squared_logarithmic_error: 1.The average loss for epoch 436 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5988e-07 - mean_squared_logarithmic_error: 1.5988e-07 - val_loss: 1.5254e-04 - val_mean_squared_logarithmic_error: 1.5254e-04\n",
      "\n",
      "Epoch 00437: Learning rate is 0.0000.\n",
      "Epoch 438/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5900e-07 - mean_squared_logarithmic_error: 1.5900e-07The average loss for epoch 437 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5891e-07 - mean_squared_logarithmic_error: 1.5891e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00438: Learning rate is 0.0000.\n",
      "Epoch 439/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5941e-07 - mean_squared_logarithmic_error: 1.5941e-07The average loss for epoch 438 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5941e-07 - mean_squared_logarithmic_error: 1.5941e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00439: Learning rate is 0.0000.\n",
      "Epoch 440/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5964e-07 - mean_squared_logarithmic_error: 1.5964e-07The average loss for epoch 439 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5950e-07 - mean_squared_logarithmic_error: 1.5950e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00440: Learning rate is 0.0000.\n",
      "Epoch 441/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.6048e-07 - mean_squared_logarithmic_error: 1.6048e-07The average loss for epoch 440 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5961e-07 - mean_squared_logarithmic_error: 1.5961e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00441: Learning rate is 0.0000.\n",
      "Epoch 442/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5919e-07 - mean_squared_logarithmic_error: 1.5919e-07The average loss for epoch 441 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5895e-07 - mean_squared_logarithmic_error: 1.5895e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00442: Learning rate is 0.0000.\n",
      "Epoch 443/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5794e-07 - mean_squared_logarithmic_error: 1.5794e-07The average loss for epoch 442 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5899e-07 - mean_squared_logarithmic_error: 1.5899e-07 - val_loss: 1.5272e-04 - val_mean_squared_logarithmic_error: 1.5272e-04\n",
      "\n",
      "Epoch 00443: Learning rate is 0.0000.\n",
      "Epoch 444/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5998e-07 - mean_squared_logarithmic_error: 1.5998e-07The average loss for epoch 443 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5926e-07 - mean_squared_logarithmic_error: 1.5926e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00444: Learning rate is 0.0000.\n",
      "Epoch 445/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5922e-07 - mean_squared_logarithmic_error: 1.5922e-07 ETA: 0s - loss: 1.6818e-07 - mean_squared_logarithmic_errThe average loss for epoch 444 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5911e-07 - mean_squared_logarithmic_error: 1.5911e-07 - val_loss: 1.5256e-04 - val_mean_squared_logarithmic_error: 1.5256e-04\n",
      "\n",
      "Epoch 00445: Learning rate is 0.0000.\n",
      "Epoch 446/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5883e-07 - mean_squared_logarithmic_error: 1.5883e-07The average loss for epoch 445 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5893e-07 - mean_squared_logarithmic_error: 1.5893e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00446: Learning rate is 0.0000.\n",
      "Epoch 447/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.6023e-07 - mean_squared_logarithmic_error: 1.6023e-07The average loss for epoch 446 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5925e-07 - mean_squared_logarithmic_error: 1.5925e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00447: Learning rate is 0.0000.\n",
      "Epoch 448/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.6030e-07 - mean_squared_logarithmic_error: 1.6030e-07The average loss for epoch 447 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5884e-07 - mean_squared_logarithmic_error: 1.5884e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00448: Learning rate is 0.0000.\n",
      "Epoch 449/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5946e-07 - mean_squared_logarithmic_error: 1.5946e-07The average loss for epoch 448 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5865e-07 - mean_squared_logarithmic_error: 1.5865e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00449: Learning rate is 0.0000.\n",
      "Epoch 450/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5938e-07 - mean_squared_logarithmic_error: 1.5938e-07The average loss for epoch 449 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5926e-07 - mean_squared_logarithmic_error: 1.5926e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00450: Learning rate is 0.0000.\n",
      "Epoch 451/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5999e-07 - mean_squared_logarithmic_error: 1.5999e-07The average loss for epoch 450 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5896e-07 - mean_squared_logarithmic_error: 1.5896e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00451: Learning rate is 0.0000.\n",
      "Epoch 452/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5973e-07 - mean_squared_logarithmic_error: 1.5973e-07The average loss for epoch 451 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5859e-07 - mean_squared_logarithmic_error: 1.5859e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00452: Learning rate is 0.0000.\n",
      "Epoch 453/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/264 [============================>.] - ETA: 0s - loss: 1.5993e-07 - mean_squared_logarithmic_error: 1.5993e-07The average loss for epoch 452 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5915e-07 - mean_squared_logarithmic_error: 1.5915e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00453: Learning rate is 0.0000.\n",
      "Epoch 454/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5884e-07 - mean_squared_logarithmic_error: 1.5884e-07The average loss for epoch 453 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5851e-07 - mean_squared_logarithmic_error: 1.5851e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00454: Learning rate is 0.0000.\n",
      "Epoch 455/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5915e-07 - mean_squared_logarithmic_error: 1.5915e-07The average loss for epoch 454 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5813e-07 - mean_squared_logarithmic_error: 1.5813e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00455: Learning rate is 0.0000.\n",
      "Epoch 456/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5972e-07 - mean_squared_logarithmic_error: 1.5972e-07The average loss for epoch 455 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5850e-07 - mean_squared_logarithmic_error: 1.5850e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00456: Learning rate is 0.0000.\n",
      "Epoch 457/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5974e-07 - mean_squared_logarithmic_error: 1.5974e-07The average loss for epoch 456 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5849e-07 - mean_squared_logarithmic_error: 1.5849e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n",
      "\n",
      "Epoch 00457: Learning rate is 0.0000.\n",
      "Epoch 458/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5865e-07 - mean_squared_logarithmic_error: 1.5865e-07The average loss for epoch 457 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5820e-07 - mean_squared_logarithmic_error: 1.5820e-07 - val_loss: 1.5274e-04 - val_mean_squared_logarithmic_error: 1.5274e-04\n",
      "\n",
      "Epoch 00458: Learning rate is 0.0000.\n",
      "Epoch 459/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5963e-07 - mean_squared_logarithmic_error: 1.5963e-07The average loss for epoch 458 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5928e-07 - mean_squared_logarithmic_error: 1.5928e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00459: Learning rate is 0.0000.\n",
      "Epoch 460/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5939e-07 - mean_squared_logarithmic_error: 1.5939e-07The average loss for epoch 459 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5858e-07 - mean_squared_logarithmic_error: 1.5858e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00460: Learning rate is 0.0000.\n",
      "Epoch 461/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.6000e-07 - mean_squared_logarithmic_error: 1.6000e-07The average loss for epoch 460 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5911e-07 - mean_squared_logarithmic_error: 1.5911e-07 - val_loss: 1.5271e-04 - val_mean_squared_logarithmic_error: 1.5271e-04\n",
      "\n",
      "Epoch 00461: Learning rate is 0.0000.\n",
      "Epoch 462/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5870e-07 - mean_squared_logarithmic_error: 1.5870e-07The average loss for epoch 461 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5860e-07 - mean_squared_logarithmic_error: 1.5860e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00462: Learning rate is 0.0000.\n",
      "Epoch 463/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5946e-07 - mean_squared_logarithmic_error: 1.5946e-07The average loss for epoch 462 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5916e-07 - mean_squared_logarithmic_error: 1.5916e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00463: Learning rate is 0.0000.\n",
      "Epoch 464/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5961e-07 - mean_squared_logarithmic_error: 1.5961e-07The average loss for epoch 463 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5898e-07 - mean_squared_logarithmic_error: 1.5898e-07 - val_loss: 1.5261e-04 - val_mean_squared_logarithmic_error: 1.5261e-04\n",
      "\n",
      "Epoch 00464: Learning rate is 0.0000.\n",
      "Epoch 465/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5858e-07 - mean_squared_logarithmic_error: 1.5858e-07 ETA: 0s - loss: 1.5942e-07 - mean_squared_logarithmic_error: 1.5942e-The average loss for epoch 464 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5812e-07 - mean_squared_logarithmic_error: 1.5812e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00465: Learning rate is 0.0000.\n",
      "Epoch 466/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5856e-07 - mean_squared_logarithmic_error: 1.5856e-07The average loss for epoch 465 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5841e-07 - mean_squared_logarithmic_error: 1.5841e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00466: Learning rate is 0.0000.\n",
      "Epoch 467/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5898e-07 - mean_squared_logarithmic_error: 1.5898e-07The average loss for epoch 466 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5895e-07 - mean_squared_logarithmic_error: 1.5895e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00467: Learning rate is 0.0000.\n",
      "Epoch 468/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5871e-07 - mean_squared_logarithmic_error: 1.5871e-07The average loss for epoch 467 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5817e-07 - mean_squared_logarithmic_error: 1.5817e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00468: Learning rate is 0.0000.\n",
      "Epoch 469/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5935e-07 - mean_squared_logarithmic_error: 1.5935e-07The average loss for epoch 468 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5843e-07 - mean_squared_logarithmic_error: 1.5843e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00469: Learning rate is 0.0000.\n",
      "Epoch 470/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5915e-07 - mean_squared_logarithmic_error: 1.5915e-07The average loss for epoch 469 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5840e-07 - mean_squared_logarithmic_error: 1.5840e-07 - val_loss: 1.5273e-04 - val_mean_squared_logarithmic_error: 1.5273e-04\n",
      "\n",
      "Epoch 00470: Learning rate is 0.0000.\n",
      "Epoch 471/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5802e-07 - mean_squared_logarithmic_error: 1.5802e-07The average loss for epoch 470 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5778e-07 - mean_squared_logarithmic_error: 1.5778e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00471: Learning rate is 0.0000.\n",
      "Epoch 472/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5720e-07 - mean_squared_logarithmic_error: 1.5720e-07The average loss for epoch 471 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5798e-07 - mean_squared_logarithmic_error: 1.5798e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00472: Learning rate is 0.0000.\n",
      "Epoch 473/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5675e-07 - mean_squared_logarithmic_error: 1.5675e-07The average loss for epoch 472 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5800e-07 - mean_squared_logarithmic_error: 1.5800e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00473: Learning rate is 0.0000.\n",
      "Epoch 474/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5783e-07 - mean_squared_logarithmic_error: 1.5783e-07The average loss for epoch 473 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5781e-07 - mean_squared_logarithmic_error: 1.5781e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00474: Learning rate is 0.0000.\n",
      "Epoch 475/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5866e-07 - mean_squared_logarithmic_error: 1.5866e-07The average loss for epoch 474 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5791e-07 - mean_squared_logarithmic_error: 1.5791e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00475: Learning rate is 0.0000.\n",
      "Epoch 476/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5783e-07 - mean_squared_logarithmic_error: 1.5783e-07The average loss for epoch 475 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5783e-07 - mean_squared_logarithmic_error: 1.5783e-07 - val_loss: 1.5255e-04 - val_mean_squared_logarithmic_error: 1.5255e-04\n",
      "\n",
      "Epoch 00476: Learning rate is 0.0000.\n",
      "Epoch 477/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5913e-07 - mean_squared_logarithmic_error: 1.5913e-07The average loss for epoch 476 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5866e-07 - mean_squared_logarithmic_error: 1.5866e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00477: Learning rate is 0.0000.\n",
      "Epoch 478/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5693e-07 - mean_squared_logarithmic_error: 1.5693e-07The average loss for epoch 477 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5794e-07 - mean_squared_logarithmic_error: 1.5794e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00478: Learning rate is 0.0000.\n",
      "Epoch 479/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.4156e-07 - mean_squared_logarithmic_error: 1.4156e-07The average loss for epoch 478 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5791e-07 - mean_squared_logarithmic_error: 1.5791e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00479: Learning rate is 0.0000.\n",
      "Epoch 480/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5828e-07 - mean_squared_logarithmic_error: 1.5828e-07The average loss for epoch 479 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5828e-07 - mean_squared_logarithmic_error: 1.5828e-07 - val_loss: 1.5271e-04 - val_mean_squared_logarithmic_error: 1.5271e-04\n",
      "\n",
      "Epoch 00480: Learning rate is 0.0000.\n",
      "Epoch 481/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5827e-07 - mean_squared_logarithmic_error: 1.5827e-07The average loss for epoch 480 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5808e-07 - mean_squared_logarithmic_error: 1.5808e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00481: Learning rate is 0.0000.\n",
      "Epoch 482/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5870e-07 - mean_squared_logarithmic_error: 1.5870e-07The average loss for epoch 481 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5838e-07 - mean_squared_logarithmic_error: 1.5838e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00482: Learning rate is 0.0000.\n",
      "Epoch 483/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5620e-07 - mean_squared_logarithmic_error: 1.5620e-07The average loss for epoch 482 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5803e-07 - mean_squared_logarithmic_error: 1.5803e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00483: Learning rate is 0.0000.\n",
      "Epoch 484/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5772e-07 - mean_squared_logarithmic_error: 1.5772e-07The average loss for epoch 483 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5744e-07 - mean_squared_logarithmic_error: 1.5744e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00484: Learning rate is 0.0000.\n",
      "Epoch 485/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5780e-07 - mean_squared_logarithmic_error: 1.5780e-07The average loss for epoch 484 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5773e-07 - mean_squared_logarithmic_error: 1.5773e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00485: Learning rate is 0.0000.\n",
      "Epoch 486/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5792e-07 - mean_squared_logarithmic_error: 1.5792e-07The average loss for epoch 485 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5742e-07 - mean_squared_logarithmic_error: 1.5742e-07 - val_loss: 1.5268e-04 - val_mean_squared_logarithmic_error: 1.5268e-04\n",
      "\n",
      "Epoch 00486: Learning rate is 0.0000.\n",
      "Epoch 487/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5838e-07 - mean_squared_logarithmic_error: 1.5838e-07The average loss for epoch 486 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5838e-07 - mean_squared_logarithmic_error: 1.5838e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00487: Learning rate is 0.0000.\n",
      "Epoch 488/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5847e-07 - mean_squared_logarithmic_error: 1.5847e-07The average loss for epoch 487 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5774e-07 - mean_squared_logarithmic_error: 1.5774e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00488: Learning rate is 0.0000.\n",
      "Epoch 489/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5758e-07 - mean_squared_logarithmic_error: 1.5758e-07The average loss for epoch 488 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5753e-07 - mean_squared_logarithmic_error: 1.5753e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00489: Learning rate is 0.0000.\n",
      "Epoch 490/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5783e-07 - mean_squared_logarithmic_error: 1.5783e-07The average loss for epoch 489 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5774e-07 - mean_squared_logarithmic_error: 1.5774e-07 - val_loss: 1.5260e-04 - val_mean_squared_logarithmic_error: 1.5260e-04\n",
      "\n",
      "Epoch 00490: Learning rate is 0.0000.\n",
      "Epoch 491/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5770e-07 - mean_squared_logarithmic_error: 1.5770e-07The average loss for epoch 490 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5754e-07 - mean_squared_logarithmic_error: 1.5754e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00491: Learning rate is 0.0000.\n",
      "Epoch 492/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5866e-07 - mean_squared_logarithmic_error: 1.5866e-07The average loss for epoch 491 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5860e-07 - mean_squared_logarithmic_error: 1.5860e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00492: Learning rate is 0.0000.\n",
      "Epoch 493/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5765e-07 - mean_squared_logarithmic_error: 1.5765e-07The average loss for epoch 492 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5740e-07 - mean_squared_logarithmic_error: 1.5740e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00493: Learning rate is 0.0000.\n",
      "Epoch 494/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5830e-07 - mean_squared_logarithmic_error: 1.5830e-07The average loss for epoch 493 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5802e-07 - mean_squared_logarithmic_error: 1.5802e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00494: Learning rate is 0.0000.\n",
      "Epoch 495/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5785e-07 - mean_squared_logarithmic_error: 1.5785e-07The average loss for epoch 494 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5761e-07 - mean_squared_logarithmic_error: 1.5761e-07 - val_loss: 1.5272e-04 - val_mean_squared_logarithmic_error: 1.5272e-04\n",
      "\n",
      "Epoch 00495: Learning rate is 0.0000.\n",
      "Epoch 496/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5749e-07 - mean_squared_logarithmic_error: 1.5749e-07The average loss for epoch 495 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5748e-07 - mean_squared_logarithmic_error: 1.5748e-07 - val_loss: 1.5257e-04 - val_mean_squared_logarithmic_error: 1.5257e-04\n",
      "\n",
      "Epoch 00496: Learning rate is 0.0000.\n",
      "Epoch 497/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5626e-07 - mean_squared_logarithmic_error: 1.5626e-07The average loss for epoch 496 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5710e-07 - mean_squared_logarithmic_error: 1.5710e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00497: Learning rate is 0.0000.\n",
      "Epoch 498/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5718e-07 - mean_squared_logarithmic_error: 1.5718e-07The average loss for epoch 497 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5707e-07 - mean_squared_logarithmic_error: 1.5707e-07 - val_loss: 1.5258e-04 - val_mean_squared_logarithmic_error: 1.5258e-04\n",
      "\n",
      "Epoch 00498: Learning rate is 0.0000.\n",
      "Epoch 499/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5792e-07 - mean_squared_logarithmic_error: 1.5792e-07The average loss for epoch 498 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5771e-07 - mean_squared_logarithmic_error: 1.5771e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00499: Learning rate is 0.0000.\n",
      "Epoch 500/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5735e-07 - mean_squared_logarithmic_error: 1.5735e-07The average loss for epoch 499 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5735e-07 - mean_squared_logarithmic_error: 1.5735e-07 - val_loss: 1.5269e-04 - val_mean_squared_logarithmic_error: 1.5269e-04\n",
      "\n",
      "Epoch 00500: Learning rate is 0.0000.\n",
      "Epoch 501/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5585e-07 - mean_squared_logarithmic_error: 1.5585e-07The average loss for epoch 500 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 6ms/step - loss: 1.5529e-07 - mean_squared_logarithmic_error: 1.5529e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00501: Learning rate is 0.0000.\n",
      "Epoch 502/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5573e-07 - mean_squared_logarithmic_error: 1.5573e-07The average loss for epoch 501 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00502: Learning rate is 0.0000.\n",
      "Epoch 503/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5575e-07 - mean_squared_logarithmic_error: 1.5575e-07The average loss for epoch 502 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5513e-07 - mean_squared_logarithmic_error: 1.5513e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00503: Learning rate is 0.0000.\n",
      "Epoch 504/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5595e-07 - mean_squared_logarithmic_error: 1.5595e-07The average loss for epoch 503 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00504: Learning rate is 0.0000.\n",
      "Epoch 505/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5543e-07 - mean_squared_logarithmic_error: 1.5543e-07The average loss for epoch 504 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5517e-07 - mean_squared_logarithmic_error: 1.5517e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00505: Learning rate is 0.0000.\n",
      "Epoch 506/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5517e-07 - mean_squared_logarithmic_error: 1.5517e-07The average loss for epoch 505 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00506: Learning rate is 0.0000.\n",
      "Epoch 507/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5527e-07 - mean_squared_logarithmic_error: 1.5527e-07The average loss for epoch 506 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00507: Learning rate is 0.0000.\n",
      "Epoch 508/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07The average loss for epoch 507 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00508: Learning rate is 0.0000.\n",
      "Epoch 509/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5572e-07 - mean_squared_logarithmic_error: 1.5572e-07The average loss for epoch 508 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00509: Learning rate is 0.0000.\n",
      "Epoch 510/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5524e-07 - mean_squared_logarithmic_error: 1.5524e-07The average loss for epoch 509 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5516e-07 - mean_squared_logarithmic_error: 1.5516e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00510: Learning rate is 0.0000.\n",
      "Epoch 511/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5520e-07 - mean_squared_logarithmic_error: 1.5520e-07The average loss for epoch 510 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5520e-07 - mean_squared_logarithmic_error: 1.5520e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00511: Learning rate is 0.0000.\n",
      "Epoch 512/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5566e-07 - mean_squared_logarithmic_error: 1.5566e-07The average loss for epoch 511 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00512: Learning rate is 0.0000.\n",
      "Epoch 513/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5602e-07 - mean_squared_logarithmic_error: 1.5602e-07The average loss for epoch 512 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5527e-07 - mean_squared_logarithmic_error: 1.5527e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00513: Learning rate is 0.0000.\n",
      "Epoch 514/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07The average loss for epoch 513 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5517e-07 - mean_squared_logarithmic_error: 1.5517e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00514: Learning rate is 0.0000.\n",
      "Epoch 515/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07The average loss for epoch 514 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00515: Learning rate is 0.0000.\n",
      "Epoch 516/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5565e-07 - mean_squared_logarithmic_error: 1.5565e-07The average loss for epoch 515 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5523e-07 - mean_squared_logarithmic_error: 1.5523e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00516: Learning rate is 0.0000.\n",
      "Epoch 517/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5586e-07 - mean_squared_logarithmic_error: 1.5586e-07The average loss for epoch 516 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00517: Learning rate is 0.0000.\n",
      "Epoch 518/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5556e-07 - mean_squared_logarithmic_error: 1.5556e-07The average loss for epoch 517 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00518: Learning rate is 0.0000.\n",
      "Epoch 519/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5532e-07 - mean_squared_logarithmic_error: 1.5532e-07The average loss for epoch 518 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00519: Learning rate is 0.0000.\n",
      "Epoch 520/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5516e-07 - mean_squared_logarithmic_error: 1.5516e-07The average loss for epoch 519 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00520: Learning rate is 0.0000.\n",
      "Epoch 521/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5611e-07 - mean_squared_logarithmic_error: 1.5611e-07The average loss for epoch 520 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5517e-07 - mean_squared_logarithmic_error: 1.5517e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00521: Learning rate is 0.0000.\n",
      "Epoch 522/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07The average loss for epoch 521 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 6ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00522: Learning rate is 0.0000.\n",
      "Epoch 523/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5634e-07 - mean_squared_logarithmic_error: 1.5634e-07The average loss for epoch 522 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00523: Learning rate is 0.0000.\n",
      "Epoch 524/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5569e-07 - mean_squared_logarithmic_error: 1.5569e-07The average loss for epoch 523 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 6ms/step - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00524: Learning rate is 0.0000.\n",
      "Epoch 525/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5552e-07 - mean_squared_logarithmic_error: 1.5552e-07The average loss for epoch 524 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00525: Learning rate is 0.0000.\n",
      "Epoch 526/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5569e-07 - mean_squared_logarithmic_error: 1.5569e-07The average loss for epoch 525 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00526: Learning rate is 0.0000.\n",
      "Epoch 527/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5582e-07 - mean_squared_logarithmic_error: 1.5582e-07The average loss for epoch 526 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5526e-07 - mean_squared_logarithmic_error: 1.5526e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00527: Learning rate is 0.0000.\n",
      "Epoch 528/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5559e-07 - mean_squared_logarithmic_error: 1.5559e-07The average loss for epoch 527 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5513e-07 - mean_squared_logarithmic_error: 1.5513e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00528: Learning rate is 0.0000.\n",
      "Epoch 529/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5529e-07 - mean_squared_logarithmic_error: 1.5529e-07The average loss for epoch 528 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00529: Learning rate is 0.0000.\n",
      "Epoch 530/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5542e-07 - mean_squared_logarithmic_error: 1.5542e-07The average loss for epoch 529 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5510e-07 - mean_squared_logarithmic_error: 1.5510e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00530: Learning rate is 0.0000.\n",
      "Epoch 531/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5567e-07 - mean_squared_logarithmic_error: 1.5567e-07The average loss for epoch 530 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5510e-07 - mean_squared_logarithmic_error: 1.5510e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00531: Learning rate is 0.0000.\n",
      "Epoch 532/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5580e-07 - mean_squared_logarithmic_error: 1.5580e-07The average loss for epoch 531 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5523e-07 - mean_squared_logarithmic_error: 1.5523e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00532: Learning rate is 0.0000.\n",
      "Epoch 533/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5549e-07 - mean_squared_logarithmic_error: 1.5549e-07The average loss for epoch 532 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00533: Learning rate is 0.0000.\n",
      "Epoch 534/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 533 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5511e-07 - mean_squared_logarithmic_error: 1.5511e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00534: Learning rate is 0.0000.\n",
      "Epoch 535/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5558e-07 - mean_squared_logarithmic_error: 1.5558e-07The average loss for epoch 534 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00535: Learning rate is 0.0000.\n",
      "Epoch 536/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5655e-07 - mean_squared_logarithmic_error: 1.5655e-07The average loss for epoch 535 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00536: Learning rate is 0.0000.\n",
      "Epoch 537/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5643e-07 - mean_squared_logarithmic_error: 1.5643e-07The average loss for epoch 536 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00537: Learning rate is 0.0000.\n",
      "Epoch 538/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5587e-07 - mean_squared_logarithmic_error: 1.5587e-07The average loss for epoch 537 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00538: Learning rate is 0.0000.\n",
      "Epoch 539/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5526e-07 - mean_squared_logarithmic_error: 1.5526e-07The average loss for epoch 538 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00539: Learning rate is 0.0000.\n",
      "Epoch 540/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5637e-07 - mean_squared_logarithmic_error: 1.5637e-07The average loss for epoch 539 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5528e-07 - mean_squared_logarithmic_error: 1.5528e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00540: Learning rate is 0.0000.\n",
      "Epoch 541/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5563e-07 - mean_squared_logarithmic_error: 1.5563e-07The average loss for epoch 540 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5511e-07 - mean_squared_logarithmic_error: 1.5511e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00541: Learning rate is 0.0000.\n",
      "Epoch 542/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5516e-07 - mean_squared_logarithmic_error: 1.5516e-07The average loss for epoch 541 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5511e-07 - mean_squared_logarithmic_error: 1.5511e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00542: Learning rate is 0.0000.\n",
      "Epoch 543/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5571e-07 - mean_squared_logarithmic_error: 1.5571e-07The average loss for epoch 542 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00543: Learning rate is 0.0000.\n",
      "Epoch 544/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5569e-07 - mean_squared_logarithmic_error: 1.5569e-07The average loss for epoch 543 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00544: Learning rate is 0.0000.\n",
      "Epoch 545/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5555e-07 - mean_squared_logarithmic_error: 1.5555e-07The average loss for epoch 544 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5509e-07 - mean_squared_logarithmic_error: 1.5509e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00545: Learning rate is 0.0000.\n",
      "Epoch 546/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5570e-07 - mean_squared_logarithmic_error: 1.5570e-07The average loss for epoch 545 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00546: Learning rate is 0.0000.\n",
      "Epoch 547/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5619e-07 - mean_squared_logarithmic_error: 1.5619e-07The average loss for epoch 546 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00547: Learning rate is 0.0000.\n",
      "Epoch 548/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5576e-07 - mean_squared_logarithmic_error: 1.5576e-07The average loss for epoch 547 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5504e-07 - mean_squared_logarithmic_error: 1.5504e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00548: Learning rate is 0.0000.\n",
      "Epoch 549/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5627e-07 - mean_squared_logarithmic_error: 1.5627e-07The average loss for epoch 548 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00549: Learning rate is 0.0000.\n",
      "Epoch 550/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5578e-07 - mean_squared_logarithmic_error: 1.5578e-07The average loss for epoch 549 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00550: Learning rate is 0.0000.\n",
      "Epoch 551/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07The average loss for epoch 550 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5499e-07 - mean_squared_logarithmic_error: 1.5499e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00551: Learning rate is 0.0000.\n",
      "Epoch 552/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 551 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5523e-07 - mean_squared_logarithmic_error: 1.5523e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00552: Learning rate is 0.0000.\n",
      "Epoch 553/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5624e-07 - mean_squared_logarithmic_error: 1.5624e-07The average loss for epoch 552 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00553: Learning rate is 0.0000.\n",
      "Epoch 554/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5548e-07 - mean_squared_logarithmic_error: 1.5548e-07The average loss for epoch 553 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5511e-07 - mean_squared_logarithmic_error: 1.5511e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00554: Learning rate is 0.0000.\n",
      "Epoch 555/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5580e-07 - mean_squared_logarithmic_error: 1.5580e-07The average loss for epoch 554 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5504e-07 - mean_squared_logarithmic_error: 1.5504e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00555: Learning rate is 0.0000.\n",
      "Epoch 556/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5607e-07 - mean_squared_logarithmic_error: 1.5607e-07The average loss for epoch 555 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00556: Learning rate is 0.0000.\n",
      "Epoch 557/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5610e-07 - mean_squared_logarithmic_error: 1.5610e-07The average loss for epoch 556 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00557: Learning rate is 0.0000.\n",
      "Epoch 558/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5580e-07 - mean_squared_logarithmic_error: 1.5580e-07The average loss for epoch 557 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00558: Learning rate is 0.0000.\n",
      "Epoch 559/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5556e-07 - mean_squared_logarithmic_error: 1.5556e-07The average loss for epoch 558 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00559: Learning rate is 0.0000.\n",
      "Epoch 560/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5638e-07 - mean_squared_logarithmic_error: 1.5638e-07The average loss for epoch 559 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5506e-07 - mean_squared_logarithmic_error: 1.5506e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00560: Learning rate is 0.0000.\n",
      "Epoch 561/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5643e-07 - mean_squared_logarithmic_error: 1.5643e-07The average loss for epoch 560 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00561: Learning rate is 0.0000.\n",
      "Epoch 562/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5554e-07 - mean_squared_logarithmic_error: 1.5554e-07The average loss for epoch 561 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00562: Learning rate is 0.0000.\n",
      "Epoch 563/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5444e-07 - mean_squared_logarithmic_error: 1.5444e-07The average loss for epoch 562 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00563: Learning rate is 0.0000.\n",
      "Epoch 564/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5586e-07 - mean_squared_logarithmic_error: 1.5586e-07The average loss for epoch 563 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5509e-07 - mean_squared_logarithmic_error: 1.5509e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00564: Learning rate is 0.0000.\n",
      "Epoch 565/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5492e-07 - mean_squared_logarithmic_error: 1.5492e-07The average loss for epoch 564 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5504e-07 - mean_squared_logarithmic_error: 1.5504e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00565: Learning rate is 0.0000.\n",
      "Epoch 566/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5627e-07 - mean_squared_logarithmic_error: 1.5627e-07 ETA: 0s - loss: 1.4123e-07 - mean_squared_logaritThe average loss for epoch 565 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00566: Learning rate is 0.0000.\n",
      "Epoch 567/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5564e-07 - mean_squared_logarithmic_error: 1.5564e-07The average loss for epoch 566 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00567: Learning rate is 0.0000.\n",
      "Epoch 568/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07The average loss for epoch 567 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5518e-07 - mean_squared_logarithmic_error: 1.5518e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00568: Learning rate is 0.0000.\n",
      "Epoch 569/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5562e-07 - mean_squared_logarithmic_error: 1.5562e-07The average loss for epoch 568 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5497e-07 - mean_squared_logarithmic_error: 1.5497e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00569: Learning rate is 0.0000.\n",
      "Epoch 570/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5579e-07 - mean_squared_logarithmic_error: 1.5579e-07The average loss for epoch 569 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00570: Learning rate is 0.0000.\n",
      "Epoch 571/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5581e-07 - mean_squared_logarithmic_error: 1.5581e-07 ETA: 0s - loss: 1.4058e-07 - mean_squared_logarithmic_error: 1.40The average loss for epoch 570 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5506e-07 - mean_squared_logarithmic_error: 1.5506e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00571: Learning rate is 0.0000.\n",
      "Epoch 572/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5612e-07 - mean_squared_logarithmic_error: 1.5612e-07The average loss for epoch 571 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00572: Learning rate is 0.0000.\n",
      "Epoch 573/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5618e-07 - mean_squared_logarithmic_error: 1.5618e-07The average loss for epoch 572 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5513e-07 - mean_squared_logarithmic_error: 1.5513e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00573: Learning rate is 0.0000.\n",
      "Epoch 574/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5534e-07 - mean_squared_logarithmic_error: 1.5534e-07The average loss for epoch 573 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00574: Learning rate is 0.0000.\n",
      "Epoch 575/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5501e-07 - mean_squared_logarithmic_error: 1.5501e-07The average loss for epoch 574 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00575: Learning rate is 0.0000.\n",
      "Epoch 576/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5589e-07 - mean_squared_logarithmic_error: 1.5589e-07The average loss for epoch 575 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5499e-07 - mean_squared_logarithmic_error: 1.5499e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00576: Learning rate is 0.0000.\n",
      "Epoch 577/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5602e-07 - mean_squared_logarithmic_error: 1.5602e-07The average loss for epoch 576 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5499e-07 - mean_squared_logarithmic_error: 1.5499e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00577: Learning rate is 0.0000.\n",
      "Epoch 578/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07The average loss for epoch 577 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5500e-07 - mean_squared_logarithmic_error: 1.5500e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00578: Learning rate is 0.0000.\n",
      "Epoch 579/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5606e-07 - mean_squared_logarithmic_error: 1.5606e-07The average loss for epoch 578 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5500e-07 - mean_squared_logarithmic_error: 1.5500e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00579: Learning rate is 0.0000.\n",
      "Epoch 580/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5542e-07 - mean_squared_logarithmic_error: 1.5542e-07The average loss for epoch 579 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5500e-07 - mean_squared_logarithmic_error: 1.5500e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00580: Learning rate is 0.0000.\n",
      "Epoch 581/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5504e-07 - mean_squared_logarithmic_error: 1.5504e-07The average loss for epoch 580 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5498e-07 - mean_squared_logarithmic_error: 1.5498e-07 - val_loss: 1.5262e-04 - val_mean_squared_logarithmic_error: 1.5262e-04\n",
      "\n",
      "Epoch 00581: Learning rate is 0.0000.\n",
      "Epoch 582/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5525e-07 - mean_squared_logarithmic_error: 1.5525e-07The average loss for epoch 581 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5499e-07 - mean_squared_logarithmic_error: 1.5499e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00582: Learning rate is 0.0000.\n",
      "Epoch 583/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5577e-07 - mean_squared_logarithmic_error: 1.5577e-07The average loss for epoch 582 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00583: Learning rate is 0.0000.\n",
      "Epoch 584/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5600e-07 - mean_squared_logarithmic_error: 1.5600e-07The average loss for epoch 583 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00584: Learning rate is 0.0000.\n",
      "Epoch 585/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5580e-07 - mean_squared_logarithmic_error: 1.5580e-07The average loss for epoch 584 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5511e-07 - mean_squared_logarithmic_error: 1.5511e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00585: Learning rate is 0.0000.\n",
      "Epoch 586/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5578e-07 - mean_squared_logarithmic_error: 1.5578e-07The average loss for epoch 585 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00586: Learning rate is 0.0000.\n",
      "Epoch 587/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 586 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5494e-07 - mean_squared_logarithmic_error: 1.5494e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00587: Learning rate is 0.0000.\n",
      "Epoch 588/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5605e-07 - mean_squared_logarithmic_error: 1.5605e-07The average loss for epoch 587 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5496e-07 - mean_squared_logarithmic_error: 1.5496e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00588: Learning rate is 0.0000.\n",
      "Epoch 589/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5546e-07 - mean_squared_logarithmic_error: 1.5546e-07The average loss for epoch 588 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00589: Learning rate is 0.0000.\n",
      "Epoch 590/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5489e-07 - mean_squared_logarithmic_error: 1.5489e-07The average loss for epoch 589 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5489e-07 - mean_squared_logarithmic_error: 1.5489e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00590: Learning rate is 0.0000.\n",
      "Epoch 591/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5554e-07 - mean_squared_logarithmic_error: 1.5554e-07The average loss for epoch 590 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5504e-07 - mean_squared_logarithmic_error: 1.5504e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00591: Learning rate is 0.0000.\n",
      "Epoch 592/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5610e-07 - mean_squared_logarithmic_error: 1.5610e-07The average loss for epoch 591 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5508e-07 - mean_squared_logarithmic_error: 1.5508e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00592: Learning rate is 0.0000.\n",
      "Epoch 593/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5551e-07 - mean_squared_logarithmic_error: 1.5551e-07The average loss for epoch 592 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5493e-07 - mean_squared_logarithmic_error: 1.5493e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00593: Learning rate is 0.0000.\n",
      "Epoch 594/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5592e-07 - mean_squared_logarithmic_error: 1.5592e-07The average loss for epoch 593 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5499e-07 - mean_squared_logarithmic_error: 1.5499e-07 - val_loss: 1.5267e-04 - val_mean_squared_logarithmic_error: 1.5267e-04\n",
      "\n",
      "Epoch 00594: Learning rate is 0.0000.\n",
      "Epoch 595/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5608e-07 - mean_squared_logarithmic_error: 1.5608e-07The average loss for epoch 594 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5495e-07 - mean_squared_logarithmic_error: 1.5495e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00595: Learning rate is 0.0000.\n",
      "Epoch 596/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5566e-07 - mean_squared_logarithmic_error: 1.5566e-07The average loss for epoch 595 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5493e-07 - mean_squared_logarithmic_error: 1.5493e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00596: Learning rate is 0.0000.\n",
      "Epoch 597/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5546e-07 - mean_squared_logarithmic_error: 1.5546e-07The average loss for epoch 596 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5506e-07 - mean_squared_logarithmic_error: 1.5506e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00597: Learning rate is 0.0000.\n",
      "Epoch 598/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5430e-07 - mean_squared_logarithmic_error: 1.5430e-07The average loss for epoch 597 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5500e-07 - mean_squared_logarithmic_error: 1.5500e-07 - val_loss: 1.5264e-04 - val_mean_squared_logarithmic_error: 1.5264e-04\n",
      "\n",
      "Epoch 00598: Learning rate is 0.0000.\n",
      "Epoch 599/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5551e-07 - mean_squared_logarithmic_error: 1.5551e-07The average loss for epoch 598 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00599: Learning rate is 0.0000.\n",
      "Epoch 600/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5549e-07 - mean_squared_logarithmic_error: 1.5549e-07The average loss for epoch 599 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5493e-07 - mean_squared_logarithmic_error: 1.5493e-07 - val_loss: 1.5263e-04 - val_mean_squared_logarithmic_error: 1.5263e-04\n",
      "\n",
      "Epoch 00600: Learning rate is 0.0000.\n",
      "Epoch 601/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5597e-07 - mean_squared_logarithmic_error: 1.5597e-07The average loss for epoch 600 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00601: Learning rate is 0.0000.\n",
      "Epoch 602/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5590e-07 - mean_squared_logarithmic_error: 1.5590e-07The average loss for epoch 601 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00602: Learning rate is 0.0000.\n",
      "Epoch 603/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.3863e-07 - mean_squared_logarithmic_error: 1.3863e-07The average loss for epoch 602 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00603: Learning rate is 0.0000.\n",
      "Epoch 604/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5485e-07 - mean_squared_logarithmic_error: 1.5485e-07The average loss for epoch 603 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00604: Learning rate is 0.0000.\n",
      "Epoch 605/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07The average loss for epoch 604 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00605: Learning rate is 0.0000.\n",
      "Epoch 606/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07The average loss for epoch 605 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00606: Learning rate is 0.0000.\n",
      "Epoch 607/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5585e-07 - mean_squared_logarithmic_error: 1.5585e-07The average loss for epoch 606 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00607: Learning rate is 0.0000.\n",
      "Epoch 608/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5584e-07 - mean_squared_logarithmic_error: 1.5584e-07The average loss for epoch 607 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00608: Learning rate is 0.0000.\n",
      "Epoch 609/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5480e-07 - mean_squared_logarithmic_error: 1.5480e-07The average loss for epoch 608 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00609: Learning rate is 0.0000.\n",
      "Epoch 610/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5637e-07 - mean_squared_logarithmic_error: 1.5637e-07 ETA: 0s - loss: 1.7294e-07 - mean_squared_logarithmic_eThe average loss for epoch 609 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00610: Learning rate is 0.0000.\n",
      "Epoch 611/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5563e-07 - mean_squared_logarithmic_error: 1.5563e-07The average loss for epoch 610 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00611: Learning rate is 0.0000.\n",
      "Epoch 612/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07The average loss for epoch 611 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00612: Learning rate is 0.0000.\n",
      "Epoch 613/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5531e-07 - mean_squared_logarithmic_error: 1.5531e-07The average loss for epoch 612 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00613: Learning rate is 0.0000.\n",
      "Epoch 614/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5597e-07 - mean_squared_logarithmic_error: 1.5597e-07The average loss for epoch 613 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00614: Learning rate is 0.0000.\n",
      "Epoch 615/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5569e-07 - mean_squared_logarithmic_error: 1.5569e-07The average loss for epoch 614 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00615: Learning rate is 0.0000.\n",
      "Epoch 616/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07The average loss for epoch 615 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00616: Learning rate is 0.0000.\n",
      "Epoch 617/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07The average loss for epoch 616 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00617: Learning rate is 0.0000.\n",
      "Epoch 618/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5303e-07 - mean_squared_logarithmic_error: 1.5303e-07The average loss for epoch 617 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00618: Learning rate is 0.0000.\n",
      "Epoch 619/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5541e-07 - mean_squared_logarithmic_error: 1.5541e-07The average loss for epoch 618 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00619: Learning rate is 0.0000.\n",
      "Epoch 620/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5589e-07 - mean_squared_logarithmic_error: 1.5589e-07The average loss for epoch 619 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00620: Learning rate is 0.0000.\n",
      "Epoch 621/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5550e-07 - mean_squared_logarithmic_error: 1.5550e-07The average loss for epoch 620 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00621: Learning rate is 0.0000.\n",
      "Epoch 622/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5507e-07 - mean_squared_logarithmic_error: 1.5507e-07 ETA: 0s - loss: 1.5803e-07 - mean_squared_logarithmic_error: 1.5803The average loss for epoch 621 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00622: Learning rate is 0.0000.\n",
      "Epoch 623/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.3825e-07 - mean_squared_logarithmic_error: 1.3825e-07The average loss for epoch 622 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00623: Learning rate is 0.0000.\n",
      "Epoch 624/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5470e-07 - mean_squared_logarithmic_error: 1.5470e-07The average loss for epoch 623 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00624: Learning rate is 0.0000.\n",
      "Epoch 625/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5513e-07 - mean_squared_logarithmic_error: 1.5513e-07The average loss for epoch 624 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00625: Learning rate is 0.0000.\n",
      "Epoch 626/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5495e-07 - mean_squared_logarithmic_error: 1.5495e-07The average loss for epoch 625 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00626: Learning rate is 0.0000.\n",
      "Epoch 627/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5519e-07 - mean_squared_logarithmic_error: 1.5519e-07The average loss for epoch 626 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00627: Learning rate is 0.0000.\n",
      "Epoch 628/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.5627e-07 - mean_squared_logarithmic_error: 1.5627e-07The average loss for epoch 627 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00628: Learning rate is 0.0000.\n",
      "Epoch 629/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5341e-07 - mean_squared_logarithmic_error: 1.5341e-07The average loss for epoch 628 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00629: Learning rate is 0.0000.\n",
      "Epoch 630/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5477e-07 - mean_squared_logarithmic_error: 1.5477e-07The average loss for epoch 629 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00630: Learning rate is 0.0000.\n",
      "Epoch 631/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.5600e-07 - mean_squared_logarithmic_error: 1.5600e-07The average loss for epoch 630 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00631: Learning rate is 0.0000.\n",
      "Epoch 632/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 ETA: 0s - loss: 1.3969e-07 - mean_squared_logarithmic_errorThe average loss for epoch 631 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00632: Learning rate is 0.0000.\n",
      "Epoch 633/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07The average loss for epoch 632 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00633: Learning rate is 0.0000.\n",
      "Epoch 634/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07The average loss for epoch 633 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00634: Learning rate is 0.0000.\n",
      "Epoch 635/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5487e-07 - mean_squared_logarithmic_error: 1.5487e-07The average loss for epoch 634 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00635: Learning rate is 0.0000.\n",
      "Epoch 636/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07The average loss for epoch 635 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00636: Learning rate is 0.0000.\n",
      "Epoch 637/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5479e-07 - mean_squared_logarithmic_error: 1.5479e-07The average loss for epoch 636 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00637: Learning rate is 0.0000.\n",
      "Epoch 638/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5601e-07 - mean_squared_logarithmic_error: 1.5601e-07The average loss for epoch 637 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00638: Learning rate is 0.0000.\n",
      "Epoch 639/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5510e-07 - mean_squared_logarithmic_error: 1.5510e-07The average loss for epoch 638 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00639: Learning rate is 0.0000.\n",
      "Epoch 640/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5521e-07 - mean_squared_logarithmic_error: 1.5521e-07The average loss for epoch 639 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00640: Learning rate is 0.0000.\n",
      "Epoch 641/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5552e-07 - mean_squared_logarithmic_error: 1.5552e-07The average loss for epoch 640 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00641: Learning rate is 0.0000.\n",
      "Epoch 642/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5547e-07 - mean_squared_logarithmic_error: 1.5547e-07The average loss for epoch 641 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00642: Learning rate is 0.0000.\n",
      "Epoch 643/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5553e-07 - mean_squared_logarithmic_error: 1.5553e-07The average loss for epoch 642 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00643: Learning rate is 0.0000.\n",
      "Epoch 644/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5532e-07 - mean_squared_logarithmic_error: 1.5532e-07The average loss for epoch 643 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00644: Learning rate is 0.0000.\n",
      "Epoch 645/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.3853e-07 - mean_squared_logarithmic_error: 1.3853e-07The average loss for epoch 644 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00645: Learning rate is 0.0000.\n",
      "Epoch 646/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5607e-07 - mean_squared_logarithmic_error: 1.5607e-07The average loss for epoch 645 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00646: Learning rate is 0.0000.\n",
      "Epoch 647/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 646 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00647: Learning rate is 0.0000.\n",
      "Epoch 648/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5501e-07 - mean_squared_logarithmic_error: 1.5501e-07The average loss for epoch 647 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00648: Learning rate is 0.0000.\n",
      "Epoch 649/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07The average loss for epoch 648 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00649: Learning rate is 0.0000.\n",
      "Epoch 650/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5602e-07 - mean_squared_logarithmic_error: 1.5602e-07The average loss for epoch 649 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00650: Learning rate is 0.0000.\n",
      "Epoch 651/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07The average loss for epoch 650 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00651: Learning rate is 0.0000.\n",
      "Epoch 652/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5491e-07 - mean_squared_logarithmic_error: 1.5491e-07The average loss for epoch 651 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00652: Learning rate is 0.0000.\n",
      "Epoch 653/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5536e-07 - mean_squared_logarithmic_error: 1.5536e-07The average loss for epoch 652 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00653: Learning rate is 0.0000.\n",
      "Epoch 654/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5501e-07 - mean_squared_logarithmic_error: 1.5501e-07The average loss for epoch 653 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00654: Learning rate is 0.0000.\n",
      "Epoch 655/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5510e-07 - mean_squared_logarithmic_error: 1.5510e-07The average loss for epoch 654 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00655: Learning rate is 0.0000.\n",
      "Epoch 656/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5633e-07 - mean_squared_logarithmic_error: 1.5633e-07The average loss for epoch 655 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00656: Learning rate is 0.0000.\n",
      "Epoch 657/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5470e-07 - mean_squared_logarithmic_error: 1.5470e-07The average loss for epoch 656 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00657: Learning rate is 0.0000.\n",
      "Epoch 658/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5535e-07 - mean_squared_logarithmic_error: 1.5535e-07The average loss for epoch 657 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00658: Learning rate is 0.0000.\n",
      "Epoch 659/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5598e-07 - mean_squared_logarithmic_error: 1.5598e-07The average loss for epoch 658 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00659: Learning rate is 0.0000.\n",
      "Epoch 660/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5476e-07 - mean_squared_logarithmic_error: 1.5476e-07The average loss for epoch 659 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00660: Learning rate is 0.0000.\n",
      "Epoch 661/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5562e-07 - mean_squared_logarithmic_error: 1.5562e-07The average loss for epoch 660 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00661: Learning rate is 0.0000.\n",
      "Epoch 662/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.5614e-07 - mean_squared_logarithmic_error: 1.5614e-07The average loss for epoch 661 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00662: Learning rate is 0.0000.\n",
      "Epoch 663/800\n",
      "248/264 [===========================>..] - ETA: 0s - loss: 1.5601e-07 - mean_squared_logarithmic_error: 1.5601e-07The average loss for epoch 662 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00663: Learning rate is 0.0000.\n",
      "Epoch 664/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5459e-07 - mean_squared_logarithmic_error: 1.5459e-07The average loss for epoch 663 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00664: Learning rate is 0.0000.\n",
      "Epoch 665/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07The average loss for epoch 664 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00665: Learning rate is 0.0000.\n",
      "Epoch 666/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5548e-07 - mean_squared_logarithmic_error: 1.5548e-07The average loss for epoch 665 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00666: Learning rate is 0.0000.\n",
      "Epoch 667/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 666 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00667: Learning rate is 0.0000.\n",
      "Epoch 668/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5545e-07 - mean_squared_logarithmic_error: 1.5545e-07The average loss for epoch 667 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00668: Learning rate is 0.0000.\n",
      "Epoch 669/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5584e-07 - mean_squared_logarithmic_error: 1.5584e-07The average loss for epoch 668 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00669: Learning rate is 0.0000.\n",
      "Epoch 670/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5604e-07 - mean_squared_logarithmic_error: 1.5604e-07The average loss for epoch 669 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00670: Learning rate is 0.0000.\n",
      "Epoch 671/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.3876e-07 - mean_squared_logarithmic_error: 1.3876e-07The average loss for epoch 670 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00671: Learning rate is 0.0000.\n",
      "Epoch 672/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5520e-07 - mean_squared_logarithmic_error: 1.5520e-07The average loss for epoch 671 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00672: Learning rate is 0.0000.\n",
      "Epoch 673/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07The average loss for epoch 672 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00673: Learning rate is 0.0000.\n",
      "Epoch 674/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5572e-07 - mean_squared_logarithmic_error: 1.5572e-07The average loss for epoch 673 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00674: Learning rate is 0.0000.\n",
      "Epoch 675/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07The average loss for epoch 674 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00675: Learning rate is 0.0000.\n",
      "Epoch 676/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5609e-07 - mean_squared_logarithmic_error: 1.5609e-07The average loss for epoch 675 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00676: Learning rate is 0.0000.\n",
      "Epoch 677/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5481e-07 - mean_squared_logarithmic_error: 1.5481e-07The average loss for epoch 676 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00677: Learning rate is 0.0000.\n",
      "Epoch 678/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 677 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00678: Learning rate is 0.0000.\n",
      "Epoch 679/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5581e-07 - mean_squared_logarithmic_error: 1.5581e-07The average loss for epoch 678 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00679: Learning rate is 0.0000.\n",
      "Epoch 680/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5470e-07 - mean_squared_logarithmic_error: 1.5470e-07The average loss for epoch 679 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00680: Learning rate is 0.0000.\n",
      "Epoch 681/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5458e-07 - mean_squared_logarithmic_error: 1.5458e-07The average loss for epoch 680 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00681: Learning rate is 0.0000.\n",
      "Epoch 682/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07The average loss for epoch 681 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00682: Learning rate is 0.0000.\n",
      "Epoch 683/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5545e-07 - mean_squared_logarithmic_error: 1.5545e-07 ETA: 0s - loss: 1.3854e-07 - mean_squared_logarithmic_errorThe average loss for epoch 682 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00683: Learning rate is 0.0000.\n",
      "Epoch 684/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5490e-07 - mean_squared_logarithmic_error: 1.5490e-07The average loss for epoch 683 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00684: Learning rate is 0.0000.\n",
      "Epoch 685/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5575e-07 - mean_squared_logarithmic_error: 1.5575e-07The average loss for epoch 684 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00685: Learning rate is 0.0000.\n",
      "Epoch 686/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5552e-07 - mean_squared_logarithmic_error: 1.5552e-07The average loss for epoch 685 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00686: Learning rate is 0.0000.\n",
      "Epoch 687/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07The average loss for epoch 686 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00687: Learning rate is 0.0000.\n",
      "Epoch 688/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5534e-07 - mean_squared_logarithmic_error: 1.5534e-07The average loss for epoch 687 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00688: Learning rate is 0.0000.\n",
      "Epoch 689/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5552e-07 - mean_squared_logarithmic_error: 1.5552e-07The average loss for epoch 688 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00689: Learning rate is 0.0000.\n",
      "Epoch 690/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5483e-07 - mean_squared_logarithmic_error: 1.5483e-07The average loss for epoch 689 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00690: Learning rate is 0.0000.\n",
      "Epoch 691/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5557e-07 - mean_squared_logarithmic_error: 1.5557e-07The average loss for epoch 690 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00691: Learning rate is 0.0000.\n",
      "Epoch 692/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5582e-07 - mean_squared_logarithmic_error: 1.5582e-07The average loss for epoch 691 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00692: Learning rate is 0.0000.\n",
      "Epoch 693/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07The average loss for epoch 692 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00693: Learning rate is 0.0000.\n",
      "Epoch 694/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5456e-07 - mean_squared_logarithmic_error: 1.5456e-07The average loss for epoch 693 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00694: Learning rate is 0.0000.\n",
      "Epoch 695/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5593e-07 - mean_squared_logarithmic_error: 1.5593e-07The average loss for epoch 694 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00695: Learning rate is 0.0000.\n",
      "Epoch 696/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.3869e-07 - mean_squared_logarithmic_error: 1.3869e-07The average loss for epoch 695 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00696: Learning rate is 0.0000.\n",
      "Epoch 697/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5569e-07 - mean_squared_logarithmic_error: 1.5569e-07The average loss for epoch 696 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00697: Learning rate is 0.0000.\n",
      "Epoch 698/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5467e-07 - mean_squared_logarithmic_error: 1.5467e-07The average loss for epoch 697 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00698: Learning rate is 0.0000.\n",
      "Epoch 699/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5479e-07 - mean_squared_logarithmic_error: 1.5479e-07The average loss for epoch 698 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00699: Learning rate is 0.0000.\n",
      "Epoch 700/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5532e-07 - mean_squared_logarithmic_error: 1.5532e-07The average loss for epoch 699 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00700: Learning rate is 0.0000.\n",
      "Epoch 701/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07The average loss for epoch 700 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00701: Learning rate is 0.0000.\n",
      "Epoch 702/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5279e-07 - mean_squared_logarithmic_error: 1.5279e-07The average loss for epoch 701 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00702: Learning rate is 0.0000.\n",
      "Epoch 703/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5621e-07 - mean_squared_logarithmic_error: 1.5621e-07The average loss for epoch 702 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00703: Learning rate is 0.0000.\n",
      "Epoch 704/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5458e-07 - mean_squared_logarithmic_error: 1.5458e-07The average loss for epoch 703 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00704: Learning rate is 0.0000.\n",
      "Epoch 705/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07The average loss for epoch 704 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00705: Learning rate is 0.0000.\n",
      "Epoch 706/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5473e-07 - mean_squared_logarithmic_error: 1.5473e-07The average loss for epoch 705 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00706: Learning rate is 0.0000.\n",
      "Epoch 707/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5487e-07 - mean_squared_logarithmic_error: 1.5487e-07The average loss for epoch 706 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00707: Learning rate is 0.0000.\n",
      "Epoch 708/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07The average loss for epoch 707 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00708: Learning rate is 0.0000.\n",
      "Epoch 709/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5528e-07 - mean_squared_logarithmic_error: 1.5528e-07The average loss for epoch 708 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00709: Learning rate is 0.0000.\n",
      "Epoch 710/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5546e-07 - mean_squared_logarithmic_error: 1.5546e-07 ETA: 0s - loss: 1.7845e-07 - mean_squared_logarithmThe average loss for epoch 709 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00710: Learning rate is 0.0000.\n",
      "Epoch 711/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5532e-07 - mean_squared_logarithmic_error: 1.5532e-07The average loss for epoch 710 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00711: Learning rate is 0.0000.\n",
      "Epoch 712/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5541e-07 - mean_squared_logarithmic_error: 1.5541e-07The average loss for epoch 711 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00712: Learning rate is 0.0000.\n",
      "Epoch 713/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07The average loss for epoch 712 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00713: Learning rate is 0.0000.\n",
      "Epoch 714/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5525e-07 - mean_squared_logarithmic_error: 1.5525e-07The average loss for epoch 713 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00714: Learning rate is 0.0000.\n",
      "Epoch 715/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5474e-07 - mean_squared_logarithmic_error: 1.5474e-07The average loss for epoch 714 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00715: Learning rate is 0.0000.\n",
      "Epoch 716/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5483e-07 - mean_squared_logarithmic_error: 1.5483e-07The average loss for epoch 715 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00716: Learning rate is 0.0000.\n",
      "Epoch 717/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5503e-07 - mean_squared_logarithmic_error: 1.5503e-07The average loss for epoch 716 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00717: Learning rate is 0.0000.\n",
      "Epoch 718/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5475e-07 - mean_squared_logarithmic_error: 1.5475e-07The average loss for epoch 717 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00718: Learning rate is 0.0000.\n",
      "Epoch 719/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5549e-07 - mean_squared_logarithmic_error: 1.5549e-07The average loss for epoch 718 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00719: Learning rate is 0.0000.\n",
      "Epoch 720/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.3857e-07 - mean_squared_logarithmic_error: 1.3857e-07The average loss for epoch 719 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00720: Learning rate is 0.0000.\n",
      "Epoch 721/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5528e-07 - mean_squared_logarithmic_error: 1.5528e-07The average loss for epoch 720 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00721: Learning rate is 0.0000.\n",
      "Epoch 722/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5570e-07 - mean_squared_logarithmic_error: 1.5570e-07The average loss for epoch 721 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00722: Learning rate is 0.0000.\n",
      "Epoch 723/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5540e-07 - mean_squared_logarithmic_error: 1.5540e-07The average loss for epoch 722 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00723: Learning rate is 0.0000.\n",
      "Epoch 724/800\n",
      "264/264 [==============================] - ETA: 0s - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07The average loss for epoch 723 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00724: Learning rate is 0.0000.\n",
      "Epoch 725/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5496e-07 - mean_squared_logarithmic_error: 1.5496e-07The average loss for epoch 724 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00725: Learning rate is 0.0000.\n",
      "Epoch 726/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5483e-07 - mean_squared_logarithmic_error: 1.5483e-07The average loss for epoch 725 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00726: Learning rate is 0.0000.\n",
      "Epoch 727/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5491e-07 - mean_squared_logarithmic_error: 1.5491e-07The average loss for epoch 726 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00727: Learning rate is 0.0000.\n",
      "Epoch 728/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263/264 [============================>.] - ETA: 0s - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07The average loss for epoch 727 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 6ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00728: Learning rate is 0.0000.\n",
      "Epoch 729/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5479e-07 - mean_squared_logarithmic_error: 1.5479e-07The average loss for epoch 728 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00729: Learning rate is 0.0000.\n",
      "Epoch 730/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5495e-07 - mean_squared_logarithmic_error: 1.5495e-07The average loss for epoch 729 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00730: Learning rate is 0.0000.\n",
      "Epoch 731/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5475e-07 - mean_squared_logarithmic_error: 1.5475e-07The average loss for epoch 730 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00731: Learning rate is 0.0000.\n",
      "Epoch 732/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07The average loss for epoch 731 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 7ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00732: Learning rate is 0.0000.\n",
      "Epoch 733/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5487e-07 - mean_squared_logarithmic_error: 1.5487e-07The average loss for epoch 732 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00733: Learning rate is 0.0000.\n",
      "Epoch 734/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5578e-07 - mean_squared_logarithmic_error: 1.5578e-07The average loss for epoch 733 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00734: Learning rate is 0.0000.\n",
      "Epoch 735/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5498e-07 - mean_squared_logarithmic_error: 1.5498e-07The average loss for epoch 734 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00735: Learning rate is 0.0000.\n",
      "Epoch 736/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5501e-07 - mean_squared_logarithmic_error: 1.5501e-07The average loss for epoch 735 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00736: Learning rate is 0.0000.\n",
      "Epoch 737/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5522e-07 - mean_squared_logarithmic_error: 1.5522e-07The average loss for epoch 736 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00737: Learning rate is 0.0000.\n",
      "Epoch 738/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5538e-07 - mean_squared_logarithmic_error: 1.5538e-07The average loss for epoch 737 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00738: Learning rate is 0.0000.\n",
      "Epoch 739/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5476e-07 - mean_squared_logarithmic_error: 1.5476e-07The average loss for epoch 738 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 8ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00739: Learning rate is 0.0000.\n",
      "Epoch 740/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5489e-07 - mean_squared_logarithmic_error: 1.5489e-07The average loss for epoch 739 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00740: Learning rate is 0.0000.\n",
      "Epoch 741/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5493e-07 - mean_squared_logarithmic_error: 1.5493e-07The average loss for epoch 740 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00741: Learning rate is 0.0000.\n",
      "Epoch 742/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5593e-07 - mean_squared_logarithmic_error: 1.5593e-07The average loss for epoch 741 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 2s 6ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00742: Learning rate is 0.0000.\n",
      "Epoch 743/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5495e-07 - mean_squared_logarithmic_error: 1.5495e-07The average loss for epoch 742 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00743: Learning rate is 0.0000.\n",
      "Epoch 744/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5486e-07 - mean_squared_logarithmic_error: 1.5486e-07The average loss for epoch 743 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00744: Learning rate is 0.0000.\n",
      "Epoch 745/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5481e-07 - mean_squared_logarithmic_error: 1.5481e-07The average loss for epoch 744 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00745: Learning rate is 0.0000.\n",
      "Epoch 746/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5489e-07 - mean_squared_logarithmic_error: 1.5489e-07The average loss for epoch 745 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00746: Learning rate is 0.0000.\n",
      "Epoch 747/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5527e-07 - mean_squared_logarithmic_error: 1.5527e-07The average loss for epoch 746 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00747: Learning rate is 0.0000.\n",
      "Epoch 748/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5562e-07 - mean_squared_logarithmic_error: 1.5562e-07The average loss for epoch 747 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00748: Learning rate is 0.0000.\n",
      "Epoch 749/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5520e-07 - mean_squared_logarithmic_error: 1.5520e-07The average loss for epoch 748 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5266e-04 - val_mean_squared_logarithmic_error: 1.5266e-04\n",
      "\n",
      "Epoch 00749: Learning rate is 0.0000.\n",
      "Epoch 750/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5524e-07 - mean_squared_logarithmic_error: 1.5524e-07The average loss for epoch 749 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00750: Learning rate is 0.0000.\n",
      "Epoch 751/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5613e-07 - mean_squared_logarithmic_error: 1.5613e-07The average loss for epoch 750 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00751: Learning rate is 0.0000.\n",
      "Epoch 752/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5428e-07 - mean_squared_logarithmic_error: 1.5428e-07The average loss for epoch 751 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00752: Learning rate is 0.0000.\n",
      "Epoch 753/800\n",
      "249/264 [===========================>..] - ETA: 0s - loss: 1.5598e-07 - mean_squared_logarithmic_error: 1.5598e-07The average loss for epoch 752 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00753: Learning rate is 0.0000.\n",
      "Epoch 754/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5486e-07 - mean_squared_logarithmic_error: 1.5486e-07The average loss for epoch 753 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00754: Learning rate is 0.0000.\n",
      "Epoch 755/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5544e-07 - mean_squared_logarithmic_error: 1.5544e-07The average loss for epoch 754 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00755: Learning rate is 0.0000.\n",
      "Epoch 756/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5502e-07 - mean_squared_logarithmic_error: 1.5502e-07The average loss for epoch 755 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00756: Learning rate is 0.0000.\n",
      "Epoch 757/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5527e-07 - mean_squared_logarithmic_error: 1.5527e-07The average loss for epoch 756 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00757: Learning rate is 0.0000.\n",
      "Epoch 758/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5482e-07 - mean_squared_logarithmic_error: 1.5482e-07The average loss for epoch 757 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00758: Learning rate is 0.0000.\n",
      "Epoch 759/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5470e-07 - mean_squared_logarithmic_error: 1.5470e-07The average loss for epoch 758 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00759: Learning rate is 0.0000.\n",
      "Epoch 760/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5540e-07 - mean_squared_logarithmic_error: 1.5540e-07The average loss for epoch 759 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00760: Learning rate is 0.0000.\n",
      "Epoch 761/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5516e-07 - mean_squared_logarithmic_error: 1.5516e-07The average loss for epoch 760 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00761: Learning rate is 0.0000.\n",
      "Epoch 762/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5492e-07 - mean_squared_logarithmic_error: 1.5492e-07The average loss for epoch 761 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00762: Learning rate is 0.0000.\n",
      "Epoch 763/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5505e-07 - mean_squared_logarithmic_error: 1.5505e-07The average loss for epoch 762 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00763: Learning rate is 0.0000.\n",
      "Epoch 764/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5567e-07 - mean_squared_logarithmic_error: 1.5567e-07The average loss for epoch 763 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00764: Learning rate is 0.0000.\n",
      "Epoch 765/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5468e-07 - mean_squared_logarithmic_error: 1.5468e-07The average loss for epoch 764 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00765: Learning rate is 0.0000.\n",
      "Epoch 766/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5315e-07 - mean_squared_logarithmic_error: 1.5315e-07The average loss for epoch 765 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 3ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00766: Learning rate is 0.0000.\n",
      "Epoch 767/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5510e-07 - mean_squared_logarithmic_error: 1.5510e-07The average loss for epoch 766 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00767: Learning rate is 0.0000.\n",
      "Epoch 768/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5515e-07 - mean_squared_logarithmic_error: 1.5515e-07The average loss for epoch 767 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00768: Learning rate is 0.0000.\n",
      "Epoch 769/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5480e-07 - mean_squared_logarithmic_error: 1.5480e-07The average loss for epoch 768 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00769: Learning rate is 0.0000.\n",
      "Epoch 770/800\n",
      "255/264 [===========================>..] - ETA: 0s - loss: 1.5583e-07 - mean_squared_logarithmic_error: 1.5583e-07 ETA: 0s - loss: 2.0509e-07 - mean_sThe average loss for epoch 769 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 5ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00770: Learning rate is 0.0000.\n",
      "Epoch 771/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5533e-07 - mean_squared_logarithmic_error: 1.5533e-07The average loss for epoch 770 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00771: Learning rate is 0.0000.\n",
      "Epoch 772/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5466e-07 - mean_squared_logarithmic_error: 1.5466e-07The average loss for epoch 771 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00772: Learning rate is 0.0000.\n",
      "Epoch 773/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5552e-07 - mean_squared_logarithmic_error: 1.5552e-07The average loss for epoch 772 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00773: Learning rate is 0.0000.\n",
      "Epoch 774/800\n",
      "257/264 [============================>.] - ETA: 0s - loss: 1.5513e-07 - mean_squared_logarithmic_error: 1.5513e-07The average loss for epoch 773 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00774: Learning rate is 0.0000.\n",
      "Epoch 775/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5549e-07 - mean_squared_logarithmic_error: 1.5549e-07The average loss for epoch 774 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00775: Learning rate is 0.0000.\n",
      "Epoch 776/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5632e-07 - mean_squared_logarithmic_error: 1.5632e-07The average loss for epoch 775 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00776: Learning rate is 0.0000.\n",
      "Epoch 777/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5490e-07 - mean_squared_logarithmic_error: 1.5490e-07The average loss for epoch 776 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00777: Learning rate is 0.0000.\n",
      "Epoch 778/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5509e-07 - mean_squared_logarithmic_error: 1.5509e-07The average loss for epoch 777 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00778: Learning rate is 0.0000.\n",
      "Epoch 779/800\n",
      "252/264 [===========================>..] - ETA: 0s - loss: 1.5554e-07 - mean_squared_logarithmic_error: 1.5554e-07The average loss for epoch 778 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00779: Learning rate is 0.0000.\n",
      "Epoch 780/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5563e-07 - mean_squared_logarithmic_error: 1.5563e-07The average loss for epoch 779 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00780: Learning rate is 0.0000.\n",
      "Epoch 781/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5564e-07 - mean_squared_logarithmic_error: 1.5564e-07The average loss for epoch 780 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00781: Learning rate is 0.0000.\n",
      "Epoch 782/800\n",
      "251/264 [===========================>..] - ETA: 0s - loss: 1.5535e-07 - mean_squared_logarithmic_error: 1.5535e-07The average loss for epoch 781 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00782: Learning rate is 0.0000.\n",
      "Epoch 783/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5501e-07 - mean_squared_logarithmic_error: 1.5501e-07The average loss for epoch 782 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00783: Learning rate is 0.0000.\n",
      "Epoch 784/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5531e-07 - mean_squared_logarithmic_error: 1.5531e-07The average loss for epoch 783 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00784: Learning rate is 0.0000.\n",
      "Epoch 785/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5494e-07 - mean_squared_logarithmic_error: 1.5494e-07The average loss for epoch 784 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00785: Learning rate is 0.0000.\n",
      "Epoch 786/800\n",
      "250/264 [===========================>..] - ETA: 0s - loss: 1.5592e-07 - mean_squared_logarithmic_error: 1.5592e-07The average loss for epoch 785 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00786: Learning rate is 0.0000.\n",
      "Epoch 787/800\n",
      "263/264 [============================>.] - ETA: 0s - loss: 1.5465e-07 - mean_squared_logarithmic_error: 1.5465e-07The average loss for epoch 786 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00787: Learning rate is 0.0000.\n",
      "Epoch 788/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5470e-07 - mean_squared_logarithmic_error: 1.5470e-07The average loss for epoch 787 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00788: Learning rate is 0.0000.\n",
      "Epoch 789/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5532e-07 - mean_squared_logarithmic_error: 1.5532e-07The average loss for epoch 788 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00789: Learning rate is 0.0000.\n",
      "Epoch 790/800\n",
      "253/264 [===========================>..] - ETA: 0s - loss: 1.5437e-07 - mean_squared_logarithmic_error: 1.5437e-07The average loss for epoch 789 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00790: Learning rate is 0.0000.\n",
      "Epoch 791/800\n",
      "262/264 [============================>.] - ETA: 0s - loss: 1.5483e-07 - mean_squared_logarithmic_error: 1.5483e-07The average loss for epoch 790 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00791: Learning rate is 0.0000.\n",
      "Epoch 792/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5476e-07 - mean_squared_logarithmic_error: 1.5476e-07The average loss for epoch 791 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00792: Learning rate is 0.0000.\n",
      "Epoch 793/800\n",
      "254/264 [===========================>..] - ETA: 0s - loss: 1.5555e-07 - mean_squared_logarithmic_error: 1.5555e-07The average loss for epoch 792 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00793: Learning rate is 0.0000.\n",
      "Epoch 794/800\n",
      "259/264 [============================>.] - ETA: 0s - loss: 1.5516e-07 - mean_squared_logarithmic_error: 1.5516e-07The average loss for epoch 793 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00794: Learning rate is 0.0000.\n",
      "Epoch 795/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5495e-07 - mean_squared_logarithmic_error: 1.5495e-07The average loss for epoch 794 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5464e-07 - mean_squared_logarithmic_error: 1.5464e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00795: Learning rate is 0.0000.\n",
      "Epoch 796/800\n",
      "258/264 [============================>.] - ETA: 0s - loss: 1.5546e-07 - mean_squared_logarithmic_error: 1.5546e-07The average loss for epoch 795 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5461e-07 - mean_squared_logarithmic_error: 1.5461e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00796: Learning rate is 0.0000.\n",
      "Epoch 797/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5469e-07 - mean_squared_logarithmic_error: 1.5469e-07The average loss for epoch 796 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00797: Learning rate is 0.0000.\n",
      "Epoch 798/800\n",
      "256/264 [============================>.] - ETA: 0s - loss: 1.5546e-07 - mean_squared_logarithmic_error: 1.5546e-07The average loss for epoch 797 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00798: Learning rate is 0.0000.\n",
      "Epoch 799/800\n",
      "260/264 [============================>.] - ETA: 0s - loss: 1.5514e-07 - mean_squared_logarithmic_error: 1.5514e-07The average loss for epoch 798 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5463e-07 - mean_squared_logarithmic_error: 1.5463e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n",
      "\n",
      "Epoch 00799: Learning rate is 0.0000.\n",
      "Epoch 800/800\n",
      "261/264 [============================>.] - ETA: 0s - loss: 1.5512e-07 - mean_squared_logarithmic_error: 1.5512e-07The average loss for epoch 799 is    0.00 and MSE is    0.00.\n",
      "264/264 [==============================] - 1s 4ms/step - loss: 1.5462e-07 - mean_squared_logarithmic_error: 1.5462e-07 - val_loss: 1.5265e-04 - val_mean_squared_logarithmic_error: 1.5265e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16f4f90a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keff_model.fit(X_train,y_train.T, epochs=800, batch_size=133, verbose=1,\n",
    "               shuffle=True,callbacks=[LossAndErrorPrintingCallback(),\n",
    "        CustomLearningRateScheduler(lr_schedule)],\n",
    "               validation_data=(X_val,y_val.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013637822\n",
      "[[1.9424855 1.8810194 1.7897842]\n",
      " [1.9422452 1.8854885 1.8009222]\n",
      " [1.9515618 1.893796  1.8078206]\n",
      " ...\n",
      " [1.9201685 1.8615544 1.7744462]\n",
      " [1.8779131 1.8172464 1.7275801]\n",
      " [1.9130841 1.8489418 1.7554371]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = keff_model.predict(X_test)\n",
    "metric = tf.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
    "metric.update_state(np.array(y_predicted*normConst),np.array(y_test.T*normConst))\n",
    "print(metric.result().numpy())\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating dataset\n",
    "a = (y_predicted-y_test.T)\n",
    "print(a.shape)\n",
    "# Creating histogram\n",
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "ax.hist(a[:,2],bins=70,alpha=0.75,label=\"EOL\",color='r')\n",
    "ax.hist(a[:,1],bins=70,alpha=0.25,label=\"MOL\",color='b')\n",
    "ax.hist(a[:,0],bins=70,alpha=0.25,label=\"BOL\",color='g')\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.title(\"LWR $k_{\\infty}$, eq leth serialization\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"PICS/SFR_completeData_eqleth_boxlog_raytune_relu_wide.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16186506663871794\n",
      "0.036416457500692126\n",
      "0.16571913677010885\n",
      "0.03687024270129258\n",
      "0.16685606944661246\n",
      "0.03746634616229854\n"
     ]
    }
   ],
   "source": [
    "print(np.std(X_test[:,0]))\n",
    "print(np.std(a[:,0]))\n",
    "print(np.std(X_test[:,1]))\n",
    "print(np.std(a[:,1]))\n",
    "print(np.std(X_test[:,2]))\n",
    "print(np.std(a[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9424855 1.9422452 1.9515618 ... 1.9201685 1.8779131 1.9130841]\n",
      "(9999, 3)\n",
      "(3, 9999)\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted[:,0])\n",
    "print(y_predicted.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    " \n",
    "\n",
    "n_bins = 500\n",
    " \n",
    "\n",
    "# Creating histogram\n",
    "fig, axs = plt.subplots(figsize =(10, 7))\n",
    " \n",
    "axs.hist(y_test.T[:,0], bins = n_bins,label=\"BOL actual\")\n",
    "axs.hist(y_predicted[:,0], bins = n_bins,label=\"BOL predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.title(\"SFR $k_{\\infty}$, eq leth serialization\")\n",
    "plt.savefig(\"kinfPredDist_boxlog_raytune_relu_wide.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "\n",
    "plt.show()\n",
    "# Show plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
