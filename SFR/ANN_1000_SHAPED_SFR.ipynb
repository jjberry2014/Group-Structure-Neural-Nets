{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "1.0.2\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import Utilities\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import kerastuner\n",
    "print(tf.__version__)\n",
    "print(kerastuner.__version__)\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "    #    print(\n",
    "    #        \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "    #    )\n",
    "        return\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "     #   print(\n",
    "     #       \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "     #   )\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and MSE is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_squared_logarithmic_error\"]#logs[\"mean_squared_logarithmic_error\"]\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Sphere           LWR           SFR\n",
      "0     2.615712e+02  1.000000e-03  1.000000e-03\n",
      "1     5.737464e+02  6.865624e-03  6.865624e-03\n",
      "2     8.888122e+02  9.160728e-03  9.160728e-03\n",
      "3     1.195336e+03  1.107134e-02  1.107134e-02\n",
      "4     1.481597e+03  1.273122e-02  1.273122e-02\n",
      "...            ...           ...           ...\n",
      "997   4.267269e+06  7.188363e+06  7.188363e+06\n",
      "998   4.568146e+06  7.556836e+06  7.556836e+06\n",
      "999   4.976225e+06  8.021766e+06  8.021766e+06\n",
      "1000  5.607251e+06  8.701618e+06  8.701618e+06\n",
      "1001  7.020693e+06           NaN           NaN\n",
      "\n",
      "[1002 rows x 3 columns]\n",
      "0       1.000000e-03\n",
      "1       6.865624e-03\n",
      "2       9.160728e-03\n",
      "3       1.107134e-02\n",
      "4       1.273122e-02\n",
      "            ...     \n",
      "997     7.188363e+06\n",
      "998     7.556836e+06\n",
      "999     8.021766e+06\n",
      "1000    8.701618e+06\n",
      "1001             NaN\n",
      "Name: SFR, Length: 1002, dtype: float64\n",
      "[1.00000000e-03 6.86562376e-03 9.16072825e-03 ... 7.55683605e+06\n",
      " 8.02176620e+06 8.70161772e+06]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel ('/Volumes/data/LosAlamosSummer/Serializationstructures.xlsx')\n",
    "print (df)\n",
    "print(df.SFR)\n",
    "SS=np.array(df.SFR[:-1])\n",
    "print(SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading In Data\n",
      "Finished Loading Data\n"
     ]
    }
   ],
   "source": [
    "#datapath = '/Users/jessiejo/data/VBUDS/GroupStructurePaper/NeuralNetworks/All_Libraries/NewDataSetFull1.mat'\n",
    "datapath='/Volumes/data/LosAlamosSummer/SFR/DATA/SFR_data_6.mat'\n",
    "print('Loading In Data')\n",
    "kinfBOL,kinfMOL,kinfEOL,GS=Utilities.LoadData(datapath,1)\n",
    "#MakeGroupDensity(X, nDecades)\n",
    "Nfeatures = 1000;\n",
    "allData= Utilities.ProcessData(datapath, 1,1000,1,SS,1)\n",
    "# allData: (100,000x1,000) y_direct: (100,000x3)\n",
    "print('Finished Loading Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41021,)\n",
      "41021\n",
      "(41021, 3)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q -U keras-tuner\n",
    "print(kinfBOL.shape)\n",
    "print(len(kinfBOL))\n",
    "kinf=np.array(np.zeros((len(kinfBOL),3)))\n",
    "kinf[:,0]=kinfBOL\n",
    "kinf[:,1]=kinfMOL\n",
    "kinf[:,2]=kinfEOL#np.concatenate((kinfBOL,kinfMOL,kinfEOL),axis=0)\n",
    "print(kinf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8204)\n",
      "(3, 4102)\n",
      "(41021, 3)\n",
      "(3, 28713)\n"
     ]
    }
   ],
   "source": [
    "Nsamples,Ndecades = allData.shape\n",
    "vldF=.1\n",
    "testF=.2\n",
    "normConst=1#np.linalg.norm(kinf)\n",
    "y_norm=np.array(kinf/normConst)\n",
    "\n",
    "X, X_test, y, y_test, vldF_corr = Utilities.makeFractions(Nsamples, vldF, testF, allData, y_norm, 1)\n",
    "\n",
    "\n",
    "NtrainingSamples = int(Nsamples*(1 - testF))\n",
    "tranValSplit=int(NtrainingSamples*(1-vldF_corr))\n",
    "X_train=X[:tranValSplit,:]\n",
    "y_train=y[:,:tranValSplit]\n",
    "X_val=X[tranValSplit+1:,:]\n",
    "y_val=y[:,tranValSplit+1:]\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_norm.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (200, 0.0001),\n",
    "    (400, 0.00001),\n",
    "    (500, 0.000001),\n",
    "    (600,0.0000001),\n",
    "]\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 22)                22022     \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 79)                1817      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 240       \n",
      "=================================================================\n",
      "Total params: 24,079\n",
      "Trainable params: 24,079\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keff_model = tf.keras.Sequential([\n",
    "    layers.Dense(22, activation='relu', name='hidden_1', input_dim=1000),\n",
    "    layers.Dense(79, activation='relu',  name='hidden_2'),\n",
    "    layers.Dense(3, activation='linear',name='output')])\n",
    "keff_model.compile(loss=\"mean_squared_logarithmic_error\",optimizer=tf.keras.optimizers.Adam(1e-03),metrics=\"mean_squared_logarithmic_error\")\n",
    "keff_model.build()\n",
    "keff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.0010.\n",
      "Epoch 1/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 0.0316 - mean_squared_logarithmic_error: 0.0316The average loss for epoch 0 is    0.03 and MSE is    0.03.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 0.0309 - mean_squared_logarithmic_error: 0.0309 - val_loss: 0.0043 - val_mean_squared_logarithmic_error: 0.0043\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0010.\n",
      "Epoch 2/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 0.0022 - mean_squared_logarithmic_error: 0.0022The average loss for epoch 1 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 0.0022 - mean_squared_logarithmic_error: 0.0022 - val_loss: 5.8728e-04 - val_mean_squared_logarithmic_error: 5.8728e-04\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0010.\n",
      "Epoch 3/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 3.8862e-04 - mean_squared_logarithmic_error: 3.8862e-04The average loss for epoch 2 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 3.8502e-04 - mean_squared_logarithmic_error: 3.8502e-04 - val_loss: 2.6972e-04 - val_mean_squared_logarithmic_error: 2.6972e-04\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0010.\n",
      "Epoch 4/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.2051e-04 - mean_squared_logarithmic_error: 2.2051e-04The average loss for epoch 3 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 999us/step - loss: 2.2031e-04 - mean_squared_logarithmic_error: 2.2031e-04 - val_loss: 2.8932e-04 - val_mean_squared_logarithmic_error: 2.8932e-04\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0010.\n",
      "Epoch 5/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 1.8971e-04 - mean_squared_logarithmic_error: 1.8971e-04The average loss for epoch 4 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 988us/step - loss: 1.8858e-04 - mean_squared_logarithmic_error: 1.8858e-04 - val_loss: 1.9165e-04 - val_mean_squared_logarithmic_error: 1.9165e-04\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0010.\n",
      "Epoch 6/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 1.8102e-04 - mean_squared_logarithmic_error: 1.8102e-04The average loss for epoch 5 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 1.8077e-04 - mean_squared_logarithmic_error: 1.8077e-04 - val_loss: 1.8626e-04 - val_mean_squared_logarithmic_error: 1.8626e-04\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0010.\n",
      "Epoch 7/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 1.7588e-04 - mean_squared_logarithmic_error: 1.7588e-04The average loss for epoch 6 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 964us/step - loss: 1.7535e-04 - mean_squared_logarithmic_error: 1.7535e-04 - val_loss: 1.7337e-04 - val_mean_squared_logarithmic_error: 1.7337e-04\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0010.\n",
      "Epoch 8/800\n",
      "625/653 [===========================>..] - ETA: 0s - loss: 1.6647e-04 - mean_squared_logarithmic_error: 1.6647e-04The average loss for epoch 7 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 1.6636e-04 - mean_squared_logarithmic_error: 1.6636e-04 - val_loss: 1.6274e-04 - val_mean_squared_logarithmic_error: 1.6274e-04\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0010.\n",
      "Epoch 9/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 1.6048e-04 - mean_squared_logarithmic_error: 1.6048e-04The average loss for epoch 8 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 928us/step - loss: 1.6147e-04 - mean_squared_logarithmic_error: 1.6147e-04 - val_loss: 1.6406e-04 - val_mean_squared_logarithmic_error: 1.6406e-04\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0010.\n",
      "Epoch 10/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 1.5422e-04 - mean_squared_logarithmic_error: 1.5422e-04The average loss for epoch 9 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 989us/step - loss: 1.5403e-04 - mean_squared_logarithmic_error: 1.5403e-04 - val_loss: 2.3554e-04 - val_mean_squared_logarithmic_error: 2.3554e-04\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0010.\n",
      "Epoch 11/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 1.4762e-04 - mean_squared_logarithmic_error: 1.4762e-04The average loss for epoch 10 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 1.4773e-04 - mean_squared_logarithmic_error: 1.4773e-04 - val_loss: 1.4144e-04 - val_mean_squared_logarithmic_error: 1.4144e-04\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0010.\n",
      "Epoch 12/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 1.4820e-04 - mean_squared_logarithmic_error: 1.4820e-04The average loss for epoch 11 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 1.4814e-04 - mean_squared_logarithmic_error: 1.4814e-04 - val_loss: 1.4408e-04 - val_mean_squared_logarithmic_error: 1.4408e-04\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0010.\n",
      "Epoch 13/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 1.4146e-04 - mean_squared_logarithmic_error: 1.4146e-04The average loss for epoch 12 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 1.4171e-04 - mean_squared_logarithmic_error: 1.4171e-04 - val_loss: 1.7660e-04 - val_mean_squared_logarithmic_error: 1.7660e-04\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0010.\n",
      "Epoch 14/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 1.3492e-04 - mean_squared_logarithmic_error: 1.3492e-04The average loss for epoch 13 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 990us/step - loss: 1.3411e-04 - mean_squared_logarithmic_error: 1.3411e-04 - val_loss: 1.3239e-04 - val_mean_squared_logarithmic_error: 1.3239e-04\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0010.\n",
      "Epoch 15/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 1.3291e-04 - mean_squared_logarithmic_error: 1.3291e-04The average loss for epoch 14 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 1.3293e-04 - mean_squared_logarithmic_error: 1.3293e-04 - val_loss: 1.2930e-04 - val_mean_squared_logarithmic_error: 1.2930e-04\n",
      "\n",
      "Epoch 00015: Learning rate is 0.0010.\n",
      "Epoch 16/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 1.2958e-04 - mean_squared_logarithmic_error: 1.2958e-04The average loss for epoch 15 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 922us/step - loss: 1.3010e-04 - mean_squared_logarithmic_error: 1.3010e-04 - val_loss: 1.3613e-04 - val_mean_squared_logarithmic_error: 1.3613e-04\n",
      "\n",
      "Epoch 00016: Learning rate is 0.0010.\n",
      "Epoch 17/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 1.2725e-04 - mean_squared_logarithmic_error: 1.2725e-04The average loss for epoch 16 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 920us/step - loss: 1.2601e-04 - mean_squared_logarithmic_error: 1.2601e-04 - val_loss: 1.1712e-04 - val_mean_squared_logarithmic_error: 1.1712e-04\n",
      "\n",
      "Epoch 00017: Learning rate is 0.0010.\n",
      "Epoch 18/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 1.1730e-04 - mean_squared_logarithmic_error: 1.1730e-04The average loss for epoch 17 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 1.1774e-04 - mean_squared_logarithmic_error: 1.1774e-04 - val_loss: 1.2710e-04 - val_mean_squared_logarithmic_error: 1.2710e-04\n",
      "\n",
      "Epoch 00018: Learning rate is 0.0010.\n",
      "Epoch 19/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 1.1724e-04 - mean_squared_logarithmic_error: 1.1724e-04The average loss for epoch 18 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 1.1680e-04 - mean_squared_logarithmic_error: 1.1680e-04 - val_loss: 1.1730e-04 - val_mean_squared_logarithmic_error: 1.1730e-04\n",
      "\n",
      "Epoch 00019: Learning rate is 0.0010.\n",
      "Epoch 20/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 1.0850e-04 - mean_squared_logarithmic_error: 1.0850e-04The average loss for epoch 19 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 897us/step - loss: 1.1011e-04 - mean_squared_logarithmic_error: 1.1011e-04 - val_loss: 1.3152e-04 - val_mean_squared_logarithmic_error: 1.3152e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: Learning rate is 0.0010.\n",
      "Epoch 21/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 1.0987e-04 - mean_squared_logarithmic_error: 1.0987e-04The average loss for epoch 20 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 917us/step - loss: 1.1002e-04 - mean_squared_logarithmic_error: 1.1002e-04 - val_loss: 1.2236e-04 - val_mean_squared_logarithmic_error: 1.2236e-04\n",
      "\n",
      "Epoch 00021: Learning rate is 0.0010.\n",
      "Epoch 22/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 1.0630e-04 - mean_squared_logarithmic_error: 1.0630e-04The average loss for epoch 21 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 1.0625e-04 - mean_squared_logarithmic_error: 1.0625e-04 - val_loss: 1.1182e-04 - val_mean_squared_logarithmic_error: 1.1182e-04\n",
      "\n",
      "Epoch 00022: Learning rate is 0.0010.\n",
      "Epoch 23/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 1.0195e-04 - mean_squared_logarithmic_error: 1.0195e-04The average loss for epoch 22 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 1.0177e-04 - mean_squared_logarithmic_error: 1.0177e-04 - val_loss: 1.0691e-04 - val_mean_squared_logarithmic_error: 1.0691e-04\n",
      "\n",
      "Epoch 00023: Learning rate is 0.0010.\n",
      "Epoch 24/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 9.7095e-05 - mean_squared_logarithmic_error: 9.7095e-05The average loss for epoch 23 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 9.7563e-05 - mean_squared_logarithmic_error: 9.7563e-05 - val_loss: 1.1482e-04 - val_mean_squared_logarithmic_error: 1.1482e-04\n",
      "\n",
      "Epoch 00024: Learning rate is 0.0010.\n",
      "Epoch 25/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 9.4557e-05 - mean_squared_logarithmic_error: 9.4557e-05The average loss for epoch 24 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 9.4436e-05 - mean_squared_logarithmic_error: 9.4436e-05 - val_loss: 1.1006e-04 - val_mean_squared_logarithmic_error: 1.1006e-04\n",
      "\n",
      "Epoch 00025: Learning rate is 0.0010.\n",
      "Epoch 26/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 9.0608e-05 - mean_squared_logarithmic_error: 9.0608e-05The average loss for epoch 25 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 9.0878e-05 - mean_squared_logarithmic_error: 9.0878e-05 - val_loss: 1.1212e-04 - val_mean_squared_logarithmic_error: 1.1212e-04\n",
      "\n",
      "Epoch 00026: Learning rate is 0.0010.\n",
      "Epoch 27/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 9.0952e-05 - mean_squared_logarithmic_error: 9.0952e-05The average loss for epoch 26 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 9.0491e-05 - mean_squared_logarithmic_error: 9.0491e-05 - val_loss: 1.0860e-04 - val_mean_squared_logarithmic_error: 1.0860e-04\n",
      "\n",
      "Epoch 00027: Learning rate is 0.0010.\n",
      "Epoch 28/800\n",
      "623/653 [===========================>..] - ETA: 0s - loss: 8.5138e-05 - mean_squared_logarithmic_error: 8.5138e-05The average loss for epoch 27 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 915us/step - loss: 8.5559e-05 - mean_squared_logarithmic_error: 8.5559e-05 - val_loss: 1.3566e-04 - val_mean_squared_logarithmic_error: 1.3566e-04\n",
      "\n",
      "Epoch 00028: Learning rate is 0.0010.\n",
      "Epoch 29/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 8.3880e-05 - mean_squared_logarithmic_error: 8.3880e-05The average loss for epoch 28 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 884us/step - loss: 8.3933e-05 - mean_squared_logarithmic_error: 8.3933e-05 - val_loss: 1.2421e-04 - val_mean_squared_logarithmic_error: 1.2421e-04\n",
      "\n",
      "Epoch 00029: Learning rate is 0.0010.\n",
      "Epoch 30/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 8.3253e-05 - mean_squared_logarithmic_error: 8.3253e-05The average loss for epoch 29 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 8.3150e-05 - mean_squared_logarithmic_error: 8.3150e-05 - val_loss: 1.3029e-04 - val_mean_squared_logarithmic_error: 1.3029e-04\n",
      "\n",
      "Epoch 00030: Learning rate is 0.0010.\n",
      "Epoch 31/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 7.9358e-05 - mean_squared_logarithmic_error: 7.9358e-05The average loss for epoch 30 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 7.9420e-05 - mean_squared_logarithmic_error: 7.9420e-05 - val_loss: 1.1149e-04 - val_mean_squared_logarithmic_error: 1.1149e-04\n",
      "\n",
      "Epoch 00031: Learning rate is 0.0010.\n",
      "Epoch 32/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 7.9678e-05 - mean_squared_logarithmic_error: 7.9678e-05The average loss for epoch 31 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 7.9962e-05 - mean_squared_logarithmic_error: 7.9962e-05 - val_loss: 1.2050e-04 - val_mean_squared_logarithmic_error: 1.2050e-04\n",
      "\n",
      "Epoch 00032: Learning rate is 0.0010.\n",
      "Epoch 33/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 7.5683e-05 - mean_squared_logarithmic_error: 7.5683e-05The average loss for epoch 32 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 897us/step - loss: 7.5922e-05 - mean_squared_logarithmic_error: 7.5922e-05 - val_loss: 1.0717e-04 - val_mean_squared_logarithmic_error: 1.0717e-04\n",
      "\n",
      "Epoch 00033: Learning rate is 0.0010.\n",
      "Epoch 34/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 7.5818e-05 - mean_squared_logarithmic_error: 7.5818e-05The average loss for epoch 33 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 7.5998e-05 - mean_squared_logarithmic_error: 7.5998e-05 - val_loss: 1.1521e-04 - val_mean_squared_logarithmic_error: 1.1521e-04\n",
      "\n",
      "Epoch 00034: Learning rate is 0.0010.\n",
      "Epoch 35/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 7.3446e-05 - mean_squared_logarithmic_error: 7.3446e-05The average loss for epoch 34 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 7.3247e-05 - mean_squared_logarithmic_error: 7.3247e-05 - val_loss: 1.0634e-04 - val_mean_squared_logarithmic_error: 1.0634e-04\n",
      "\n",
      "Epoch 00035: Learning rate is 0.0010.\n",
      "Epoch 36/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 7.2687e-05 - mean_squared_logarithmic_error: 7.2687e-05The average loss for epoch 35 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 7.3066e-05 - mean_squared_logarithmic_error: 7.3066e-05 - val_loss: 1.4312e-04 - val_mean_squared_logarithmic_error: 1.4312e-04\n",
      "\n",
      "Epoch 00036: Learning rate is 0.0010.\n",
      "Epoch 37/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 7.1942e-05 - mean_squared_logarithmic_error: 7.1942e-05The average loss for epoch 36 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 910us/step - loss: 7.1645e-05 - mean_squared_logarithmic_error: 7.1645e-05 - val_loss: 1.0940e-04 - val_mean_squared_logarithmic_error: 1.0940e-04\n",
      "\n",
      "Epoch 00037: Learning rate is 0.0010.\n",
      "Epoch 38/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 6.8897e-05 - mean_squared_logarithmic_error: 6.8897e-05The average loss for epoch 37 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 6.9142e-05 - mean_squared_logarithmic_error: 6.9142e-05 - val_loss: 1.2421e-04 - val_mean_squared_logarithmic_error: 1.2421e-04\n",
      "\n",
      "Epoch 00038: Learning rate is 0.0010.\n",
      "Epoch 39/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 6.7222e-05 - mean_squared_logarithmic_error: 6.7222e-05The average loss for epoch 38 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 920us/step - loss: 6.7275e-05 - mean_squared_logarithmic_error: 6.7275e-05 - val_loss: 1.3240e-04 - val_mean_squared_logarithmic_error: 1.3240e-04\n",
      "\n",
      "Epoch 00039: Learning rate is 0.0010.\n",
      "Epoch 40/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 6.5800e-05 - mean_squared_logarithmic_error: 6.5800e-05The average loss for epoch 39 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 907us/step - loss: 6.6291e-05 - mean_squared_logarithmic_error: 6.6291e-05 - val_loss: 1.2076e-04 - val_mean_squared_logarithmic_error: 1.2076e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: Learning rate is 0.0010.\n",
      "Epoch 41/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 6.6527e-05 - mean_squared_logarithmic_error: 6.6527e-05The average loss for epoch 40 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 6.6746e-05 - mean_squared_logarithmic_error: 6.6746e-05 - val_loss: 1.2344e-04 - val_mean_squared_logarithmic_error: 1.2344e-04\n",
      "\n",
      "Epoch 00041: Learning rate is 0.0010.\n",
      "Epoch 42/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 6.5200e-05 - mean_squared_logarithmic_error: 6.5200e-05The average loss for epoch 41 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 868us/step - loss: 6.5279e-05 - mean_squared_logarithmic_error: 6.5279e-05 - val_loss: 1.2922e-04 - val_mean_squared_logarithmic_error: 1.2922e-04\n",
      "\n",
      "Epoch 00042: Learning rate is 0.0010.\n",
      "Epoch 43/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 6.5282e-05 - mean_squared_logarithmic_error: 6.5282e-05The average loss for epoch 42 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 6.5318e-05 - mean_squared_logarithmic_error: 6.5318e-05 - val_loss: 1.1291e-04 - val_mean_squared_logarithmic_error: 1.1291e-04\n",
      "\n",
      "Epoch 00043: Learning rate is 0.0010.\n",
      "Epoch 44/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 6.4537e-05 - mean_squared_logarithmic_error: 6.4537e-05The average loss for epoch 43 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 893us/step - loss: 6.4104e-05 - mean_squared_logarithmic_error: 6.4104e-05 - val_loss: 1.1401e-04 - val_mean_squared_logarithmic_error: 1.1401e-04\n",
      "\n",
      "Epoch 00044: Learning rate is 0.0010.\n",
      "Epoch 45/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 6.2751e-05 - mean_squared_logarithmic_error: 6.2751e-05The average loss for epoch 44 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 6.2753e-05 - mean_squared_logarithmic_error: 6.2753e-05 - val_loss: 1.3235e-04 - val_mean_squared_logarithmic_error: 1.3235e-04\n",
      "\n",
      "Epoch 00045: Learning rate is 0.0010.\n",
      "Epoch 46/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 6.0778e-05 - mean_squared_logarithmic_error: 6.0778e-05The average loss for epoch 45 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 868us/step - loss: 6.0718e-05 - mean_squared_logarithmic_error: 6.0718e-05 - val_loss: 1.0787e-04 - val_mean_squared_logarithmic_error: 1.0787e-04\n",
      "\n",
      "Epoch 00046: Learning rate is 0.0010.\n",
      "Epoch 47/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 6.1709e-05 - mean_squared_logarithmic_error: 6.1709e-05The average loss for epoch 46 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 867us/step - loss: 6.1817e-05 - mean_squared_logarithmic_error: 6.1817e-05 - val_loss: 1.2011e-04 - val_mean_squared_logarithmic_error: 1.2011e-04\n",
      "\n",
      "Epoch 00047: Learning rate is 0.0010.\n",
      "Epoch 48/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 6.0510e-05 - mean_squared_logarithmic_error: 6.0510e-05The average loss for epoch 47 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 859us/step - loss: 6.0531e-05 - mean_squared_logarithmic_error: 6.0531e-05 - val_loss: 1.7261e-04 - val_mean_squared_logarithmic_error: 1.7261e-04\n",
      "\n",
      "Epoch 00048: Learning rate is 0.0010.\n",
      "Epoch 49/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 6.0249e-05 - mean_squared_logarithmic_error: 6.0249e-05The average loss for epoch 48 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 867us/step - loss: 6.0186e-05 - mean_squared_logarithmic_error: 6.0186e-05 - val_loss: 1.1772e-04 - val_mean_squared_logarithmic_error: 1.1772e-04\n",
      "\n",
      "Epoch 00049: Learning rate is 0.0010.\n",
      "Epoch 50/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 5.8779e-05 - mean_squared_logarithmic_error: 5.8779e-05The average loss for epoch 49 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 863us/step - loss: 5.8733e-05 - mean_squared_logarithmic_error: 5.8733e-05 - val_loss: 1.1434e-04 - val_mean_squared_logarithmic_error: 1.1434e-04\n",
      "\n",
      "Epoch 00050: Learning rate is 0.0010.\n",
      "Epoch 51/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 5.9417e-05 - mean_squared_logarithmic_error: 5.9417e-05The average loss for epoch 50 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 864us/step - loss: 5.9417e-05 - mean_squared_logarithmic_error: 5.9417e-05 - val_loss: 1.4257e-04 - val_mean_squared_logarithmic_error: 1.4257e-04\n",
      "\n",
      "Epoch 00051: Learning rate is 0.0010.\n",
      "Epoch 52/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 5.7869e-05 - mean_squared_logarithmic_error: 5.7869e-05The average loss for epoch 51 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 865us/step - loss: 5.7897e-05 - mean_squared_logarithmic_error: 5.7897e-05 - val_loss: 1.0985e-04 - val_mean_squared_logarithmic_error: 1.0985e-04\n",
      "\n",
      "Epoch 00052: Learning rate is 0.0010.\n",
      "Epoch 53/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 5.5874e-05 - mean_squared_logarithmic_error: 5.5874e-05The average loss for epoch 52 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 5.5869e-05 - mean_squared_logarithmic_error: 5.5869e-05 - val_loss: 1.1526e-04 - val_mean_squared_logarithmic_error: 1.1526e-04\n",
      "\n",
      "Epoch 00053: Learning rate is 0.0010.\n",
      "Epoch 54/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 5.6848e-05 - mean_squared_logarithmic_error: 5.6848e-05The average loss for epoch 53 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 5.6932e-05 - mean_squared_logarithmic_error: 5.6932e-05 - val_loss: 1.1567e-04 - val_mean_squared_logarithmic_error: 1.1567e-04\n",
      "\n",
      "Epoch 00054: Learning rate is 0.0010.\n",
      "Epoch 55/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 5.6139e-05 - mean_squared_logarithmic_error: 5.6139e-05The average loss for epoch 54 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 5.6166e-05 - mean_squared_logarithmic_error: 5.6166e-05 - val_loss: 1.2255e-04 - val_mean_squared_logarithmic_error: 1.2255e-04\n",
      "\n",
      "Epoch 00055: Learning rate is 0.0010.\n",
      "Epoch 56/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 5.5670e-05 - mean_squared_logarithmic_error: 5.5670e-05The average loss for epoch 55 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 5.5750e-05 - mean_squared_logarithmic_error: 5.5750e-05 - val_loss: 1.1805e-04 - val_mean_squared_logarithmic_error: 1.1805e-04\n",
      "\n",
      "Epoch 00056: Learning rate is 0.0010.\n",
      "Epoch 57/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 5.4282e-05 - mean_squared_logarithmic_error: 5.4282e-05The average loss for epoch 56 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 962us/step - loss: 5.4294e-05 - mean_squared_logarithmic_error: 5.4294e-05 - val_loss: 1.1370e-04 - val_mean_squared_logarithmic_error: 1.1370e-04\n",
      "\n",
      "Epoch 00057: Learning rate is 0.0010.\n",
      "Epoch 58/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 5.4000e-05 - mean_squared_logarithmic_error: 5.4000e-05The average loss for epoch 57 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 866us/step - loss: 5.4120e-05 - mean_squared_logarithmic_error: 5.4120e-05 - val_loss: 1.1501e-04 - val_mean_squared_logarithmic_error: 1.1501e-04\n",
      "\n",
      "Epoch 00058: Learning rate is 0.0010.\n",
      "Epoch 59/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 5.2913e-05 - mean_squared_logarithmic_error: 5.2913e-05The average loss for epoch 58 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 920us/step - loss: 5.3692e-05 - mean_squared_logarithmic_error: 5.3692e-05 - val_loss: 1.2902e-04 - val_mean_squared_logarithmic_error: 1.2902e-04\n",
      "\n",
      "Epoch 00059: Learning rate is 0.0010.\n",
      "Epoch 60/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 5.2265e-05 - mean_squared_logarithmic_error: 5.2265e-05The average loss for epoch 59 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 5.2321e-05 - mean_squared_logarithmic_error: 5.2321e-05 - val_loss: 1.1627e-04 - val_mean_squared_logarithmic_error: 1.1627e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: Learning rate is 0.0010.\n",
      "Epoch 61/800\n",
      "588/653 [==========================>...] - ETA: 0s - loss: 5.2667e-05 - mean_squared_logarithmic_error: 5.2667e-05The average loss for epoch 60 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 863us/step - loss: 5.2736e-05 - mean_squared_logarithmic_error: 5.2736e-05 - val_loss: 1.2273e-04 - val_mean_squared_logarithmic_error: 1.2273e-04\n",
      "\n",
      "Epoch 00061: Learning rate is 0.0010.\n",
      "Epoch 62/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 5.2321e-05 - mean_squared_logarithmic_error: 5.2321e-05The average loss for epoch 61 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 5.2448e-05 - mean_squared_logarithmic_error: 5.2448e-05 - val_loss: 1.1595e-04 - val_mean_squared_logarithmic_error: 1.1595e-04\n",
      "\n",
      "Epoch 00062: Learning rate is 0.0010.\n",
      "Epoch 63/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 5.2357e-05 - mean_squared_logarithmic_error: 5.2357e-05The average loss for epoch 62 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 5.2272e-05 - mean_squared_logarithmic_error: 5.2272e-05 - val_loss: 1.3291e-04 - val_mean_squared_logarithmic_error: 1.3291e-04\n",
      "\n",
      "Epoch 00063: Learning rate is 0.0010.\n",
      "Epoch 64/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 5.0828e-05 - mean_squared_logarithmic_error: 5.0828e-05The average loss for epoch 63 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 896us/step - loss: 5.0853e-05 - mean_squared_logarithmic_error: 5.0853e-05 - val_loss: 1.3324e-04 - val_mean_squared_logarithmic_error: 1.3324e-04\n",
      "\n",
      "Epoch 00064: Learning rate is 0.0010.\n",
      "Epoch 65/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 5.1252e-05 - mean_squared_logarithmic_error: 5.1252e-05The average loss for epoch 64 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 5.1104e-05 - mean_squared_logarithmic_error: 5.1104e-05 - val_loss: 1.1823e-04 - val_mean_squared_logarithmic_error: 1.1823e-04\n",
      "\n",
      "Epoch 00065: Learning rate is 0.0010.\n",
      "Epoch 66/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 5.0816e-05 - mean_squared_logarithmic_error: 5.0816e-05The average loss for epoch 65 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 5.0791e-05 - mean_squared_logarithmic_error: 5.0791e-05 - val_loss: 1.3093e-04 - val_mean_squared_logarithmic_error: 1.3093e-04\n",
      "\n",
      "Epoch 00066: Learning rate is 0.0010.\n",
      "Epoch 67/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 4.9505e-05 - mean_squared_logarithmic_error: 4.9505e-05The average loss for epoch 66 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 865us/step - loss: 4.9607e-05 - mean_squared_logarithmic_error: 4.9607e-05 - val_loss: 1.2459e-04 - val_mean_squared_logarithmic_error: 1.2459e-04\n",
      "\n",
      "Epoch 00067: Learning rate is 0.0010.\n",
      "Epoch 68/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 4.9601e-05 - mean_squared_logarithmic_error: 4.9601e-05The average loss for epoch 67 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 870us/step - loss: 4.9764e-05 - mean_squared_logarithmic_error: 4.9764e-05 - val_loss: 1.2216e-04 - val_mean_squared_logarithmic_error: 1.2216e-04\n",
      "\n",
      "Epoch 00068: Learning rate is 0.0010.\n",
      "Epoch 69/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 5.0290e-05 - mean_squared_logarithmic_error: 5.0290e-05The average loss for epoch 68 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 889us/step - loss: 5.0323e-05 - mean_squared_logarithmic_error: 5.0323e-05 - val_loss: 1.2602e-04 - val_mean_squared_logarithmic_error: 1.2602e-04\n",
      "\n",
      "Epoch 00069: Learning rate is 0.0010.\n",
      "Epoch 70/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 4.7986e-05 - mean_squared_logarithmic_error: 4.7986e-05The average loss for epoch 69 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 4.8165e-05 - mean_squared_logarithmic_error: 4.8165e-05 - val_loss: 1.1826e-04 - val_mean_squared_logarithmic_error: 1.1826e-04\n",
      "\n",
      "Epoch 00070: Learning rate is 0.0010.\n",
      "Epoch 71/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 4.9457e-05 - mean_squared_logarithmic_error: 4.9457e-05The average loss for epoch 70 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 4.9462e-05 - mean_squared_logarithmic_error: 4.9462e-05 - val_loss: 1.1572e-04 - val_mean_squared_logarithmic_error: 1.1572e-04\n",
      "\n",
      "Epoch 00071: Learning rate is 0.0010.\n",
      "Epoch 72/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.9729e-05 - mean_squared_logarithmic_error: 4.9729e-05The average loss for epoch 71 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 5.0024e-05 - mean_squared_logarithmic_error: 5.0024e-05 - val_loss: 1.3271e-04 - val_mean_squared_logarithmic_error: 1.3271e-04\n",
      "\n",
      "Epoch 00072: Learning rate is 0.0010.\n",
      "Epoch 73/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 4.8372e-05 - mean_squared_logarithmic_error: 4.8372e-05The average loss for epoch 72 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 4.8553e-05 - mean_squared_logarithmic_error: 4.8553e-05 - val_loss: 1.3156e-04 - val_mean_squared_logarithmic_error: 1.3156e-04\n",
      "\n",
      "Epoch 00073: Learning rate is 0.0010.\n",
      "Epoch 74/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 4.8817e-05 - mean_squared_logarithmic_error: 4.8817e-05The average loss for epoch 73 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 914us/step - loss: 4.9090e-05 - mean_squared_logarithmic_error: 4.9090e-05 - val_loss: 1.2221e-04 - val_mean_squared_logarithmic_error: 1.2221e-04\n",
      "\n",
      "Epoch 00074: Learning rate is 0.0010.\n",
      "Epoch 75/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.7770e-05 - mean_squared_logarithmic_error: 4.7770e-05The average loss for epoch 74 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 4.8106e-05 - mean_squared_logarithmic_error: 4.8106e-05 - val_loss: 1.2721e-04 - val_mean_squared_logarithmic_error: 1.2721e-04\n",
      "\n",
      "Epoch 00075: Learning rate is 0.0010.\n",
      "Epoch 76/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 4.8300e-05 - mean_squared_logarithmic_error: 4.8300e-05The average loss for epoch 75 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 4.8783e-05 - mean_squared_logarithmic_error: 4.8783e-05 - val_loss: 1.2236e-04 - val_mean_squared_logarithmic_error: 1.2236e-04\n",
      "\n",
      "Epoch 00076: Learning rate is 0.0010.\n",
      "Epoch 77/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 4.7638e-05 - mean_squared_logarithmic_error: 4.7638e-05The average loss for epoch 76 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.7732e-05 - mean_squared_logarithmic_error: 4.7732e-05 - val_loss: 1.1784e-04 - val_mean_squared_logarithmic_error: 1.1784e-04\n",
      "\n",
      "Epoch 00077: Learning rate is 0.0010.\n",
      "Epoch 78/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 4.6812e-05 - mean_squared_logarithmic_error: 4.6812e-05The average loss for epoch 77 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 4.7045e-05 - mean_squared_logarithmic_error: 4.7045e-05 - val_loss: 1.1782e-04 - val_mean_squared_logarithmic_error: 1.1782e-04\n",
      "\n",
      "Epoch 00078: Learning rate is 0.0010.\n",
      "Epoch 79/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 4.5028e-05 - mean_squared_logarithmic_error: 4.5028e-05The average loss for epoch 78 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 919us/step - loss: 4.5050e-05 - mean_squared_logarithmic_error: 4.5050e-05 - val_loss: 1.2333e-04 - val_mean_squared_logarithmic_error: 1.2333e-04\n",
      "\n",
      "Epoch 00079: Learning rate is 0.0010.\n",
      "Epoch 80/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 4.6925e-05 - mean_squared_logarithmic_error: 4.6925e-05The average loss for epoch 79 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.6918e-05 - mean_squared_logarithmic_error: 4.6918e-05 - val_loss: 1.1974e-04 - val_mean_squared_logarithmic_error: 1.1974e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00080: Learning rate is 0.0010.\n",
      "Epoch 81/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.6291e-05 - mean_squared_logarithmic_error: 4.6291e-05The average loss for epoch 80 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 4.6510e-05 - mean_squared_logarithmic_error: 4.6510e-05 - val_loss: 1.3384e-04 - val_mean_squared_logarithmic_error: 1.3384e-04\n",
      "\n",
      "Epoch 00081: Learning rate is 0.0010.\n",
      "Epoch 82/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 4.8440e-05 - mean_squared_logarithmic_error: 4.8440e-05The average loss for epoch 81 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 4.8653e-05 - mean_squared_logarithmic_error: 4.8653e-05 - val_loss: 1.1907e-04 - val_mean_squared_logarithmic_error: 1.1907e-04\n",
      "\n",
      "Epoch 00082: Learning rate is 0.0010.\n",
      "Epoch 83/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 4.6703e-05 - mean_squared_logarithmic_error: 4.6703e-05The average loss for epoch 82 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 4.7123e-05 - mean_squared_logarithmic_error: 4.7123e-05 - val_loss: 1.1868e-04 - val_mean_squared_logarithmic_error: 1.1868e-04\n",
      "\n",
      "Epoch 00083: Learning rate is 0.0010.\n",
      "Epoch 84/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 4.6360e-05 - mean_squared_logarithmic_error: 4.6360e-05The average loss for epoch 83 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 870us/step - loss: 4.6452e-05 - mean_squared_logarithmic_error: 4.6452e-05 - val_loss: 1.1856e-04 - val_mean_squared_logarithmic_error: 1.1856e-04\n",
      "\n",
      "Epoch 00084: Learning rate is 0.0010.\n",
      "Epoch 85/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 4.5147e-05 - mean_squared_logarithmic_error: 4.5147e-05The average loss for epoch 84 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 4.5267e-05 - mean_squared_logarithmic_error: 4.5267e-05 - val_loss: 1.1778e-04 - val_mean_squared_logarithmic_error: 1.1778e-04\n",
      "\n",
      "Epoch 00085: Learning rate is 0.0010.\n",
      "Epoch 86/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 4.6991e-05 - mean_squared_logarithmic_error: 4.6991e-05The average loss for epoch 85 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 4.6898e-05 - mean_squared_logarithmic_error: 4.6898e-05 - val_loss: 1.2170e-04 - val_mean_squared_logarithmic_error: 1.2170e-04\n",
      "\n",
      "Epoch 00086: Learning rate is 0.0010.\n",
      "Epoch 87/800\n",
      "625/653 [===========================>..] - ETA: 0s - loss: 4.5603e-05 - mean_squared_logarithmic_error: 4.5603e-05The average loss for epoch 86 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 896us/step - loss: 4.5437e-05 - mean_squared_logarithmic_error: 4.5437e-05 - val_loss: 1.1370e-04 - val_mean_squared_logarithmic_error: 1.1370e-04\n",
      "\n",
      "Epoch 00087: Learning rate is 0.0010.\n",
      "Epoch 88/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.6146e-05 - mean_squared_logarithmic_error: 4.6146e-05 ETA: 0s - loss: 4.6011e-05 - mean_squared_logarithmic_error: 4.60The average loss for epoch 87 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.6082e-05 - mean_squared_logarithmic_error: 4.6082e-05 - val_loss: 1.1422e-04 - val_mean_squared_logarithmic_error: 1.1422e-04\n",
      "\n",
      "Epoch 00088: Learning rate is 0.0010.\n",
      "Epoch 89/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 4.5172e-05 - mean_squared_logarithmic_error: 4.5172e-05The average loss for epoch 88 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 4.5206e-05 - mean_squared_logarithmic_error: 4.5206e-05 - val_loss: 1.2151e-04 - val_mean_squared_logarithmic_error: 1.2151e-04\n",
      "\n",
      "Epoch 00089: Learning rate is 0.0010.\n",
      "Epoch 90/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 4.6824e-05 - mean_squared_logarithmic_error: 4.6824e-05The average loss for epoch 89 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 4.6669e-05 - mean_squared_logarithmic_error: 4.6669e-05 - val_loss: 1.1379e-04 - val_mean_squared_logarithmic_error: 1.1379e-04\n",
      "\n",
      "Epoch 00090: Learning rate is 0.0010.\n",
      "Epoch 91/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 4.4302e-05 - mean_squared_logarithmic_error: 4.4302e-05The average loss for epoch 90 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 909us/step - loss: 4.4487e-05 - mean_squared_logarithmic_error: 4.4487e-05 - val_loss: 1.1972e-04 - val_mean_squared_logarithmic_error: 1.1972e-04\n",
      "\n",
      "Epoch 00091: Learning rate is 0.0010.\n",
      "Epoch 92/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 4.6040e-05 - mean_squared_logarithmic_error: 4.6040e-05The average loss for epoch 91 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 4.5882e-05 - mean_squared_logarithmic_error: 4.5882e-05 - val_loss: 1.1666e-04 - val_mean_squared_logarithmic_error: 1.1666e-04\n",
      "\n",
      "Epoch 00092: Learning rate is 0.0010.\n",
      "Epoch 93/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.4756e-05 - mean_squared_logarithmic_error: 4.4756e-05The average loss for epoch 92 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.4728e-05 - mean_squared_logarithmic_error: 4.4728e-05 - val_loss: 1.2083e-04 - val_mean_squared_logarithmic_error: 1.2083e-04\n",
      "\n",
      "Epoch 00093: Learning rate is 0.0010.\n",
      "Epoch 94/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 4.4303e-05 - mean_squared_logarithmic_error: 4.4303e-05The average loss for epoch 93 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 919us/step - loss: 4.4439e-05 - mean_squared_logarithmic_error: 4.4439e-05 - val_loss: 1.1929e-04 - val_mean_squared_logarithmic_error: 1.1929e-04\n",
      "\n",
      "Epoch 00094: Learning rate is 0.0010.\n",
      "Epoch 95/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.4462e-05 - mean_squared_logarithmic_error: 4.4462e-05The average loss for epoch 94 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.4382e-05 - mean_squared_logarithmic_error: 4.4382e-05 - val_loss: 1.1558e-04 - val_mean_squared_logarithmic_error: 1.1558e-04\n",
      "\n",
      "Epoch 00095: Learning rate is 0.0010.\n",
      "Epoch 96/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 4.5448e-05 - mean_squared_logarithmic_error: 4.5448e-05The average loss for epoch 95 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 914us/step - loss: 4.5332e-05 - mean_squared_logarithmic_error: 4.5332e-05 - val_loss: 1.1927e-04 - val_mean_squared_logarithmic_error: 1.1927e-04\n",
      "\n",
      "Epoch 00096: Learning rate is 0.0010.\n",
      "Epoch 97/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 4.4276e-05 - mean_squared_logarithmic_error: 4.4276e-05The average loss for epoch 96 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 4.4483e-05 - mean_squared_logarithmic_error: 4.4483e-05 - val_loss: 1.2063e-04 - val_mean_squared_logarithmic_error: 1.2063e-04\n",
      "\n",
      "Epoch 00097: Learning rate is 0.0010.\n",
      "Epoch 98/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 4.3898e-05 - mean_squared_logarithmic_error: 4.3898e-05The average loss for epoch 97 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 4.4064e-05 - mean_squared_logarithmic_error: 4.4064e-05 - val_loss: 1.1956e-04 - val_mean_squared_logarithmic_error: 1.1956e-04\n",
      "\n",
      "Epoch 00098: Learning rate is 0.0010.\n",
      "Epoch 99/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.5163e-05 - mean_squared_logarithmic_error: 4.5163e-05The average loss for epoch 98 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 4.5122e-05 - mean_squared_logarithmic_error: 4.5122e-05 - val_loss: 1.2528e-04 - val_mean_squared_logarithmic_error: 1.2528e-04\n",
      "\n",
      "Epoch 00099: Learning rate is 0.0010.\n",
      "Epoch 100/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/653 [============================>.] - ETA: 0s - loss: 4.3639e-05 - mean_squared_logarithmic_error: 4.3639e-05The average loss for epoch 99 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 4.3629e-05 - mean_squared_logarithmic_error: 4.3629e-05 - val_loss: 1.1414e-04 - val_mean_squared_logarithmic_error: 1.1414e-04\n",
      "\n",
      "Epoch 00100: Learning rate is 0.0010.\n",
      "Epoch 101/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 4.3678e-05 - mean_squared_logarithmic_error: 4.3678e-05The average loss for epoch 100 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 4.3671e-05 - mean_squared_logarithmic_error: 4.3671e-05 - val_loss: 1.2031e-04 - val_mean_squared_logarithmic_error: 1.2031e-04\n",
      "\n",
      "Epoch 00101: Learning rate is 0.0010.\n",
      "Epoch 102/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 4.3592e-05 - mean_squared_logarithmic_error: 4.3592e-05The average loss for epoch 101 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 4.3630e-05 - mean_squared_logarithmic_error: 4.3630e-05 - val_loss: 1.2459e-04 - val_mean_squared_logarithmic_error: 1.2459e-04\n",
      "\n",
      "Epoch 00102: Learning rate is 0.0010.\n",
      "Epoch 103/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 4.4158e-05 - mean_squared_logarithmic_error: 4.4158e-05The average loss for epoch 102 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 860us/step - loss: 4.4158e-05 - mean_squared_logarithmic_error: 4.4158e-05 - val_loss: 1.2162e-04 - val_mean_squared_logarithmic_error: 1.2162e-04\n",
      "\n",
      "Epoch 00103: Learning rate is 0.0010.\n",
      "Epoch 104/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 4.3198e-05 - mean_squared_logarithmic_error: 4.3198e-05The average loss for epoch 103 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 4.3164e-05 - mean_squared_logarithmic_error: 4.3164e-05 - val_loss: 1.1861e-04 - val_mean_squared_logarithmic_error: 1.1861e-04\n",
      "\n",
      "Epoch 00104: Learning rate is 0.0010.\n",
      "Epoch 105/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 4.2629e-05 - mean_squared_logarithmic_error: 4.2629e-05The average loss for epoch 104 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 903us/step - loss: 4.2473e-05 - mean_squared_logarithmic_error: 4.2473e-05 - val_loss: 1.2289e-04 - val_mean_squared_logarithmic_error: 1.2289e-04\n",
      "\n",
      "Epoch 00105: Learning rate is 0.0010.\n",
      "Epoch 106/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 4.4042e-05 - mean_squared_logarithmic_error: 4.4042e-05The average loss for epoch 105 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 4.4025e-05 - mean_squared_logarithmic_error: 4.4025e-05 - val_loss: 1.1229e-04 - val_mean_squared_logarithmic_error: 1.1229e-04\n",
      "\n",
      "Epoch 00106: Learning rate is 0.0010.\n",
      "Epoch 107/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 4.2302e-05 - mean_squared_logarithmic_error: 4.2302e-05The average loss for epoch 106 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.2495e-05 - mean_squared_logarithmic_error: 4.2495e-05 - val_loss: 1.2539e-04 - val_mean_squared_logarithmic_error: 1.2539e-04\n",
      "\n",
      "Epoch 00107: Learning rate is 0.0010.\n",
      "Epoch 108/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.2458e-05 - mean_squared_logarithmic_error: 4.2458e-05The average loss for epoch 107 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 4.2418e-05 - mean_squared_logarithmic_error: 4.2418e-05 - val_loss: 1.1679e-04 - val_mean_squared_logarithmic_error: 1.1679e-04\n",
      "\n",
      "Epoch 00108: Learning rate is 0.0010.\n",
      "Epoch 109/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 4.3438e-05 - mean_squared_logarithmic_error: 4.3438e-05The average loss for epoch 108 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 4.3350e-05 - mean_squared_logarithmic_error: 4.3350e-05 - val_loss: 1.2632e-04 - val_mean_squared_logarithmic_error: 1.2632e-04\n",
      "\n",
      "Epoch 00109: Learning rate is 0.0010.\n",
      "Epoch 110/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 4.2886e-05 - mean_squared_logarithmic_error: 4.2886e-05The average loss for epoch 109 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.2965e-05 - mean_squared_logarithmic_error: 4.2965e-05 - val_loss: 1.2581e-04 - val_mean_squared_logarithmic_error: 1.2581e-04\n",
      "\n",
      "Epoch 00110: Learning rate is 0.0010.\n",
      "Epoch 111/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 4.2504e-05 - mean_squared_logarithmic_error: 4.2504e-05The average loss for epoch 110 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 4.2500e-05 - mean_squared_logarithmic_error: 4.2500e-05 - val_loss: 1.1685e-04 - val_mean_squared_logarithmic_error: 1.1685e-04\n",
      "\n",
      "Epoch 00111: Learning rate is 0.0010.\n",
      "Epoch 112/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 4.1492e-05 - mean_squared_logarithmic_error: 4.1492e-05The average loss for epoch 111 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 4.1363e-05 - mean_squared_logarithmic_error: 4.1363e-05 - val_loss: 1.1939e-04 - val_mean_squared_logarithmic_error: 1.1939e-04\n",
      "\n",
      "Epoch 00112: Learning rate is 0.0010.\n",
      "Epoch 113/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 4.2247e-05 - mean_squared_logarithmic_error: 4.2247e-05The average loss for epoch 112 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.2288e-05 - mean_squared_logarithmic_error: 4.2288e-05 - val_loss: 1.1920e-04 - val_mean_squared_logarithmic_error: 1.1920e-04\n",
      "\n",
      "Epoch 00113: Learning rate is 0.0010.\n",
      "Epoch 114/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.2064e-05 - mean_squared_logarithmic_error: 4.2064e-05The average loss for epoch 113 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.2102e-05 - mean_squared_logarithmic_error: 4.2102e-05 - val_loss: 1.2643e-04 - val_mean_squared_logarithmic_error: 1.2643e-04\n",
      "\n",
      "Epoch 00114: Learning rate is 0.0010.\n",
      "Epoch 115/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 4.2025e-05 - mean_squared_logarithmic_error: 4.2025e-05The average loss for epoch 114 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 4.2134e-05 - mean_squared_logarithmic_error: 4.2134e-05 - val_loss: 1.2164e-04 - val_mean_squared_logarithmic_error: 1.2164e-04\n",
      "\n",
      "Epoch 00115: Learning rate is 0.0010.\n",
      "Epoch 116/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.1424e-05 - mean_squared_logarithmic_error: 4.1424e-05The average loss for epoch 115 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 4.1455e-05 - mean_squared_logarithmic_error: 4.1455e-05 - val_loss: 1.1714e-04 - val_mean_squared_logarithmic_error: 1.1714e-04\n",
      "\n",
      "Epoch 00116: Learning rate is 0.0010.\n",
      "Epoch 117/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 4.1774e-05 - mean_squared_logarithmic_error: 4.1774e-05The average loss for epoch 116 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.1931e-05 - mean_squared_logarithmic_error: 4.1931e-05 - val_loss: 1.1714e-04 - val_mean_squared_logarithmic_error: 1.1714e-04\n",
      "\n",
      "Epoch 00117: Learning rate is 0.0010.\n",
      "Epoch 118/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 4.1381e-05 - mean_squared_logarithmic_error: 4.1381e-05The average loss for epoch 117 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 4.1360e-05 - mean_squared_logarithmic_error: 4.1360e-05 - val_loss: 1.2044e-04 - val_mean_squared_logarithmic_error: 1.2044e-04\n",
      "\n",
      "Epoch 00118: Learning rate is 0.0010.\n",
      "Epoch 119/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 4.1573e-05 - mean_squared_logarithmic_error: 4.1573e-05The average loss for epoch 118 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 919us/step - loss: 4.1807e-05 - mean_squared_logarithmic_error: 4.1807e-05 - val_loss: 1.2344e-04 - val_mean_squared_logarithmic_error: 1.2344e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00119: Learning rate is 0.0010.\n",
      "Epoch 120/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.1335e-05 - mean_squared_logarithmic_error: 4.1335e-05The average loss for epoch 119 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 4.1473e-05 - mean_squared_logarithmic_error: 4.1473e-05 - val_loss: 1.1960e-04 - val_mean_squared_logarithmic_error: 1.1960e-04\n",
      "\n",
      "Epoch 00120: Learning rate is 0.0010.\n",
      "Epoch 121/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 4.1368e-05 - mean_squared_logarithmic_error: 4.1368e-05The average loss for epoch 120 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 4.1618e-05 - mean_squared_logarithmic_error: 4.1618e-05 - val_loss: 1.1958e-04 - val_mean_squared_logarithmic_error: 1.1958e-04\n",
      "\n",
      "Epoch 00121: Learning rate is 0.0010.\n",
      "Epoch 122/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 4.2208e-05 - mean_squared_logarithmic_error: 4.2208e-05The average loss for epoch 121 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 908us/step - loss: 4.2200e-05 - mean_squared_logarithmic_error: 4.2200e-05 - val_loss: 1.1985e-04 - val_mean_squared_logarithmic_error: 1.1985e-04\n",
      "\n",
      "Epoch 00122: Learning rate is 0.0010.\n",
      "Epoch 123/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.2564e-05 - mean_squared_logarithmic_error: 4.2564e-05The average loss for epoch 122 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.2770e-05 - mean_squared_logarithmic_error: 4.2770e-05 - val_loss: 1.1610e-04 - val_mean_squared_logarithmic_error: 1.1610e-04\n",
      "\n",
      "Epoch 00123: Learning rate is 0.0010.\n",
      "Epoch 124/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 4.1038e-05 - mean_squared_logarithmic_error: 4.1038e-05The average loss for epoch 123 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.1092e-05 - mean_squared_logarithmic_error: 4.1092e-05 - val_loss: 1.1639e-04 - val_mean_squared_logarithmic_error: 1.1639e-04\n",
      "\n",
      "Epoch 00124: Learning rate is 0.0010.\n",
      "Epoch 125/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 4.0779e-05 - mean_squared_logarithmic_error: 4.0779e-05The average loss for epoch 124 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 4.0866e-05 - mean_squared_logarithmic_error: 4.0866e-05 - val_loss: 1.2041e-04 - val_mean_squared_logarithmic_error: 1.2041e-04\n",
      "\n",
      "Epoch 00125: Learning rate is 0.0010.\n",
      "Epoch 126/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 4.1255e-05 - mean_squared_logarithmic_error: 4.1255e-05The average loss for epoch 125 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 4.1244e-05 - mean_squared_logarithmic_error: 4.1244e-05 - val_loss: 1.2337e-04 - val_mean_squared_logarithmic_error: 1.2337e-04\n",
      "\n",
      "Epoch 00126: Learning rate is 0.0010.\n",
      "Epoch 127/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 4.1008e-05 - mean_squared_logarithmic_error: 4.1008e-05The average loss for epoch 126 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 903us/step - loss: 4.0780e-05 - mean_squared_logarithmic_error: 4.0780e-05 - val_loss: 1.1665e-04 - val_mean_squared_logarithmic_error: 1.1665e-04\n",
      "\n",
      "Epoch 00127: Learning rate is 0.0010.\n",
      "Epoch 128/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 4.0144e-05 - mean_squared_logarithmic_error: 4.0144e-05The average loss for epoch 127 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 4.0135e-05 - mean_squared_logarithmic_error: 4.0135e-05 - val_loss: 1.2923e-04 - val_mean_squared_logarithmic_error: 1.2923e-04\n",
      "\n",
      "Epoch 00128: Learning rate is 0.0010.\n",
      "Epoch 129/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 4.1132e-05 - mean_squared_logarithmic_error: 4.1132e-05The average loss for epoch 128 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 4.1253e-05 - mean_squared_logarithmic_error: 4.1253e-05 - val_loss: 1.1958e-04 - val_mean_squared_logarithmic_error: 1.1958e-04\n",
      "\n",
      "Epoch 00129: Learning rate is 0.0010.\n",
      "Epoch 130/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 4.0527e-05 - mean_squared_logarithmic_error: 4.0527e-05The average loss for epoch 129 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 916us/step - loss: 4.0729e-05 - mean_squared_logarithmic_error: 4.0729e-05 - val_loss: 1.1442e-04 - val_mean_squared_logarithmic_error: 1.1442e-04\n",
      "\n",
      "Epoch 00130: Learning rate is 0.0010.\n",
      "Epoch 131/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 4.0619e-05 - mean_squared_logarithmic_error: 4.0619e-05The average loss for epoch 130 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 4.0676e-05 - mean_squared_logarithmic_error: 4.0676e-05 - val_loss: 1.1192e-04 - val_mean_squared_logarithmic_error: 1.1192e-04\n",
      "\n",
      "Epoch 00131: Learning rate is 0.0010.\n",
      "Epoch 132/800\n",
      "626/653 [===========================>..] - ETA: 0s - loss: 3.9436e-05 - mean_squared_logarithmic_error: 3.9436e-05The average loss for epoch 131 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 898us/step - loss: 3.9442e-05 - mean_squared_logarithmic_error: 3.9442e-05 - val_loss: 1.1928e-04 - val_mean_squared_logarithmic_error: 1.1928e-04\n",
      "\n",
      "Epoch 00132: Learning rate is 0.0010.\n",
      "Epoch 133/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 3.9997e-05 - mean_squared_logarithmic_error: 3.9997e-05The average loss for epoch 132 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.0310e-05 - mean_squared_logarithmic_error: 4.0310e-05 - val_loss: 1.1419e-04 - val_mean_squared_logarithmic_error: 1.1419e-04\n",
      "\n",
      "Epoch 00133: Learning rate is 0.0010.\n",
      "Epoch 134/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 4.1480e-05 - mean_squared_logarithmic_error: 4.1480e-05The average loss for epoch 133 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 4.1416e-05 - mean_squared_logarithmic_error: 4.1416e-05 - val_loss: 1.1413e-04 - val_mean_squared_logarithmic_error: 1.1413e-04\n",
      "\n",
      "Epoch 00134: Learning rate is 0.0010.\n",
      "Epoch 135/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 3.9670e-05 - mean_squared_logarithmic_error: 3.9670e-05The average loss for epoch 134 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 3.9577e-05 - mean_squared_logarithmic_error: 3.9577e-05 - val_loss: 1.1434e-04 - val_mean_squared_logarithmic_error: 1.1434e-04\n",
      "\n",
      "Epoch 00135: Learning rate is 0.0010.\n",
      "Epoch 136/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 4.0299e-05 - mean_squared_logarithmic_error: 4.0299e-05The average loss for epoch 135 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.0217e-05 - mean_squared_logarithmic_error: 4.0217e-05 - val_loss: 1.2213e-04 - val_mean_squared_logarithmic_error: 1.2213e-04\n",
      "\n",
      "Epoch 00136: Learning rate is 0.0010.\n",
      "Epoch 137/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 4.1123e-05 - mean_squared_logarithmic_error: 4.1123e-05The average loss for epoch 136 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 4.1295e-05 - mean_squared_logarithmic_error: 4.1295e-05 - val_loss: 1.1345e-04 - val_mean_squared_logarithmic_error: 1.1345e-04\n",
      "\n",
      "Epoch 00137: Learning rate is 0.0010.\n",
      "Epoch 138/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 3.8685e-05 - mean_squared_logarithmic_error: 3.8685e-05The average loss for epoch 137 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 3.8615e-05 - mean_squared_logarithmic_error: 3.8615e-05 - val_loss: 1.1913e-04 - val_mean_squared_logarithmic_error: 1.1913e-04\n",
      "\n",
      "Epoch 00138: Learning rate is 0.0010.\n",
      "Epoch 139/800\n",
      "622/653 [===========================>..] - ETA: 0s - loss: 3.9609e-05 - mean_squared_logarithmic_error: 3.9609e-05The average loss for epoch 138 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 896us/step - loss: 3.9872e-05 - mean_squared_logarithmic_error: 3.9872e-05 - val_loss: 1.1348e-04 - val_mean_squared_logarithmic_error: 1.1348e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: Learning rate is 0.0010.\n",
      "Epoch 140/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 4.0669e-05 - mean_squared_logarithmic_error: 4.0669e-05The average loss for epoch 139 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 4.0792e-05 - mean_squared_logarithmic_error: 4.0792e-05 - val_loss: 1.1535e-04 - val_mean_squared_logarithmic_error: 1.1535e-04\n",
      "\n",
      "Epoch 00140: Learning rate is 0.0010.\n",
      "Epoch 141/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 3.9288e-05 - mean_squared_logarithmic_error: 3.9288e-05The average loss for epoch 140 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 3.9415e-05 - mean_squared_logarithmic_error: 3.9415e-05 - val_loss: 1.2560e-04 - val_mean_squared_logarithmic_error: 1.2560e-04\n",
      "\n",
      "Epoch 00141: Learning rate is 0.0010.\n",
      "Epoch 142/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 3.9874e-05 - mean_squared_logarithmic_error: 3.9874e-05The average loss for epoch 141 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 912us/step - loss: 3.9950e-05 - mean_squared_logarithmic_error: 3.9950e-05 - val_loss: 1.1808e-04 - val_mean_squared_logarithmic_error: 1.1808e-04\n",
      "\n",
      "Epoch 00142: Learning rate is 0.0010.\n",
      "Epoch 143/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 3.9101e-05 - mean_squared_logarithmic_error: 3.9101e-05The average loss for epoch 142 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 3.9288e-05 - mean_squared_logarithmic_error: 3.9288e-05 - val_loss: 1.1782e-04 - val_mean_squared_logarithmic_error: 1.1782e-04\n",
      "\n",
      "Epoch 00143: Learning rate is 0.0010.\n",
      "Epoch 144/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 4.0348e-05 - mean_squared_logarithmic_error: 4.0348e-05The average loss for epoch 143 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 4.0379e-05 - mean_squared_logarithmic_error: 4.0379e-05 - val_loss: 1.1308e-04 - val_mean_squared_logarithmic_error: 1.1308e-04\n",
      "\n",
      "Epoch 00144: Learning rate is 0.0010.\n",
      "Epoch 145/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 3.9566e-05 - mean_squared_logarithmic_error: 3.9566e-05The average loss for epoch 144 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 3.9470e-05 - mean_squared_logarithmic_error: 3.9470e-05 - val_loss: 1.2033e-04 - val_mean_squared_logarithmic_error: 1.2033e-04\n",
      "\n",
      "Epoch 00145: Learning rate is 0.0010.\n",
      "Epoch 146/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 3.8172e-05 - mean_squared_logarithmic_error: 3.8172e-05The average loss for epoch 145 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 3.8297e-05 - mean_squared_logarithmic_error: 3.8297e-05 - val_loss: 1.2038e-04 - val_mean_squared_logarithmic_error: 1.2038e-04\n",
      "\n",
      "Epoch 00146: Learning rate is 0.0010.\n",
      "Epoch 147/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 3.8611e-05 - mean_squared_logarithmic_error: 3.8611e-05The average loss for epoch 146 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 3.8588e-05 - mean_squared_logarithmic_error: 3.8588e-05 - val_loss: 1.1952e-04 - val_mean_squared_logarithmic_error: 1.1952e-04\n",
      "\n",
      "Epoch 00147: Learning rate is 0.0010.\n",
      "Epoch 148/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 3.9815e-05 - mean_squared_logarithmic_error: 3.9815e-05The average loss for epoch 147 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 890us/step - loss: 3.9635e-05 - mean_squared_logarithmic_error: 3.9635e-05 - val_loss: 1.1646e-04 - val_mean_squared_logarithmic_error: 1.1646e-04\n",
      "\n",
      "Epoch 00148: Learning rate is 0.0010.\n",
      "Epoch 149/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 3.8483e-05 - mean_squared_logarithmic_error: 3.8483e-05The average loss for epoch 148 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 3.8596e-05 - mean_squared_logarithmic_error: 3.8596e-05 - val_loss: 1.1911e-04 - val_mean_squared_logarithmic_error: 1.1911e-04\n",
      "\n",
      "Epoch 00149: Learning rate is 0.0010.\n",
      "Epoch 150/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 3.9234e-05 - mean_squared_logarithmic_error: 3.9234e-05The average loss for epoch 149 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 3.9358e-05 - mean_squared_logarithmic_error: 3.9358e-05 - val_loss: 1.1473e-04 - val_mean_squared_logarithmic_error: 1.1473e-04\n",
      "\n",
      "Epoch 00150: Learning rate is 0.0010.\n",
      "Epoch 151/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 3.9678e-05 - mean_squared_logarithmic_error: 3.9678e-05The average loss for epoch 150 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 3.9661e-05 - mean_squared_logarithmic_error: 3.9661e-05 - val_loss: 1.1703e-04 - val_mean_squared_logarithmic_error: 1.1703e-04\n",
      "\n",
      "Epoch 00151: Learning rate is 0.0010.\n",
      "Epoch 152/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 3.8708e-05 - mean_squared_logarithmic_error: 3.8708e-05The average loss for epoch 151 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 3.8712e-05 - mean_squared_logarithmic_error: 3.8712e-05 - val_loss: 1.1611e-04 - val_mean_squared_logarithmic_error: 1.1611e-04\n",
      "\n",
      "Epoch 00152: Learning rate is 0.0010.\n",
      "Epoch 153/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 3.9925e-05 - mean_squared_logarithmic_error: 3.9925e-05The average loss for epoch 152 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 4.0029e-05 - mean_squared_logarithmic_error: 4.0029e-05 - val_loss: 1.3178e-04 - val_mean_squared_logarithmic_error: 1.3178e-04\n",
      "\n",
      "Epoch 00153: Learning rate is 0.0010.\n",
      "Epoch 154/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 3.9079e-05 - mean_squared_logarithmic_error: 3.9079e-05The average loss for epoch 153 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 3.9143e-05 - mean_squared_logarithmic_error: 3.9143e-05 - val_loss: 1.1972e-04 - val_mean_squared_logarithmic_error: 1.1972e-04\n",
      "\n",
      "Epoch 00154: Learning rate is 0.0010.\n",
      "Epoch 155/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 3.8730e-05 - mean_squared_logarithmic_error: 3.8730e-05The average loss for epoch 154 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 884us/step - loss: 3.8582e-05 - mean_squared_logarithmic_error: 3.8582e-05 - val_loss: 1.2060e-04 - val_mean_squared_logarithmic_error: 1.2060e-04\n",
      "\n",
      "Epoch 00155: Learning rate is 0.0010.\n",
      "Epoch 156/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 3.8116e-05 - mean_squared_logarithmic_error: 3.8116e-05The average loss for epoch 155 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 908us/step - loss: 3.8366e-05 - mean_squared_logarithmic_error: 3.8366e-05 - val_loss: 1.1681e-04 - val_mean_squared_logarithmic_error: 1.1681e-04\n",
      "\n",
      "Epoch 00156: Learning rate is 0.0010.\n",
      "Epoch 157/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 3.8559e-05 - mean_squared_logarithmic_error: 3.8559e-05The average loss for epoch 156 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 3.8533e-05 - mean_squared_logarithmic_error: 3.8533e-05 - val_loss: 1.1730e-04 - val_mean_squared_logarithmic_error: 1.1730e-04\n",
      "\n",
      "Epoch 00157: Learning rate is 0.0010.\n",
      "Epoch 158/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 3.8445e-05 - mean_squared_logarithmic_error: 3.8445e-05The average loss for epoch 157 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 915us/step - loss: 3.8402e-05 - mean_squared_logarithmic_error: 3.8402e-05 - val_loss: 1.1696e-04 - val_mean_squared_logarithmic_error: 1.1696e-04\n",
      "\n",
      "Epoch 00158: Learning rate is 0.0010.\n",
      "Epoch 159/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 3.9123e-05 - mean_squared_logarithmic_error: 3.9123e-05The average loss for epoch 158 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 884us/step - loss: 3.9087e-05 - mean_squared_logarithmic_error: 3.9087e-05 - val_loss: 1.1865e-04 - val_mean_squared_logarithmic_error: 1.1865e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00159: Learning rate is 0.0010.\n",
      "Epoch 160/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 3.8208e-05 - mean_squared_logarithmic_error: 3.8208e-05The average loss for epoch 159 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 3.8189e-05 - mean_squared_logarithmic_error: 3.8189e-05 - val_loss: 1.1905e-04 - val_mean_squared_logarithmic_error: 1.1905e-04\n",
      "\n",
      "Epoch 00160: Learning rate is 0.0010.\n",
      "Epoch 161/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 3.8777e-05 - mean_squared_logarithmic_error: 3.8777e-05The average loss for epoch 160 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 3.8646e-05 - mean_squared_logarithmic_error: 3.8646e-05 - val_loss: 1.1539e-04 - val_mean_squared_logarithmic_error: 1.1539e-04\n",
      "\n",
      "Epoch 00161: Learning rate is 0.0010.\n",
      "Epoch 162/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 3.8207e-05 - mean_squared_logarithmic_error: 3.8207e-05The average loss for epoch 161 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 3.8318e-05 - mean_squared_logarithmic_error: 3.8318e-05 - val_loss: 1.1680e-04 - val_mean_squared_logarithmic_error: 1.1680e-04\n",
      "\n",
      "Epoch 00162: Learning rate is 0.0010.\n",
      "Epoch 163/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 3.7562e-05 - mean_squared_logarithmic_error: 3.7562e-05The average loss for epoch 162 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 3.7532e-05 - mean_squared_logarithmic_error: 3.7532e-05 - val_loss: 1.2022e-04 - val_mean_squared_logarithmic_error: 1.2022e-04\n",
      "\n",
      "Epoch 00163: Learning rate is 0.0010.\n",
      "Epoch 164/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 3.8328e-05 - mean_squared_logarithmic_error: 3.8328e-05The average loss for epoch 163 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 3.8269e-05 - mean_squared_logarithmic_error: 3.8269e-05 - val_loss: 1.2030e-04 - val_mean_squared_logarithmic_error: 1.2030e-04\n",
      "\n",
      "Epoch 00164: Learning rate is 0.0010.\n",
      "Epoch 165/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 3.7772e-05 - mean_squared_logarithmic_error: 3.7772e-05The average loss for epoch 164 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 894us/step - loss: 3.7598e-05 - mean_squared_logarithmic_error: 3.7598e-05 - val_loss: 1.1938e-04 - val_mean_squared_logarithmic_error: 1.1938e-04\n",
      "\n",
      "Epoch 00165: Learning rate is 0.0010.\n",
      "Epoch 166/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 3.7686e-05 - mean_squared_logarithmic_error: 3.7686e-05The average loss for epoch 165 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 3.7890e-05 - mean_squared_logarithmic_error: 3.7890e-05 - val_loss: 1.2318e-04 - val_mean_squared_logarithmic_error: 1.2318e-04\n",
      "\n",
      "Epoch 00166: Learning rate is 0.0010.\n",
      "Epoch 167/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 3.8188e-05 - mean_squared_logarithmic_error: 3.8188e-05The average loss for epoch 166 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 3.8332e-05 - mean_squared_logarithmic_error: 3.8332e-05 - val_loss: 1.1721e-04 - val_mean_squared_logarithmic_error: 1.1721e-04\n",
      "\n",
      "Epoch 00167: Learning rate is 0.0010.\n",
      "Epoch 168/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 3.7818e-05 - mean_squared_logarithmic_error: 3.7818e-05The average loss for epoch 167 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 3.7856e-05 - mean_squared_logarithmic_error: 3.7856e-05 - val_loss: 1.2032e-04 - val_mean_squared_logarithmic_error: 1.2032e-04\n",
      "\n",
      "Epoch 00168: Learning rate is 0.0010.\n",
      "Epoch 169/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 3.6806e-05 - mean_squared_logarithmic_error: 3.6806e-05The average loss for epoch 168 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 3.6960e-05 - mean_squared_logarithmic_error: 3.6960e-05 - val_loss: 1.2363e-04 - val_mean_squared_logarithmic_error: 1.2363e-04\n",
      "\n",
      "Epoch 00169: Learning rate is 0.0010.\n",
      "Epoch 170/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 3.7784e-05 - mean_squared_logarithmic_error: 3.7784e-05The average loss for epoch 169 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 868us/step - loss: 3.7739e-05 - mean_squared_logarithmic_error: 3.7739e-05 - val_loss: 1.1896e-04 - val_mean_squared_logarithmic_error: 1.1896e-04\n",
      "\n",
      "Epoch 00170: Learning rate is 0.0010.\n",
      "Epoch 171/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 3.8467e-05 - mean_squared_logarithmic_error: 3.8467e-05The average loss for epoch 170 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 3.8561e-05 - mean_squared_logarithmic_error: 3.8561e-05 - val_loss: 1.2283e-04 - val_mean_squared_logarithmic_error: 1.2283e-04\n",
      "\n",
      "Epoch 00171: Learning rate is 0.0010.\n",
      "Epoch 172/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 3.6960e-05 - mean_squared_logarithmic_error: 3.6960e-05The average loss for epoch 171 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 3.7113e-05 - mean_squared_logarithmic_error: 3.7113e-05 - val_loss: 1.2097e-04 - val_mean_squared_logarithmic_error: 1.2097e-04\n",
      "\n",
      "Epoch 00172: Learning rate is 0.0010.\n",
      "Epoch 173/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 3.8003e-05 - mean_squared_logarithmic_error: 3.8003e-05The average loss for epoch 172 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 3.7894e-05 - mean_squared_logarithmic_error: 3.7894e-05 - val_loss: 1.1832e-04 - val_mean_squared_logarithmic_error: 1.1832e-04\n",
      "\n",
      "Epoch 00173: Learning rate is 0.0010.\n",
      "Epoch 174/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 3.7588e-05 - mean_squared_logarithmic_error: 3.7588e-05The average loss for epoch 173 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 3.7481e-05 - mean_squared_logarithmic_error: 3.7481e-05 - val_loss: 1.2090e-04 - val_mean_squared_logarithmic_error: 1.2090e-04\n",
      "\n",
      "Epoch 00174: Learning rate is 0.0010.\n",
      "Epoch 175/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 3.7249e-05 - mean_squared_logarithmic_error: 3.7249e-05The average loss for epoch 174 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 3.7232e-05 - mean_squared_logarithmic_error: 3.7232e-05 - val_loss: 1.1865e-04 - val_mean_squared_logarithmic_error: 1.1865e-04\n",
      "\n",
      "Epoch 00175: Learning rate is 0.0010.\n",
      "Epoch 176/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 3.7954e-05 - mean_squared_logarithmic_error: 3.7954e-05The average loss for epoch 175 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 866us/step - loss: 3.7995e-05 - mean_squared_logarithmic_error: 3.7995e-05 - val_loss: 1.1828e-04 - val_mean_squared_logarithmic_error: 1.1828e-04\n",
      "\n",
      "Epoch 00176: Learning rate is 0.0010.\n",
      "Epoch 177/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 3.6677e-05 - mean_squared_logarithmic_error: 3.6677e-05The average loss for epoch 176 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 889us/step - loss: 3.6973e-05 - mean_squared_logarithmic_error: 3.6973e-05 - val_loss: 1.2376e-04 - val_mean_squared_logarithmic_error: 1.2376e-04\n",
      "\n",
      "Epoch 00177: Learning rate is 0.0010.\n",
      "Epoch 178/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 3.6935e-05 - mean_squared_logarithmic_error: 3.6935e-05The average loss for epoch 177 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 893us/step - loss: 3.6997e-05 - mean_squared_logarithmic_error: 3.6997e-05 - val_loss: 1.3572e-04 - val_mean_squared_logarithmic_error: 1.3572e-04\n",
      "\n",
      "Epoch 00178: Learning rate is 0.0010.\n",
      "Epoch 179/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 3.6673e-05 - mean_squared_logarithmic_error: 3.6673e-05The average loss for epoch 178 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 3.6849e-05 - mean_squared_logarithmic_error: 3.6849e-05 - val_loss: 1.2025e-04 - val_mean_squared_logarithmic_error: 1.2025e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00179: Learning rate is 0.0010.\n",
      "Epoch 180/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 3.7585e-05 - mean_squared_logarithmic_error: 3.7585e-05The average loss for epoch 179 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 3.7631e-05 - mean_squared_logarithmic_error: 3.7631e-05 - val_loss: 1.1905e-04 - val_mean_squared_logarithmic_error: 1.1905e-04\n",
      "\n",
      "Epoch 00180: Learning rate is 0.0010.\n",
      "Epoch 181/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 3.7724e-05 - mean_squared_logarithmic_error: 3.7724e-05The average loss for epoch 180 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 3.7743e-05 - mean_squared_logarithmic_error: 3.7743e-05 - val_loss: 1.1728e-04 - val_mean_squared_logarithmic_error: 1.1728e-04\n",
      "\n",
      "Epoch 00181: Learning rate is 0.0010.\n",
      "Epoch 182/800\n",
      "622/653 [===========================>..] - ETA: 0s - loss: 3.7252e-05 - mean_squared_logarithmic_error: 3.7252e-05The average loss for epoch 181 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 3.7330e-05 - mean_squared_logarithmic_error: 3.7330e-05 - val_loss: 1.1939e-04 - val_mean_squared_logarithmic_error: 1.1939e-04\n",
      "\n",
      "Epoch 00182: Learning rate is 0.0010.\n",
      "Epoch 183/800\n",
      "623/653 [===========================>..] - ETA: 0s - loss: 3.6316e-05 - mean_squared_logarithmic_error: 3.6316e-05The average loss for epoch 182 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 3.6316e-05 - mean_squared_logarithmic_error: 3.6316e-05 - val_loss: 1.2122e-04 - val_mean_squared_logarithmic_error: 1.2122e-04\n",
      "\n",
      "Epoch 00183: Learning rate is 0.0010.\n",
      "Epoch 184/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 3.6646e-05 - mean_squared_logarithmic_error: 3.6646e-05The average loss for epoch 183 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 897us/step - loss: 3.6571e-05 - mean_squared_logarithmic_error: 3.6571e-05 - val_loss: 1.1789e-04 - val_mean_squared_logarithmic_error: 1.1789e-04\n",
      "\n",
      "Epoch 00184: Learning rate is 0.0010.\n",
      "Epoch 185/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 3.6538e-05 - mean_squared_logarithmic_error: 3.6538e-05The average loss for epoch 184 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 890us/step - loss: 3.6735e-05 - mean_squared_logarithmic_error: 3.6735e-05 - val_loss: 1.1768e-04 - val_mean_squared_logarithmic_error: 1.1768e-04\n",
      "\n",
      "Epoch 00185: Learning rate is 0.0010.\n",
      "Epoch 186/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 3.6986e-05 - mean_squared_logarithmic_error: 3.6986e-05The average loss for epoch 185 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 3.6993e-05 - mean_squared_logarithmic_error: 3.6993e-05 - val_loss: 1.1896e-04 - val_mean_squared_logarithmic_error: 1.1896e-04\n",
      "\n",
      "Epoch 00186: Learning rate is 0.0010.\n",
      "Epoch 187/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 3.6614e-05 - mean_squared_logarithmic_error: 3.6614e-05The average loss for epoch 186 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 3.6596e-05 - mean_squared_logarithmic_error: 3.6596e-05 - val_loss: 1.2296e-04 - val_mean_squared_logarithmic_error: 1.2296e-04\n",
      "\n",
      "Epoch 00187: Learning rate is 0.0010.\n",
      "Epoch 188/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 3.6434e-05 - mean_squared_logarithmic_error: 3.6434e-05The average loss for epoch 187 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 3.6525e-05 - mean_squared_logarithmic_error: 3.6525e-05 - val_loss: 1.1865e-04 - val_mean_squared_logarithmic_error: 1.1865e-04\n",
      "\n",
      "Epoch 00188: Learning rate is 0.0010.\n",
      "Epoch 189/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 3.6593e-05 - mean_squared_logarithmic_error: 3.6593e-05The average loss for epoch 188 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 3.6731e-05 - mean_squared_logarithmic_error: 3.6731e-05 - val_loss: 1.2121e-04 - val_mean_squared_logarithmic_error: 1.2121e-04\n",
      "\n",
      "Epoch 00189: Learning rate is 0.0010.\n",
      "Epoch 190/800\n",
      "623/653 [===========================>..] - ETA: 0s - loss: 3.6128e-05 - mean_squared_logarithmic_error: 3.6128e-05The average loss for epoch 189 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 898us/step - loss: 3.6092e-05 - mean_squared_logarithmic_error: 3.6092e-05 - val_loss: 1.1776e-04 - val_mean_squared_logarithmic_error: 1.1776e-04\n",
      "\n",
      "Epoch 00190: Learning rate is 0.0010.\n",
      "Epoch 191/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 3.7324e-05 - mean_squared_logarithmic_error: 3.7324e-05The average loss for epoch 190 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 3.7273e-05 - mean_squared_logarithmic_error: 3.7273e-05 - val_loss: 1.1579e-04 - val_mean_squared_logarithmic_error: 1.1579e-04\n",
      "\n",
      "Epoch 00191: Learning rate is 0.0010.\n",
      "Epoch 192/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 3.5378e-05 - mean_squared_logarithmic_error: 3.5378e-05The average loss for epoch 191 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 3.5380e-05 - mean_squared_logarithmic_error: 3.5380e-05 - val_loss: 1.1945e-04 - val_mean_squared_logarithmic_error: 1.1945e-04\n",
      "\n",
      "Epoch 00192: Learning rate is 0.0010.\n",
      "Epoch 193/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 3.6671e-05 - mean_squared_logarithmic_error: 3.6671e-05The average loss for epoch 192 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 3.6656e-05 - mean_squared_logarithmic_error: 3.6656e-05 - val_loss: 1.2107e-04 - val_mean_squared_logarithmic_error: 1.2107e-04\n",
      "\n",
      "Epoch 00193: Learning rate is 0.0010.\n",
      "Epoch 194/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 3.5922e-05 - mean_squared_logarithmic_error: 3.5922e-05The average loss for epoch 193 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 917us/step - loss: 3.5921e-05 - mean_squared_logarithmic_error: 3.5921e-05 - val_loss: 1.1655e-04 - val_mean_squared_logarithmic_error: 1.1655e-04\n",
      "\n",
      "Epoch 00194: Learning rate is 0.0010.\n",
      "Epoch 195/800\n",
      "625/653 [===========================>..] - ETA: 0s - loss: 3.5600e-05 - mean_squared_logarithmic_error: 3.5600e-05The average loss for epoch 194 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 897us/step - loss: 3.6112e-05 - mean_squared_logarithmic_error: 3.6112e-05 - val_loss: 1.2141e-04 - val_mean_squared_logarithmic_error: 1.2141e-04\n",
      "\n",
      "Epoch 00195: Learning rate is 0.0010.\n",
      "Epoch 196/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 3.7445e-05 - mean_squared_logarithmic_error: 3.7445e-05The average loss for epoch 195 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 917us/step - loss: 3.7625e-05 - mean_squared_logarithmic_error: 3.7625e-05 - val_loss: 1.2472e-04 - val_mean_squared_logarithmic_error: 1.2472e-04\n",
      "\n",
      "Epoch 00196: Learning rate is 0.0010.\n",
      "Epoch 197/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 3.5743e-05 - mean_squared_logarithmic_error: 3.5743e-05The average loss for epoch 196 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 3.5746e-05 - mean_squared_logarithmic_error: 3.5746e-05 - val_loss: 1.1969e-04 - val_mean_squared_logarithmic_error: 1.1969e-04\n",
      "\n",
      "Epoch 00197: Learning rate is 0.0010.\n",
      "Epoch 198/800\n",
      "618/653 [===========================>..] - ETA: 0s - loss: 3.5869e-05 - mean_squared_logarithmic_error: 3.5869e-05The average loss for epoch 197 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 903us/step - loss: 3.6037e-05 - mean_squared_logarithmic_error: 3.6037e-05 - val_loss: 1.2036e-04 - val_mean_squared_logarithmic_error: 1.2036e-04\n",
      "\n",
      "Epoch 00198: Learning rate is 0.0010.\n",
      "Epoch 199/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 3.6245e-05 - mean_squared_logarithmic_error: 3.6245e-05The average loss for epoch 198 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 903us/step - loss: 3.6139e-05 - mean_squared_logarithmic_error: 3.6139e-05 - val_loss: 1.2031e-04 - val_mean_squared_logarithmic_error: 1.2031e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00199: Learning rate is 0.0010.\n",
      "Epoch 200/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 3.5639e-05 - mean_squared_logarithmic_error: 3.5639e-05The average loss for epoch 199 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 3.5641e-05 - mean_squared_logarithmic_error: 3.5641e-05 - val_loss: 1.2497e-04 - val_mean_squared_logarithmic_error: 1.2497e-04\n",
      "\n",
      "Epoch 00200: Learning rate is 0.0001.\n",
      "Epoch 201/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.9874e-05 - mean_squared_logarithmic_error: 2.9874e-05The average loss for epoch 200 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 2.9840e-05 - mean_squared_logarithmic_error: 2.9840e-05 - val_loss: 1.1375e-04 - val_mean_squared_logarithmic_error: 1.1375e-04\n",
      "\n",
      "Epoch 00201: Learning rate is 0.0001.\n",
      "Epoch 202/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.8318e-05 - mean_squared_logarithmic_error: 2.8318e-05The average loss for epoch 201 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 2.8433e-05 - mean_squared_logarithmic_error: 2.8433e-05 - val_loss: 1.1550e-04 - val_mean_squared_logarithmic_error: 1.1550e-04\n",
      "\n",
      "Epoch 00202: Learning rate is 0.0001.\n",
      "Epoch 203/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.8015e-05 - mean_squared_logarithmic_error: 2.8015e-05The average loss for epoch 202 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.8033e-05 - mean_squared_logarithmic_error: 2.8033e-05 - val_loss: 1.1405e-04 - val_mean_squared_logarithmic_error: 1.1405e-04\n",
      "\n",
      "Epoch 00203: Learning rate is 0.0001.\n",
      "Epoch 204/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.7476e-05 - mean_squared_logarithmic_error: 2.7476e-05The average loss for epoch 203 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 2.7556e-05 - mean_squared_logarithmic_error: 2.7556e-05 - val_loss: 1.1424e-04 - val_mean_squared_logarithmic_error: 1.1424e-04\n",
      "\n",
      "Epoch 00204: Learning rate is 0.0001.\n",
      "Epoch 205/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.7455e-05 - mean_squared_logarithmic_error: 2.7455e-05The average loss for epoch 204 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.7428e-05 - mean_squared_logarithmic_error: 2.7428e-05 - val_loss: 1.1533e-04 - val_mean_squared_logarithmic_error: 1.1533e-04\n",
      "\n",
      "Epoch 00205: Learning rate is 0.0001.\n",
      "Epoch 206/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.7018e-05 - mean_squared_logarithmic_error: 2.7018e-05The average loss for epoch 205 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.7177e-05 - mean_squared_logarithmic_error: 2.7177e-05 - val_loss: 1.1563e-04 - val_mean_squared_logarithmic_error: 1.1563e-04\n",
      "\n",
      "Epoch 00206: Learning rate is 0.0001.\n",
      "Epoch 207/800\n",
      "625/653 [===========================>..] - ETA: 0s - loss: 2.7247e-05 - mean_squared_logarithmic_error: 2.7247e-05The average loss for epoch 206 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 899us/step - loss: 2.7167e-05 - mean_squared_logarithmic_error: 2.7167e-05 - val_loss: 1.1534e-04 - val_mean_squared_logarithmic_error: 1.1534e-04\n",
      "\n",
      "Epoch 00207: Learning rate is 0.0001.\n",
      "Epoch 208/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.6997e-05 - mean_squared_logarithmic_error: 2.6997e-05The average loss for epoch 207 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 885us/step - loss: 2.7023e-05 - mean_squared_logarithmic_error: 2.7023e-05 - val_loss: 1.1586e-04 - val_mean_squared_logarithmic_error: 1.1586e-04\n",
      "\n",
      "Epoch 00208: Learning rate is 0.0001.\n",
      "Epoch 209/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.7012e-05 - mean_squared_logarithmic_error: 2.7012e-05The average loss for epoch 208 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.6964e-05 - mean_squared_logarithmic_error: 2.6964e-05 - val_loss: 1.1636e-04 - val_mean_squared_logarithmic_error: 1.1636e-04\n",
      "\n",
      "Epoch 00209: Learning rate is 0.0001.\n",
      "Epoch 210/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.6901e-05 - mean_squared_logarithmic_error: 2.6901e-05The average loss for epoch 209 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 868us/step - loss: 2.6885e-05 - mean_squared_logarithmic_error: 2.6885e-05 - val_loss: 1.1674e-04 - val_mean_squared_logarithmic_error: 1.1674e-04\n",
      "\n",
      "Epoch 00210: Learning rate is 0.0001.\n",
      "Epoch 211/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.6776e-05 - mean_squared_logarithmic_error: 2.6776e-05The average loss for epoch 210 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 2.6832e-05 - mean_squared_logarithmic_error: 2.6832e-05 - val_loss: 1.1712e-04 - val_mean_squared_logarithmic_error: 1.1712e-04\n",
      "\n",
      "Epoch 00211: Learning rate is 0.0001.\n",
      "Epoch 212/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.6773e-05 - mean_squared_logarithmic_error: 2.6773e-05The average loss for epoch 211 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 2.6729e-05 - mean_squared_logarithmic_error: 2.6729e-05 - val_loss: 1.1706e-04 - val_mean_squared_logarithmic_error: 1.1706e-04\n",
      "\n",
      "Epoch 00212: Learning rate is 0.0001.\n",
      "Epoch 213/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.6700e-05 - mean_squared_logarithmic_error: 2.6700e-05The average loss for epoch 212 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.6652e-05 - mean_squared_logarithmic_error: 2.6652e-05 - val_loss: 1.1621e-04 - val_mean_squared_logarithmic_error: 1.1621e-04\n",
      "\n",
      "Epoch 00213: Learning rate is 0.0001.\n",
      "Epoch 214/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.6633e-05 - mean_squared_logarithmic_error: 2.6633e-05The average loss for epoch 213 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.6617e-05 - mean_squared_logarithmic_error: 2.6617e-05 - val_loss: 1.1746e-04 - val_mean_squared_logarithmic_error: 1.1746e-04\n",
      "\n",
      "Epoch 00214: Learning rate is 0.0001.\n",
      "Epoch 215/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.6606e-05 - mean_squared_logarithmic_error: 2.6606e-05The average loss for epoch 214 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.6562e-05 - mean_squared_logarithmic_error: 2.6562e-05 - val_loss: 1.1715e-04 - val_mean_squared_logarithmic_error: 1.1715e-04\n",
      "\n",
      "Epoch 00215: Learning rate is 0.0001.\n",
      "Epoch 216/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.6569e-05 - mean_squared_logarithmic_error: 2.6569e-05The average loss for epoch 215 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.6483e-05 - mean_squared_logarithmic_error: 2.6483e-05 - val_loss: 1.1863e-04 - val_mean_squared_logarithmic_error: 1.1863e-04\n",
      "\n",
      "Epoch 00216: Learning rate is 0.0001.\n",
      "Epoch 217/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.6429e-05 - mean_squared_logarithmic_error: 2.6429e-05The average loss for epoch 216 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 2.6523e-05 - mean_squared_logarithmic_error: 2.6523e-05 - val_loss: 1.1888e-04 - val_mean_squared_logarithmic_error: 1.1888e-04\n",
      "\n",
      "Epoch 00217: Learning rate is 0.0001.\n",
      "Epoch 218/800\n",
      "618/653 [===========================>..] - ETA: 0s - loss: 2.6517e-05 - mean_squared_logarithmic_error: 2.6517e-05The average loss for epoch 217 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 911us/step - loss: 2.6444e-05 - mean_squared_logarithmic_error: 2.6444e-05 - val_loss: 1.1822e-04 - val_mean_squared_logarithmic_error: 1.1822e-04\n",
      "\n",
      "Epoch 00218: Learning rate is 0.0001.\n",
      "Epoch 219/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.6455e-05 - mean_squared_logarithmic_error: 2.6455e-05The average loss for epoch 218 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.6463e-05 - mean_squared_logarithmic_error: 2.6463e-05 - val_loss: 1.1956e-04 - val_mean_squared_logarithmic_error: 1.1956e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00219: Learning rate is 0.0001.\n",
      "Epoch 220/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 2.6649e-05 - mean_squared_logarithmic_error: 2.6649e-05The average loss for epoch 219 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 910us/step - loss: 2.6496e-05 - mean_squared_logarithmic_error: 2.6496e-05 - val_loss: 1.1880e-04 - val_mean_squared_logarithmic_error: 1.1880e-04\n",
      "\n",
      "Epoch 00220: Learning rate is 0.0001.\n",
      "Epoch 221/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.6371e-05 - mean_squared_logarithmic_error: 2.6371e-05The average loss for epoch 220 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 2.6337e-05 - mean_squared_logarithmic_error: 2.6337e-05 - val_loss: 1.1945e-04 - val_mean_squared_logarithmic_error: 1.1945e-04\n",
      "\n",
      "Epoch 00221: Learning rate is 0.0001.\n",
      "Epoch 222/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.6434e-05 - mean_squared_logarithmic_error: 2.6434e-05The average loss for epoch 221 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 904us/step - loss: 2.6433e-05 - mean_squared_logarithmic_error: 2.6433e-05 - val_loss: 1.1921e-04 - val_mean_squared_logarithmic_error: 1.1921e-04\n",
      "\n",
      "Epoch 00222: Learning rate is 0.0001.\n",
      "Epoch 223/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.6391e-05 - mean_squared_logarithmic_error: 2.6391e-05The average loss for epoch 222 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 911us/step - loss: 2.6391e-05 - mean_squared_logarithmic_error: 2.6391e-05 - val_loss: 1.2048e-04 - val_mean_squared_logarithmic_error: 1.2048e-04\n",
      "\n",
      "Epoch 00223: Learning rate is 0.0001.\n",
      "Epoch 224/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.6217e-05 - mean_squared_logarithmic_error: 2.6217e-05The average loss for epoch 223 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 2.6360e-05 - mean_squared_logarithmic_error: 2.6360e-05 - val_loss: 1.1955e-04 - val_mean_squared_logarithmic_error: 1.1955e-04\n",
      "\n",
      "Epoch 00224: Learning rate is 0.0001.\n",
      "Epoch 225/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.6359e-05 - mean_squared_logarithmic_error: 2.6359e-05The average loss for epoch 224 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.6257e-05 - mean_squared_logarithmic_error: 2.6257e-05 - val_loss: 1.1960e-04 - val_mean_squared_logarithmic_error: 1.1960e-04\n",
      "\n",
      "Epoch 00225: Learning rate is 0.0001.\n",
      "Epoch 226/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.6199e-05 - mean_squared_logarithmic_error: 2.6199e-05The average loss for epoch 225 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.6199e-05 - mean_squared_logarithmic_error: 2.6199e-05 - val_loss: 1.1899e-04 - val_mean_squared_logarithmic_error: 1.1899e-04\n",
      "\n",
      "Epoch 00226: Learning rate is 0.0001.\n",
      "Epoch 227/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.6157e-05 - mean_squared_logarithmic_error: 2.6157e-05The average loss for epoch 226 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 2.6244e-05 - mean_squared_logarithmic_error: 2.6244e-05 - val_loss: 1.1958e-04 - val_mean_squared_logarithmic_error: 1.1958e-04\n",
      "\n",
      "Epoch 00227: Learning rate is 0.0001.\n",
      "Epoch 228/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.6219e-05 - mean_squared_logarithmic_error: 2.6219e-05The average loss for epoch 227 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 867us/step - loss: 2.6209e-05 - mean_squared_logarithmic_error: 2.6209e-05 - val_loss: 1.2069e-04 - val_mean_squared_logarithmic_error: 1.2069e-04\n",
      "\n",
      "Epoch 00228: Learning rate is 0.0001.\n",
      "Epoch 229/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.6130e-05 - mean_squared_logarithmic_error: 2.6130e-05The average loss for epoch 228 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 873us/step - loss: 2.6125e-05 - mean_squared_logarithmic_error: 2.6125e-05 - val_loss: 1.2039e-04 - val_mean_squared_logarithmic_error: 1.2039e-04\n",
      "\n",
      "Epoch 00229: Learning rate is 0.0001.\n",
      "Epoch 230/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.6180e-05 - mean_squared_logarithmic_error: 2.6180e-05The average loss for epoch 229 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 922us/step - loss: 2.6183e-05 - mean_squared_logarithmic_error: 2.6183e-05 - val_loss: 1.2090e-04 - val_mean_squared_logarithmic_error: 1.2090e-04\n",
      "\n",
      "Epoch 00230: Learning rate is 0.0001.\n",
      "Epoch 231/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.6221e-05 - mean_squared_logarithmic_error: 2.6221e-05The average loss for epoch 230 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 870us/step - loss: 2.6200e-05 - mean_squared_logarithmic_error: 2.6200e-05 - val_loss: 1.2135e-04 - val_mean_squared_logarithmic_error: 1.2135e-04\n",
      "\n",
      "Epoch 00231: Learning rate is 0.0001.\n",
      "Epoch 232/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.6079e-05 - mean_squared_logarithmic_error: 2.6079e-05The average loss for epoch 231 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.6127e-05 - mean_squared_logarithmic_error: 2.6127e-05 - val_loss: 1.2189e-04 - val_mean_squared_logarithmic_error: 1.2189e-04\n",
      "\n",
      "Epoch 00232: Learning rate is 0.0001.\n",
      "Epoch 233/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.6302e-05 - mean_squared_logarithmic_error: 2.6302e-05The average loss for epoch 232 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.6180e-05 - mean_squared_logarithmic_error: 2.6180e-05 - val_loss: 1.2133e-04 - val_mean_squared_logarithmic_error: 1.2133e-04\n",
      "\n",
      "Epoch 00233: Learning rate is 0.0001.\n",
      "Epoch 234/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.6080e-05 - mean_squared_logarithmic_error: 2.6080e-05The average loss for epoch 233 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 889us/step - loss: 2.6077e-05 - mean_squared_logarithmic_error: 2.6077e-05 - val_loss: 1.2102e-04 - val_mean_squared_logarithmic_error: 1.2102e-04\n",
      "\n",
      "Epoch 00234: Learning rate is 0.0001.\n",
      "Epoch 235/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.6275e-05 - mean_squared_logarithmic_error: 2.6275e-05The average loss for epoch 234 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 890us/step - loss: 2.6138e-05 - mean_squared_logarithmic_error: 2.6138e-05 - val_loss: 1.2188e-04 - val_mean_squared_logarithmic_error: 1.2188e-04\n",
      "\n",
      "Epoch 00235: Learning rate is 0.0001.\n",
      "Epoch 236/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.6114e-05 - mean_squared_logarithmic_error: 2.6114e-05The average loss for epoch 235 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.6036e-05 - mean_squared_logarithmic_error: 2.6036e-05 - val_loss: 1.2131e-04 - val_mean_squared_logarithmic_error: 1.2131e-04\n",
      "\n",
      "Epoch 00236: Learning rate is 0.0001.\n",
      "Epoch 237/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.6002e-05 - mean_squared_logarithmic_error: 2.6002e-05The average loss for epoch 236 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.6036e-05 - mean_squared_logarithmic_error: 2.6036e-05 - val_loss: 1.2080e-04 - val_mean_squared_logarithmic_error: 1.2080e-04\n",
      "\n",
      "Epoch 00237: Learning rate is 0.0001.\n",
      "Epoch 238/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5959e-05 - mean_squared_logarithmic_error: 2.5959e-05The average loss for epoch 237 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.6091e-05 - mean_squared_logarithmic_error: 2.6091e-05 - val_loss: 1.2102e-04 - val_mean_squared_logarithmic_error: 1.2102e-04\n",
      "\n",
      "Epoch 00238: Learning rate is 0.0001.\n",
      "Epoch 239/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.6041e-05 - mean_squared_logarithmic_error: 2.6041e-05The average loss for epoch 238 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 2.6035e-05 - mean_squared_logarithmic_error: 2.6035e-05 - val_loss: 1.2179e-04 - val_mean_squared_logarithmic_error: 1.2179e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00239: Learning rate is 0.0001.\n",
      "Epoch 240/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.6012e-05 - mean_squared_logarithmic_error: 2.6012e-05The average loss for epoch 239 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.6002e-05 - mean_squared_logarithmic_error: 2.6002e-05 - val_loss: 1.2176e-04 - val_mean_squared_logarithmic_error: 1.2176e-04\n",
      "\n",
      "Epoch 00240: Learning rate is 0.0001.\n",
      "Epoch 241/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.6022e-05 - mean_squared_logarithmic_error: 2.6022e-05The average loss for epoch 240 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.6008e-05 - mean_squared_logarithmic_error: 2.6008e-05 - val_loss: 1.2084e-04 - val_mean_squared_logarithmic_error: 1.2084e-04\n",
      "\n",
      "Epoch 00241: Learning rate is 0.0001.\n",
      "Epoch 242/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5916e-05 - mean_squared_logarithmic_error: 2.5916e-05The average loss for epoch 241 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5952e-05 - mean_squared_logarithmic_error: 2.5952e-05 - val_loss: 1.2216e-04 - val_mean_squared_logarithmic_error: 1.2216e-04\n",
      "\n",
      "Epoch 00242: Learning rate is 0.0001.\n",
      "Epoch 243/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.6062e-05 - mean_squared_logarithmic_error: 2.6062e-05The average loss for epoch 242 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 2.6006e-05 - mean_squared_logarithmic_error: 2.6006e-05 - val_loss: 1.2102e-04 - val_mean_squared_logarithmic_error: 1.2102e-04\n",
      "\n",
      "Epoch 00243: Learning rate is 0.0001.\n",
      "Epoch 244/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.5933e-05 - mean_squared_logarithmic_error: 2.5933e-05The average loss for epoch 243 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 2.5921e-05 - mean_squared_logarithmic_error: 2.5921e-05 - val_loss: 1.2171e-04 - val_mean_squared_logarithmic_error: 1.2171e-04\n",
      "\n",
      "Epoch 00244: Learning rate is 0.0001.\n",
      "Epoch 245/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 2.5927e-05 - mean_squared_logarithmic_error: 2.5927e-05The average loss for epoch 244 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 904us/step - loss: 2.5912e-05 - mean_squared_logarithmic_error: 2.5912e-05 - val_loss: 1.2230e-04 - val_mean_squared_logarithmic_error: 1.2230e-04\n",
      "\n",
      "Epoch 00245: Learning rate is 0.0001.\n",
      "Epoch 246/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.5998e-05 - mean_squared_logarithmic_error: 2.5998e-05The average loss for epoch 245 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 2.5933e-05 - mean_squared_logarithmic_error: 2.5933e-05 - val_loss: 1.2291e-04 - val_mean_squared_logarithmic_error: 1.2291e-04\n",
      "\n",
      "Epoch 00246: Learning rate is 0.0001.\n",
      "Epoch 247/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5926e-05 - mean_squared_logarithmic_error: 2.5926e-05The average loss for epoch 246 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5875e-05 - mean_squared_logarithmic_error: 2.5875e-05 - val_loss: 1.2261e-04 - val_mean_squared_logarithmic_error: 1.2261e-04\n",
      "\n",
      "Epoch 00247: Learning rate is 0.0001.\n",
      "Epoch 248/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.5818e-05 - mean_squared_logarithmic_error: 2.5818e-05The average loss for epoch 247 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 870us/step - loss: 2.5843e-05 - mean_squared_logarithmic_error: 2.5843e-05 - val_loss: 1.2198e-04 - val_mean_squared_logarithmic_error: 1.2198e-04\n",
      "\n",
      "Epoch 00248: Learning rate is 0.0001.\n",
      "Epoch 249/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5969e-05 - mean_squared_logarithmic_error: 2.5969e-05The average loss for epoch 248 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5884e-05 - mean_squared_logarithmic_error: 2.5884e-05 - val_loss: 1.2212e-04 - val_mean_squared_logarithmic_error: 1.2212e-04\n",
      "\n",
      "Epoch 00249: Learning rate is 0.0001.\n",
      "Epoch 250/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 2.5820e-05 - mean_squared_logarithmic_error: 2.5820e-05The average loss for epoch 249 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 902us/step - loss: 2.5937e-05 - mean_squared_logarithmic_error: 2.5937e-05 - val_loss: 1.2274e-04 - val_mean_squared_logarithmic_error: 1.2274e-04\n",
      "\n",
      "Epoch 00250: Learning rate is 0.0001.\n",
      "Epoch 251/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5816e-05 - mean_squared_logarithmic_error: 2.5816e-05The average loss for epoch 250 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5885e-05 - mean_squared_logarithmic_error: 2.5885e-05 - val_loss: 1.2288e-04 - val_mean_squared_logarithmic_error: 1.2288e-04\n",
      "\n",
      "Epoch 00251: Learning rate is 0.0001.\n",
      "Epoch 252/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5909e-05 - mean_squared_logarithmic_error: 2.5909e-05The average loss for epoch 251 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5862e-05 - mean_squared_logarithmic_error: 2.5862e-05 - val_loss: 1.2313e-04 - val_mean_squared_logarithmic_error: 1.2313e-04\n",
      "\n",
      "Epoch 00252: Learning rate is 0.0001.\n",
      "Epoch 253/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5749e-05 - mean_squared_logarithmic_error: 2.5749e-05The average loss for epoch 252 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 897us/step - loss: 2.5791e-05 - mean_squared_logarithmic_error: 2.5791e-05 - val_loss: 1.2240e-04 - val_mean_squared_logarithmic_error: 1.2240e-04\n",
      "\n",
      "Epoch 00253: Learning rate is 0.0001.\n",
      "Epoch 254/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.5787e-05 - mean_squared_logarithmic_error: 2.5787e-05The average loss for epoch 253 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 2.5766e-05 - mean_squared_logarithmic_error: 2.5766e-05 - val_loss: 1.2316e-04 - val_mean_squared_logarithmic_error: 1.2316e-04\n",
      "\n",
      "Epoch 00254: Learning rate is 0.0001.\n",
      "Epoch 255/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.5702e-05 - mean_squared_logarithmic_error: 2.5702e-05The average loss for epoch 254 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.5819e-05 - mean_squared_logarithmic_error: 2.5819e-05 - val_loss: 1.2401e-04 - val_mean_squared_logarithmic_error: 1.2401e-04\n",
      "\n",
      "Epoch 00255: Learning rate is 0.0001.\n",
      "Epoch 256/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5758e-05 - mean_squared_logarithmic_error: 2.5758e-05The average loss for epoch 255 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5803e-05 - mean_squared_logarithmic_error: 2.5803e-05 - val_loss: 1.2294e-04 - val_mean_squared_logarithmic_error: 1.2294e-04\n",
      "\n",
      "Epoch 00256: Learning rate is 0.0001.\n",
      "Epoch 257/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5722e-05 - mean_squared_logarithmic_error: 2.5722e-05The average loss for epoch 256 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 2.5747e-05 - mean_squared_logarithmic_error: 2.5747e-05 - val_loss: 1.2253e-04 - val_mean_squared_logarithmic_error: 1.2253e-04\n",
      "\n",
      "Epoch 00257: Learning rate is 0.0001.\n",
      "Epoch 258/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.5717e-05 - mean_squared_logarithmic_error: 2.5717e-05The average loss for epoch 257 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5726e-05 - mean_squared_logarithmic_error: 2.5726e-05 - val_loss: 1.2351e-04 - val_mean_squared_logarithmic_error: 1.2351e-04\n",
      "\n",
      "Epoch 00258: Learning rate is 0.0001.\n",
      "Epoch 259/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.5832e-05 - mean_squared_logarithmic_error: 2.5832e-05The average loss for epoch 258 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.5753e-05 - mean_squared_logarithmic_error: 2.5753e-05 - val_loss: 1.2263e-04 - val_mean_squared_logarithmic_error: 1.2263e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00259: Learning rate is 0.0001.\n",
      "Epoch 260/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5744e-05 - mean_squared_logarithmic_error: 2.5744e-05The average loss for epoch 259 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 2.5730e-05 - mean_squared_logarithmic_error: 2.5730e-05 - val_loss: 1.2239e-04 - val_mean_squared_logarithmic_error: 1.2239e-04\n",
      "\n",
      "Epoch 00260: Learning rate is 0.0001.\n",
      "Epoch 261/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5802e-05 - mean_squared_logarithmic_error: 2.5802e-05The average loss for epoch 260 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5786e-05 - mean_squared_logarithmic_error: 2.5786e-05 - val_loss: 1.2201e-04 - val_mean_squared_logarithmic_error: 1.2201e-04\n",
      "\n",
      "Epoch 00261: Learning rate is 0.0001.\n",
      "Epoch 262/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5800e-05 - mean_squared_logarithmic_error: 2.5800e-05The average loss for epoch 261 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5767e-05 - mean_squared_logarithmic_error: 2.5767e-05 - val_loss: 1.2246e-04 - val_mean_squared_logarithmic_error: 1.2246e-04\n",
      "\n",
      "Epoch 00262: Learning rate is 0.0001.\n",
      "Epoch 263/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.5717e-05 - mean_squared_logarithmic_error: 2.5717e-05The average loss for epoch 262 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.5751e-05 - mean_squared_logarithmic_error: 2.5751e-05 - val_loss: 1.2265e-04 - val_mean_squared_logarithmic_error: 1.2265e-04\n",
      "\n",
      "Epoch 00263: Learning rate is 0.0001.\n",
      "Epoch 264/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.5729e-05 - mean_squared_logarithmic_error: 2.5729e-05The average loss for epoch 263 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 913us/step - loss: 2.5645e-05 - mean_squared_logarithmic_error: 2.5645e-05 - val_loss: 1.2246e-04 - val_mean_squared_logarithmic_error: 1.2246e-04\n",
      "\n",
      "Epoch 00264: Learning rate is 0.0001.\n",
      "Epoch 265/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5671e-05 - mean_squared_logarithmic_error: 2.5671e-05The average loss for epoch 264 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5752e-05 - mean_squared_logarithmic_error: 2.5752e-05 - val_loss: 1.2334e-04 - val_mean_squared_logarithmic_error: 1.2334e-04\n",
      "\n",
      "Epoch 00265: Learning rate is 0.0001.\n",
      "Epoch 266/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.5585e-05 - mean_squared_logarithmic_error: 2.5585e-05The average loss for epoch 265 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5718e-05 - mean_squared_logarithmic_error: 2.5718e-05 - val_loss: 1.2263e-04 - val_mean_squared_logarithmic_error: 1.2263e-04\n",
      "\n",
      "Epoch 00266: Learning rate is 0.0001.\n",
      "Epoch 267/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.5515e-05 - mean_squared_logarithmic_error: 2.5515e-05The average loss for epoch 266 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 904us/step - loss: 2.5631e-05 - mean_squared_logarithmic_error: 2.5631e-05 - val_loss: 1.2325e-04 - val_mean_squared_logarithmic_error: 1.2325e-04\n",
      "\n",
      "Epoch 00267: Learning rate is 0.0001.\n",
      "Epoch 268/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.5642e-05 - mean_squared_logarithmic_error: 2.5642e-05The average loss for epoch 267 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5658e-05 - mean_squared_logarithmic_error: 2.5658e-05 - val_loss: 1.2379e-04 - val_mean_squared_logarithmic_error: 1.2379e-04\n",
      "\n",
      "Epoch 00268: Learning rate is 0.0001.\n",
      "Epoch 269/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.5689e-05 - mean_squared_logarithmic_error: 2.5689e-05The average loss for epoch 268 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5663e-05 - mean_squared_logarithmic_error: 2.5663e-05 - val_loss: 1.2312e-04 - val_mean_squared_logarithmic_error: 1.2312e-04\n",
      "\n",
      "Epoch 00269: Learning rate is 0.0001.\n",
      "Epoch 270/800\n",
      "622/653 [===========================>..] - ETA: 0s - loss: 2.5788e-05 - mean_squared_logarithmic_error: 2.5788e-05The average loss for epoch 269 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 900us/step - loss: 2.5702e-05 - mean_squared_logarithmic_error: 2.5702e-05 - val_loss: 1.2359e-04 - val_mean_squared_logarithmic_error: 1.2359e-04\n",
      "\n",
      "Epoch 00270: Learning rate is 0.0001.\n",
      "Epoch 271/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.5695e-05 - mean_squared_logarithmic_error: 2.5695e-05The average loss for epoch 270 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 2.5658e-05 - mean_squared_logarithmic_error: 2.5658e-05 - val_loss: 1.2322e-04 - val_mean_squared_logarithmic_error: 1.2322e-04\n",
      "\n",
      "Epoch 00271: Learning rate is 0.0001.\n",
      "Epoch 272/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5550e-05 - mean_squared_logarithmic_error: 2.5550e-05The average loss for epoch 271 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5649e-05 - mean_squared_logarithmic_error: 2.5649e-05 - val_loss: 1.2431e-04 - val_mean_squared_logarithmic_error: 1.2431e-04\n",
      "\n",
      "Epoch 00272: Learning rate is 0.0001.\n",
      "Epoch 273/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.5436e-05 - mean_squared_logarithmic_error: 2.5436e-05The average loss for epoch 272 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.5622e-05 - mean_squared_logarithmic_error: 2.5622e-05 - val_loss: 1.2339e-04 - val_mean_squared_logarithmic_error: 1.2339e-04\n",
      "\n",
      "Epoch 00273: Learning rate is 0.0001.\n",
      "Epoch 274/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.5721e-05 - mean_squared_logarithmic_error: 2.5721e-05The average loss for epoch 273 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 2.5658e-05 - mean_squared_logarithmic_error: 2.5658e-05 - val_loss: 1.2355e-04 - val_mean_squared_logarithmic_error: 1.2355e-04\n",
      "\n",
      "Epoch 00274: Learning rate is 0.0001.\n",
      "Epoch 275/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.5568e-05 - mean_squared_logarithmic_error: 2.5568e-05The average loss for epoch 274 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.5587e-05 - mean_squared_logarithmic_error: 2.5587e-05 - val_loss: 1.2354e-04 - val_mean_squared_logarithmic_error: 1.2354e-04\n",
      "\n",
      "Epoch 00275: Learning rate is 0.0001.\n",
      "Epoch 276/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.5590e-05 - mean_squared_logarithmic_error: 2.5590e-05The average loss for epoch 275 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 892us/step - loss: 2.5554e-05 - mean_squared_logarithmic_error: 2.5554e-05 - val_loss: 1.2498e-04 - val_mean_squared_logarithmic_error: 1.2498e-04\n",
      "\n",
      "Epoch 00276: Learning rate is 0.0001.\n",
      "Epoch 277/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5656e-05 - mean_squared_logarithmic_error: 2.5656e-05The average loss for epoch 276 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.5634e-05 - mean_squared_logarithmic_error: 2.5634e-05 - val_loss: 1.2377e-04 - val_mean_squared_logarithmic_error: 1.2377e-04\n",
      "\n",
      "Epoch 00277: Learning rate is 0.0001.\n",
      "Epoch 278/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.5657e-05 - mean_squared_logarithmic_error: 2.5657e-05The average loss for epoch 277 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5627e-05 - mean_squared_logarithmic_error: 2.5627e-05 - val_loss: 1.2456e-04 - val_mean_squared_logarithmic_error: 1.2456e-04\n",
      "\n",
      "Epoch 00278: Learning rate is 0.0001.\n",
      "Epoch 279/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.5616e-05 - mean_squared_logarithmic_error: 2.5616e-05The average loss for epoch 278 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 868us/step - loss: 2.5568e-05 - mean_squared_logarithmic_error: 2.5568e-05 - val_loss: 1.2450e-04 - val_mean_squared_logarithmic_error: 1.2450e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: Learning rate is 0.0001.\n",
      "Epoch 280/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.5628e-05 - mean_squared_logarithmic_error: 2.5628e-05The average loss for epoch 279 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 867us/step - loss: 2.5625e-05 - mean_squared_logarithmic_error: 2.5625e-05 - val_loss: 1.2427e-04 - val_mean_squared_logarithmic_error: 1.2427e-04\n",
      "\n",
      "Epoch 00280: Learning rate is 0.0001.\n",
      "Epoch 281/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.5546e-05 - mean_squared_logarithmic_error: 2.5546e-05The average loss for epoch 280 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 867us/step - loss: 2.5560e-05 - mean_squared_logarithmic_error: 2.5560e-05 - val_loss: 1.2476e-04 - val_mean_squared_logarithmic_error: 1.2476e-04\n",
      "\n",
      "Epoch 00281: Learning rate is 0.0001.\n",
      "Epoch 282/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.5607e-05 - mean_squared_logarithmic_error: 2.5607e-05The average loss for epoch 281 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 2.5640e-05 - mean_squared_logarithmic_error: 2.5640e-05 - val_loss: 1.2514e-04 - val_mean_squared_logarithmic_error: 1.2514e-04\n",
      "\n",
      "Epoch 00282: Learning rate is 0.0001.\n",
      "Epoch 283/800\n",
      "590/653 [==========================>...] - ETA: 0s - loss: 2.5533e-05 - mean_squared_logarithmic_error: 2.5533e-05The average loss for epoch 282 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.5561e-05 - mean_squared_logarithmic_error: 2.5561e-05 - val_loss: 1.2471e-04 - val_mean_squared_logarithmic_error: 1.2471e-04\n",
      "\n",
      "Epoch 00283: Learning rate is 0.0001.\n",
      "Epoch 284/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5455e-05 - mean_squared_logarithmic_error: 2.5455e-05The average loss for epoch 283 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5482e-05 - mean_squared_logarithmic_error: 2.5482e-05 - val_loss: 1.2411e-04 - val_mean_squared_logarithmic_error: 1.2411e-04\n",
      "\n",
      "Epoch 00284: Learning rate is 0.0001.\n",
      "Epoch 285/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5572e-05 - mean_squared_logarithmic_error: 2.5572e-05The average loss for epoch 284 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.5534e-05 - mean_squared_logarithmic_error: 2.5534e-05 - val_loss: 1.2534e-04 - val_mean_squared_logarithmic_error: 1.2534e-04\n",
      "\n",
      "Epoch 00285: Learning rate is 0.0001.\n",
      "Epoch 286/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5533e-05 - mean_squared_logarithmic_error: 2.5533e-05The average loss for epoch 285 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 2.5504e-05 - mean_squared_logarithmic_error: 2.5504e-05 - val_loss: 1.2431e-04 - val_mean_squared_logarithmic_error: 1.2431e-04\n",
      "\n",
      "Epoch 00286: Learning rate is 0.0001.\n",
      "Epoch 287/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.5460e-05 - mean_squared_logarithmic_error: 2.5460e-05The average loss for epoch 286 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 895us/step - loss: 2.5459e-05 - mean_squared_logarithmic_error: 2.5459e-05 - val_loss: 1.2484e-04 - val_mean_squared_logarithmic_error: 1.2484e-04\n",
      "\n",
      "Epoch 00287: Learning rate is 0.0001.\n",
      "Epoch 288/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.5668e-05 - mean_squared_logarithmic_error: 2.5668e-05The average loss for epoch 287 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 893us/step - loss: 2.5556e-05 - mean_squared_logarithmic_error: 2.5556e-05 - val_loss: 1.2471e-04 - val_mean_squared_logarithmic_error: 1.2471e-04\n",
      "\n",
      "Epoch 00288: Learning rate is 0.0001.\n",
      "Epoch 289/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.5272e-05 - mean_squared_logarithmic_error: 2.5272e-05The average loss for epoch 288 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 2.5460e-05 - mean_squared_logarithmic_error: 2.5460e-05 - val_loss: 1.2521e-04 - val_mean_squared_logarithmic_error: 1.2521e-04\n",
      "\n",
      "Epoch 00289: Learning rate is 0.0001.\n",
      "Epoch 290/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5745e-05 - mean_squared_logarithmic_error: 2.5745e-05The average loss for epoch 289 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5673e-05 - mean_squared_logarithmic_error: 2.5673e-05 - val_loss: 1.2535e-04 - val_mean_squared_logarithmic_error: 1.2535e-04\n",
      "\n",
      "Epoch 00290: Learning rate is 0.0001.\n",
      "Epoch 291/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5372e-05 - mean_squared_logarithmic_error: 2.5372e-05The average loss for epoch 290 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.5446e-05 - mean_squared_logarithmic_error: 2.5446e-05 - val_loss: 1.2455e-04 - val_mean_squared_logarithmic_error: 1.2455e-04\n",
      "\n",
      "Epoch 00291: Learning rate is 0.0001.\n",
      "Epoch 292/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5537e-05 - mean_squared_logarithmic_error: 2.5537e-05The average loss for epoch 291 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5501e-05 - mean_squared_logarithmic_error: 2.5501e-05 - val_loss: 1.2521e-04 - val_mean_squared_logarithmic_error: 1.2521e-04\n",
      "\n",
      "Epoch 00292: Learning rate is 0.0001.\n",
      "Epoch 293/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5503e-05 - mean_squared_logarithmic_error: 2.5503e-05The average loss for epoch 292 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.5454e-05 - mean_squared_logarithmic_error: 2.5454e-05 - val_loss: 1.2472e-04 - val_mean_squared_logarithmic_error: 1.2472e-04\n",
      "\n",
      "Epoch 00293: Learning rate is 0.0001.\n",
      "Epoch 294/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5582e-05 - mean_squared_logarithmic_error: 2.5582e-05The average loss for epoch 293 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.5520e-05 - mean_squared_logarithmic_error: 2.5520e-05 - val_loss: 1.2453e-04 - val_mean_squared_logarithmic_error: 1.2453e-04\n",
      "\n",
      "Epoch 00294: Learning rate is 0.0001.\n",
      "Epoch 295/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5500e-05 - mean_squared_logarithmic_error: 2.5500e-05The average loss for epoch 294 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 878us/step - loss: 2.5448e-05 - mean_squared_logarithmic_error: 2.5448e-05 - val_loss: 1.2512e-04 - val_mean_squared_logarithmic_error: 1.2512e-04\n",
      "\n",
      "Epoch 00295: Learning rate is 0.0001.\n",
      "Epoch 296/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5490e-05 - mean_squared_logarithmic_error: 2.5490e-05The average loss for epoch 295 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5426e-05 - mean_squared_logarithmic_error: 2.5426e-05 - val_loss: 1.2481e-04 - val_mean_squared_logarithmic_error: 1.2481e-04\n",
      "\n",
      "Epoch 00296: Learning rate is 0.0001.\n",
      "Epoch 297/800\n",
      "622/653 [===========================>..] - ETA: 0s - loss: 2.5490e-05 - mean_squared_logarithmic_error: 2.5490e-05The average loss for epoch 296 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 922us/step - loss: 2.5441e-05 - mean_squared_logarithmic_error: 2.5441e-05 - val_loss: 1.2521e-04 - val_mean_squared_logarithmic_error: 1.2521e-04\n",
      "\n",
      "Epoch 00297: Learning rate is 0.0001.\n",
      "Epoch 298/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.5279e-05 - mean_squared_logarithmic_error: 2.5279e-05The average loss for epoch 297 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.5445e-05 - mean_squared_logarithmic_error: 2.5445e-05 - val_loss: 1.2550e-04 - val_mean_squared_logarithmic_error: 1.2550e-04\n",
      "\n",
      "Epoch 00298: Learning rate is 0.0001.\n",
      "Epoch 299/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.5338e-05 - mean_squared_logarithmic_error: 2.5338e-05The average loss for epoch 298 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 886us/step - loss: 2.5465e-05 - mean_squared_logarithmic_error: 2.5465e-05 - val_loss: 1.2553e-04 - val_mean_squared_logarithmic_error: 1.2553e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00299: Learning rate is 0.0001.\n",
      "Epoch 300/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.5446e-05 - mean_squared_logarithmic_error: 2.5446e-05The average loss for epoch 299 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 2.5409e-05 - mean_squared_logarithmic_error: 2.5409e-05 - val_loss: 1.2627e-04 - val_mean_squared_logarithmic_error: 1.2627e-04\n",
      "\n",
      "Epoch 00300: Learning rate is 0.0001.\n",
      "Epoch 301/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5515e-05 - mean_squared_logarithmic_error: 2.5515e-05The average loss for epoch 300 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5440e-05 - mean_squared_logarithmic_error: 2.5440e-05 - val_loss: 1.2477e-04 - val_mean_squared_logarithmic_error: 1.2477e-04\n",
      "\n",
      "Epoch 00301: Learning rate is 0.0001.\n",
      "Epoch 302/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.5574e-05 - mean_squared_logarithmic_error: 2.5574e-05The average loss for epoch 301 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 893us/step - loss: 2.5507e-05 - mean_squared_logarithmic_error: 2.5507e-05 - val_loss: 1.2529e-04 - val_mean_squared_logarithmic_error: 1.2529e-04\n",
      "\n",
      "Epoch 00302: Learning rate is 0.0001.\n",
      "Epoch 303/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.5387e-05 - mean_squared_logarithmic_error: 2.5387e-05The average loss for epoch 302 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 887us/step - loss: 2.5380e-05 - mean_squared_logarithmic_error: 2.5380e-05 - val_loss: 1.2540e-04 - val_mean_squared_logarithmic_error: 1.2540e-04\n",
      "\n",
      "Epoch 00303: Learning rate is 0.0001.\n",
      "Epoch 304/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5407e-05 - mean_squared_logarithmic_error: 2.5407e-05The average loss for epoch 303 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5398e-05 - mean_squared_logarithmic_error: 2.5398e-05 - val_loss: 1.2552e-04 - val_mean_squared_logarithmic_error: 1.2552e-04\n",
      "\n",
      "Epoch 00304: Learning rate is 0.0001.\n",
      "Epoch 305/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.5399e-05 - mean_squared_logarithmic_error: 2.5399e-05The average loss for epoch 304 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 869us/step - loss: 2.5346e-05 - mean_squared_logarithmic_error: 2.5346e-05 - val_loss: 1.2545e-04 - val_mean_squared_logarithmic_error: 1.2545e-04\n",
      "\n",
      "Epoch 00305: Learning rate is 0.0001.\n",
      "Epoch 306/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5497e-05 - mean_squared_logarithmic_error: 2.5497e-05The average loss for epoch 305 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 2.5424e-05 - mean_squared_logarithmic_error: 2.5424e-05 - val_loss: 1.2519e-04 - val_mean_squared_logarithmic_error: 1.2519e-04\n",
      "\n",
      "Epoch 00306: Learning rate is 0.0001.\n",
      "Epoch 307/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5356e-05 - mean_squared_logarithmic_error: 2.5356e-05The average loss for epoch 306 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5352e-05 - mean_squared_logarithmic_error: 2.5352e-05 - val_loss: 1.2545e-04 - val_mean_squared_logarithmic_error: 1.2545e-04\n",
      "\n",
      "Epoch 00307: Learning rate is 0.0001.\n",
      "Epoch 308/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5334e-05 - mean_squared_logarithmic_error: 2.5334e-05The average loss for epoch 307 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5299e-05 - mean_squared_logarithmic_error: 2.5299e-05 - val_loss: 1.2555e-04 - val_mean_squared_logarithmic_error: 1.2555e-04\n",
      "\n",
      "Epoch 00308: Learning rate is 0.0001.\n",
      "Epoch 309/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5331e-05 - mean_squared_logarithmic_error: 2.5331e-05The average loss for epoch 308 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 2.5420e-05 - mean_squared_logarithmic_error: 2.5420e-05 - val_loss: 1.2552e-04 - val_mean_squared_logarithmic_error: 1.2552e-04\n",
      "\n",
      "Epoch 00309: Learning rate is 0.0001.\n",
      "Epoch 310/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5429e-05 - mean_squared_logarithmic_error: 2.5429e-05The average loss for epoch 309 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 895us/step - loss: 2.5363e-05 - mean_squared_logarithmic_error: 2.5363e-05 - val_loss: 1.2670e-04 - val_mean_squared_logarithmic_error: 1.2670e-04\n",
      "\n",
      "Epoch 00310: Learning rate is 0.0001.\n",
      "Epoch 311/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.5533e-05 - mean_squared_logarithmic_error: 2.5533e-05The average loss for epoch 310 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.5441e-05 - mean_squared_logarithmic_error: 2.5441e-05 - val_loss: 1.2585e-04 - val_mean_squared_logarithmic_error: 1.2585e-04\n",
      "\n",
      "Epoch 00311: Learning rate is 0.0001.\n",
      "Epoch 312/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.5275e-05 - mean_squared_logarithmic_error: 2.5275e-05The average loss for epoch 311 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 870us/step - loss: 2.5277e-05 - mean_squared_logarithmic_error: 2.5277e-05 - val_loss: 1.2631e-04 - val_mean_squared_logarithmic_error: 1.2631e-04\n",
      "\n",
      "Epoch 00312: Learning rate is 0.0001.\n",
      "Epoch 313/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5476e-05 - mean_squared_logarithmic_error: 2.5476e-05The average loss for epoch 312 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 894us/step - loss: 2.5408e-05 - mean_squared_logarithmic_error: 2.5408e-05 - val_loss: 1.2618e-04 - val_mean_squared_logarithmic_error: 1.2618e-04\n",
      "\n",
      "Epoch 00313: Learning rate is 0.0001.\n",
      "Epoch 314/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5385e-05 - mean_squared_logarithmic_error: 2.5385e-05The average loss for epoch 313 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5306e-05 - mean_squared_logarithmic_error: 2.5306e-05 - val_loss: 1.2592e-04 - val_mean_squared_logarithmic_error: 1.2592e-04\n",
      "\n",
      "Epoch 00314: Learning rate is 0.0001.\n",
      "Epoch 315/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.5327e-05 - mean_squared_logarithmic_error: 2.5327e-05The average loss for epoch 314 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 2.5416e-05 - mean_squared_logarithmic_error: 2.5416e-05 - val_loss: 1.2600e-04 - val_mean_squared_logarithmic_error: 1.2600e-04\n",
      "\n",
      "Epoch 00315: Learning rate is 0.0001.\n",
      "Epoch 316/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.5393e-05 - mean_squared_logarithmic_error: 2.5393e-05The average loss for epoch 315 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 884us/step - loss: 2.5407e-05 - mean_squared_logarithmic_error: 2.5407e-05 - val_loss: 1.2647e-04 - val_mean_squared_logarithmic_error: 1.2647e-04\n",
      "\n",
      "Epoch 00316: Learning rate is 0.0001.\n",
      "Epoch 317/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5333e-05 - mean_squared_logarithmic_error: 2.5333e-05The average loss for epoch 316 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5291e-05 - mean_squared_logarithmic_error: 2.5291e-05 - val_loss: 1.2632e-04 - val_mean_squared_logarithmic_error: 1.2632e-04\n",
      "\n",
      "Epoch 00317: Learning rate is 0.0001.\n",
      "Epoch 318/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.5257e-05 - mean_squared_logarithmic_error: 2.5257e-05The average loss for epoch 317 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 893us/step - loss: 2.5319e-05 - mean_squared_logarithmic_error: 2.5319e-05 - val_loss: 1.2655e-04 - val_mean_squared_logarithmic_error: 1.2655e-04\n",
      "\n",
      "Epoch 00318: Learning rate is 0.0001.\n",
      "Epoch 319/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.5387e-05 - mean_squared_logarithmic_error: 2.5387e-05The average loss for epoch 318 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 907us/step - loss: 2.5281e-05 - mean_squared_logarithmic_error: 2.5281e-05 - val_loss: 1.2551e-04 - val_mean_squared_logarithmic_error: 1.2551e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: Learning rate is 0.0001.\n",
      "Epoch 320/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.5207e-05 - mean_squared_logarithmic_error: 2.5207e-05The average loss for epoch 319 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 2.5248e-05 - mean_squared_logarithmic_error: 2.5248e-05 - val_loss: 1.2632e-04 - val_mean_squared_logarithmic_error: 1.2632e-04\n",
      "\n",
      "Epoch 00320: Learning rate is 0.0001.\n",
      "Epoch 321/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.5235e-05 - mean_squared_logarithmic_error: 2.5235e-05The average loss for epoch 320 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 2.5326e-05 - mean_squared_logarithmic_error: 2.5326e-05 - val_loss: 1.2609e-04 - val_mean_squared_logarithmic_error: 1.2609e-04\n",
      "\n",
      "Epoch 00321: Learning rate is 0.0001.\n",
      "Epoch 322/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5295e-05 - mean_squared_logarithmic_error: 2.5295e-05The average loss for epoch 321 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 874us/step - loss: 2.5265e-05 - mean_squared_logarithmic_error: 2.5265e-05 - val_loss: 1.2693e-04 - val_mean_squared_logarithmic_error: 1.2693e-04\n",
      "\n",
      "Epoch 00322: Learning rate is 0.0001.\n",
      "Epoch 323/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5293e-05 - mean_squared_logarithmic_error: 2.5293e-05The average loss for epoch 322 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5283e-05 - mean_squared_logarithmic_error: 2.5283e-05 - val_loss: 1.2742e-04 - val_mean_squared_logarithmic_error: 1.2742e-04\n",
      "\n",
      "Epoch 00323: Learning rate is 0.0001.\n",
      "Epoch 324/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.5234e-05 - mean_squared_logarithmic_error: 2.5234e-05The average loss for epoch 323 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 872us/step - loss: 2.5279e-05 - mean_squared_logarithmic_error: 2.5279e-05 - val_loss: 1.2720e-04 - val_mean_squared_logarithmic_error: 1.2720e-04\n",
      "\n",
      "Epoch 00324: Learning rate is 0.0001.\n",
      "Epoch 325/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.5289e-05 - mean_squared_logarithmic_error: 2.5289e-05The average loss for epoch 324 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 875us/step - loss: 2.5259e-05 - mean_squared_logarithmic_error: 2.5259e-05 - val_loss: 1.2652e-04 - val_mean_squared_logarithmic_error: 1.2652e-04\n",
      "\n",
      "Epoch 00325: Learning rate is 0.0001.\n",
      "Epoch 326/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.5151e-05 - mean_squared_logarithmic_error: 2.5151e-05The average loss for epoch 325 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5247e-05 - mean_squared_logarithmic_error: 2.5247e-05 - val_loss: 1.2691e-04 - val_mean_squared_logarithmic_error: 1.2691e-04\n",
      "\n",
      "Epoch 00326: Learning rate is 0.0001.\n",
      "Epoch 327/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.5188e-05 - mean_squared_logarithmic_error: 2.5188e-05The average loss for epoch 326 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 2.5221e-05 - mean_squared_logarithmic_error: 2.5221e-05 - val_loss: 1.2654e-04 - val_mean_squared_logarithmic_error: 1.2654e-04\n",
      "\n",
      "Epoch 00327: Learning rate is 0.0001.\n",
      "Epoch 328/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.5199e-05 - mean_squared_logarithmic_error: 2.5199e-05The average loss for epoch 327 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 876us/step - loss: 2.5222e-05 - mean_squared_logarithmic_error: 2.5222e-05 - val_loss: 1.2623e-04 - val_mean_squared_logarithmic_error: 1.2623e-04\n",
      "\n",
      "Epoch 00328: Learning rate is 0.0001.\n",
      "Epoch 329/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.5179e-05 - mean_squared_logarithmic_error: 2.5179e-05The average loss for epoch 328 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 891us/step - loss: 2.5276e-05 - mean_squared_logarithmic_error: 2.5276e-05 - val_loss: 1.2670e-04 - val_mean_squared_logarithmic_error: 1.2670e-04\n",
      "\n",
      "Epoch 00329: Learning rate is 0.0001.\n",
      "Epoch 330/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5176e-05 - mean_squared_logarithmic_error: 2.5176e-05The average loss for epoch 329 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5221e-05 - mean_squared_logarithmic_error: 2.5221e-05 - val_loss: 1.2670e-04 - val_mean_squared_logarithmic_error: 1.2670e-04\n",
      "\n",
      "Epoch 00330: Learning rate is 0.0001.\n",
      "Epoch 331/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5249e-05 - mean_squared_logarithmic_error: 2.5249e-05The average loss for epoch 330 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5192e-05 - mean_squared_logarithmic_error: 2.5192e-05 - val_loss: 1.2725e-04 - val_mean_squared_logarithmic_error: 1.2725e-04\n",
      "\n",
      "Epoch 00331: Learning rate is 0.0001.\n",
      "Epoch 332/800\n",
      "593/653 [==========================>...] - ETA: 0s - loss: 2.5401e-05 - mean_squared_logarithmic_error: 2.5401e-05The average loss for epoch 331 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.5105e-05 - mean_squared_logarithmic_error: 2.5105e-05 - val_loss: 1.2732e-04 - val_mean_squared_logarithmic_error: 1.2732e-04\n",
      "\n",
      "Epoch 00332: Learning rate is 0.0001.\n",
      "Epoch 333/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.5367e-05 - mean_squared_logarithmic_error: 2.5367e-05The average loss for epoch 332 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 931us/step - loss: 2.5317e-05 - mean_squared_logarithmic_error: 2.5317e-05 - val_loss: 1.2723e-04 - val_mean_squared_logarithmic_error: 1.2723e-04\n",
      "\n",
      "Epoch 00333: Learning rate is 0.0001.\n",
      "Epoch 334/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.4968e-05 - mean_squared_logarithmic_error: 2.4968e-05The average loss for epoch 333 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 2.5144e-05 - mean_squared_logarithmic_error: 2.5144e-05 - val_loss: 1.2773e-04 - val_mean_squared_logarithmic_error: 1.2773e-04\n",
      "\n",
      "Epoch 00334: Learning rate is 0.0001.\n",
      "Epoch 335/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4996e-05 - mean_squared_logarithmic_error: 2.4996e-05The average loss for epoch 334 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 881us/step - loss: 2.5210e-05 - mean_squared_logarithmic_error: 2.5210e-05 - val_loss: 1.2700e-04 - val_mean_squared_logarithmic_error: 1.2700e-04\n",
      "\n",
      "Epoch 00335: Learning rate is 0.0001.\n",
      "Epoch 336/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 2.4923e-05 - mean_squared_logarithmic_error: 2.4923e-05The average loss for epoch 335 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 908us/step - loss: 2.5185e-05 - mean_squared_logarithmic_error: 2.5185e-05 - val_loss: 1.2764e-04 - val_mean_squared_logarithmic_error: 1.2764e-04\n",
      "\n",
      "Epoch 00336: Learning rate is 0.0001.\n",
      "Epoch 337/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5297e-05 - mean_squared_logarithmic_error: 2.5297e-05The average loss for epoch 336 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 880us/step - loss: 2.5236e-05 - mean_squared_logarithmic_error: 2.5236e-05 - val_loss: 1.2722e-04 - val_mean_squared_logarithmic_error: 1.2722e-04\n",
      "\n",
      "Epoch 00337: Learning rate is 0.0001.\n",
      "Epoch 338/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.5071e-05 - mean_squared_logarithmic_error: 2.5071e-05The average loss for epoch 337 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 888us/step - loss: 2.5222e-05 - mean_squared_logarithmic_error: 2.5222e-05 - val_loss: 1.2779e-04 - val_mean_squared_logarithmic_error: 1.2779e-04\n",
      "\n",
      "Epoch 00338: Learning rate is 0.0001.\n",
      "Epoch 339/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.5234e-05 - mean_squared_logarithmic_error: 2.5234e-05The average loss for epoch 338 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 2.5180e-05 - mean_squared_logarithmic_error: 2.5180e-05 - val_loss: 1.2789e-04 - val_mean_squared_logarithmic_error: 1.2789e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00339: Learning rate is 0.0001.\n",
      "Epoch 340/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.5184e-05 - mean_squared_logarithmic_error: 2.5184e-05The average loss for epoch 339 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5224e-05 - mean_squared_logarithmic_error: 2.5224e-05 - val_loss: 1.2758e-04 - val_mean_squared_logarithmic_error: 1.2758e-04\n",
      "\n",
      "Epoch 00340: Learning rate is 0.0001.\n",
      "Epoch 341/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.5169e-05 - mean_squared_logarithmic_error: 2.5169e-05The average loss for epoch 340 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 2.5176e-05 - mean_squared_logarithmic_error: 2.5176e-05 - val_loss: 1.2684e-04 - val_mean_squared_logarithmic_error: 1.2684e-04\n",
      "\n",
      "Epoch 00341: Learning rate is 0.0001.\n",
      "Epoch 342/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.5318e-05 - mean_squared_logarithmic_error: 2.5318e-05The average loss for epoch 341 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5234e-05 - mean_squared_logarithmic_error: 2.5234e-05 - val_loss: 1.2825e-04 - val_mean_squared_logarithmic_error: 1.2825e-04\n",
      "\n",
      "Epoch 00342: Learning rate is 0.0001.\n",
      "Epoch 343/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.5193e-05 - mean_squared_logarithmic_error: 2.5193e-05The average loss for epoch 342 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 871us/step - loss: 2.5214e-05 - mean_squared_logarithmic_error: 2.5214e-05 - val_loss: 1.2793e-04 - val_mean_squared_logarithmic_error: 1.2793e-04\n",
      "\n",
      "Epoch 00343: Learning rate is 0.0001.\n",
      "Epoch 344/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5269e-05 - mean_squared_logarithmic_error: 2.5269e-05The average loss for epoch 343 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 2.5236e-05 - mean_squared_logarithmic_error: 2.5236e-05 - val_loss: 1.2809e-04 - val_mean_squared_logarithmic_error: 1.2809e-04\n",
      "\n",
      "Epoch 00344: Learning rate is 0.0001.\n",
      "Epoch 345/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5187e-05 - mean_squared_logarithmic_error: 2.5187e-05The average loss for epoch 344 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5160e-05 - mean_squared_logarithmic_error: 2.5160e-05 - val_loss: 1.2824e-04 - val_mean_squared_logarithmic_error: 1.2824e-04\n",
      "\n",
      "Epoch 00345: Learning rate is 0.0001.\n",
      "Epoch 346/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.5050e-05 - mean_squared_logarithmic_error: 2.5050e-05The average loss for epoch 345 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 877us/step - loss: 2.5092e-05 - mean_squared_logarithmic_error: 2.5092e-05 - val_loss: 1.2800e-04 - val_mean_squared_logarithmic_error: 1.2800e-04\n",
      "\n",
      "Epoch 00346: Learning rate is 0.0001.\n",
      "Epoch 347/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5110e-05 - mean_squared_logarithmic_error: 2.5110e-05The average loss for epoch 346 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5226e-05 - mean_squared_logarithmic_error: 2.5226e-05 - val_loss: 1.2699e-04 - val_mean_squared_logarithmic_error: 1.2699e-04\n",
      "\n",
      "Epoch 00347: Learning rate is 0.0001.\n",
      "Epoch 348/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.5093e-05 - mean_squared_logarithmic_error: 2.5093e-05The average loss for epoch 347 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 879us/step - loss: 2.5141e-05 - mean_squared_logarithmic_error: 2.5141e-05 - val_loss: 1.2740e-04 - val_mean_squared_logarithmic_error: 1.2740e-04\n",
      "\n",
      "Epoch 00348: Learning rate is 0.0001.\n",
      "Epoch 349/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.5108e-05 - mean_squared_logarithmic_error: 2.5108e-05The average loss for epoch 348 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 2.5165e-05 - mean_squared_logarithmic_error: 2.5165e-05 - val_loss: 1.2723e-04 - val_mean_squared_logarithmic_error: 1.2723e-04\n",
      "\n",
      "Epoch 00349: Learning rate is 0.0001.\n",
      "Epoch 350/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.5030e-05 - mean_squared_logarithmic_error: 2.5030e-05The average loss for epoch 349 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.5114e-05 - mean_squared_logarithmic_error: 2.5114e-05 - val_loss: 1.2780e-04 - val_mean_squared_logarithmic_error: 1.2780e-04\n",
      "\n",
      "Epoch 00350: Learning rate is 0.0001.\n",
      "Epoch 351/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.5136e-05 - mean_squared_logarithmic_error: 2.5136e-05The average loss for epoch 350 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 882us/step - loss: 2.5136e-05 - mean_squared_logarithmic_error: 2.5136e-05 - val_loss: 1.2757e-04 - val_mean_squared_logarithmic_error: 1.2757e-04\n",
      "\n",
      "Epoch 00351: Learning rate is 0.0001.\n",
      "Epoch 352/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.5204e-05 - mean_squared_logarithmic_error: 2.5204e-05The average loss for epoch 351 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 883us/step - loss: 2.5167e-05 - mean_squared_logarithmic_error: 2.5167e-05 - val_loss: 1.2835e-04 - val_mean_squared_logarithmic_error: 1.2835e-04\n",
      "\n",
      "Epoch 00352: Learning rate is 0.0001.\n",
      "Epoch 353/800\n",
      "606/653 [==========================>...] - ETA: 0s - loss: 2.4967e-05 - mean_squared_logarithmic_error: 2.4967e-05The average loss for epoch 352 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 924us/step - loss: 2.5103e-05 - mean_squared_logarithmic_error: 2.5103e-05 - val_loss: 1.2796e-04 - val_mean_squared_logarithmic_error: 1.2796e-04\n",
      "\n",
      "Epoch 00353: Learning rate is 0.0001.\n",
      "Epoch 354/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.5119e-05 - mean_squared_logarithmic_error: 2.5119e-05The average loss for epoch 353 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 915us/step - loss: 2.5108e-05 - mean_squared_logarithmic_error: 2.5108e-05 - val_loss: 1.2821e-04 - val_mean_squared_logarithmic_error: 1.2821e-04\n",
      "\n",
      "Epoch 00354: Learning rate is 0.0001.\n",
      "Epoch 355/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.5056e-05 - mean_squared_logarithmic_error: 2.5056e-05The average loss for epoch 354 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 915us/step - loss: 2.5081e-05 - mean_squared_logarithmic_error: 2.5081e-05 - val_loss: 1.2797e-04 - val_mean_squared_logarithmic_error: 1.2797e-04\n",
      "\n",
      "Epoch 00355: Learning rate is 0.0001.\n",
      "Epoch 356/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.5047e-05 - mean_squared_logarithmic_error: 2.5047e-05The average loss for epoch 355 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 928us/step - loss: 2.5077e-05 - mean_squared_logarithmic_error: 2.5077e-05 - val_loss: 1.2809e-04 - val_mean_squared_logarithmic_error: 1.2809e-04\n",
      "\n",
      "Epoch 00356: Learning rate is 0.0001.\n",
      "Epoch 357/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.5124e-05 - mean_squared_logarithmic_error: 2.5124e-05The average loss for epoch 356 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.5082e-05 - mean_squared_logarithmic_error: 2.5082e-05 - val_loss: 1.2802e-04 - val_mean_squared_logarithmic_error: 1.2802e-04\n",
      "\n",
      "Epoch 00357: Learning rate is 0.0001.\n",
      "Epoch 358/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.5439e-05 - mean_squared_logarithmic_error: 2.5439e-05The average loss for epoch 357 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.5151e-05 - mean_squared_logarithmic_error: 2.5151e-05 - val_loss: 1.2818e-04 - val_mean_squared_logarithmic_error: 1.2818e-04\n",
      "\n",
      "Epoch 00358: Learning rate is 0.0001.\n",
      "Epoch 359/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.5157e-05 - mean_squared_logarithmic_error: 2.5157e-05The average loss for epoch 358 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 953us/step - loss: 2.5141e-05 - mean_squared_logarithmic_error: 2.5141e-05 - val_loss: 1.2897e-04 - val_mean_squared_logarithmic_error: 1.2897e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00359: Learning rate is 0.0001.\n",
      "Epoch 360/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.5056e-05 - mean_squared_logarithmic_error: 2.5056e-05The average loss for epoch 359 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.5072e-05 - mean_squared_logarithmic_error: 2.5072e-05 - val_loss: 1.2738e-04 - val_mean_squared_logarithmic_error: 1.2738e-04\n",
      "\n",
      "Epoch 00360: Learning rate is 0.0001.\n",
      "Epoch 361/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.5012e-05 - mean_squared_logarithmic_error: 2.5012e-05The average loss for epoch 360 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.5050e-05 - mean_squared_logarithmic_error: 2.5050e-05 - val_loss: 1.2958e-04 - val_mean_squared_logarithmic_error: 1.2958e-04\n",
      "\n",
      "Epoch 00361: Learning rate is 0.0001.\n",
      "Epoch 362/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4908e-05 - mean_squared_logarithmic_error: 2.4908e-05The average loss for epoch 361 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.5072e-05 - mean_squared_logarithmic_error: 2.5072e-05 - val_loss: 1.2806e-04 - val_mean_squared_logarithmic_error: 1.2806e-04\n",
      "\n",
      "Epoch 00362: Learning rate is 0.0001.\n",
      "Epoch 363/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.5090e-05 - mean_squared_logarithmic_error: 2.5090e-05The average loss for epoch 362 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 954us/step - loss: 2.5092e-05 - mean_squared_logarithmic_error: 2.5092e-05 - val_loss: 1.2918e-04 - val_mean_squared_logarithmic_error: 1.2918e-04\n",
      "\n",
      "Epoch 00363: Learning rate is 0.0001.\n",
      "Epoch 364/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.5212e-05 - mean_squared_logarithmic_error: 2.5212e-05The average loss for epoch 363 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.5102e-05 - mean_squared_logarithmic_error: 2.5102e-05 - val_loss: 1.2857e-04 - val_mean_squared_logarithmic_error: 1.2857e-04\n",
      "\n",
      "Epoch 00364: Learning rate is 0.0001.\n",
      "Epoch 365/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4932e-05 - mean_squared_logarithmic_error: 2.4932e-05The average loss for epoch 364 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 2.5035e-05 - mean_squared_logarithmic_error: 2.5035e-05 - val_loss: 1.2843e-04 - val_mean_squared_logarithmic_error: 1.2843e-04\n",
      "\n",
      "Epoch 00365: Learning rate is 0.0001.\n",
      "Epoch 366/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.5016e-05 - mean_squared_logarithmic_error: 2.5016e-05The average loss for epoch 365 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 974us/step - loss: 2.5016e-05 - mean_squared_logarithmic_error: 2.5016e-05 - val_loss: 1.2930e-04 - val_mean_squared_logarithmic_error: 1.2930e-04\n",
      "\n",
      "Epoch 00366: Learning rate is 0.0001.\n",
      "Epoch 367/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.5139e-05 - mean_squared_logarithmic_error: 2.5139e-05The average loss for epoch 366 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 2.5133e-05 - mean_squared_logarithmic_error: 2.5133e-05 - val_loss: 1.2839e-04 - val_mean_squared_logarithmic_error: 1.2839e-04\n",
      "\n",
      "Epoch 00367: Learning rate is 0.0001.\n",
      "Epoch 368/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.5031e-05 - mean_squared_logarithmic_error: 2.5031e-05The average loss for epoch 367 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.5026e-05 - mean_squared_logarithmic_error: 2.5026e-05 - val_loss: 1.2865e-04 - val_mean_squared_logarithmic_error: 1.2865e-04\n",
      "\n",
      "Epoch 00368: Learning rate is 0.0001.\n",
      "Epoch 369/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.4860e-05 - mean_squared_logarithmic_error: 2.4860e-05The average loss for epoch 368 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 980us/step - loss: 2.5027e-05 - mean_squared_logarithmic_error: 2.5027e-05 - val_loss: 1.2913e-04 - val_mean_squared_logarithmic_error: 1.2913e-04\n",
      "\n",
      "Epoch 00369: Learning rate is 0.0001.\n",
      "Epoch 370/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.4889e-05 - mean_squared_logarithmic_error: 2.4889e-05The average loss for epoch 369 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 982us/step - loss: 2.5071e-05 - mean_squared_logarithmic_error: 2.5071e-05 - val_loss: 1.2879e-04 - val_mean_squared_logarithmic_error: 1.2879e-04\n",
      "\n",
      "Epoch 00370: Learning rate is 0.0001.\n",
      "Epoch 371/800\n",
      "623/653 [===========================>..] - ETA: 0s - loss: 2.5258e-05 - mean_squared_logarithmic_error: 2.5258e-05The average loss for epoch 370 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 986us/step - loss: 2.5109e-05 - mean_squared_logarithmic_error: 2.5109e-05 - val_loss: 1.2906e-04 - val_mean_squared_logarithmic_error: 1.2906e-04\n",
      "\n",
      "Epoch 00371: Learning rate is 0.0001.\n",
      "Epoch 372/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.5099e-05 - mean_squared_logarithmic_error: 2.5099e-05The average loss for epoch 371 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.5059e-05 - mean_squared_logarithmic_error: 2.5059e-05 - val_loss: 1.2831e-04 - val_mean_squared_logarithmic_error: 1.2831e-04\n",
      "\n",
      "Epoch 00372: Learning rate is 0.0001.\n",
      "Epoch 373/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.4979e-05 - mean_squared_logarithmic_error: 2.4979e-05The average loss for epoch 372 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 992us/step - loss: 2.5044e-05 - mean_squared_logarithmic_error: 2.5044e-05 - val_loss: 1.2826e-04 - val_mean_squared_logarithmic_error: 1.2826e-04\n",
      "\n",
      "Epoch 00373: Learning rate is 0.0001.\n",
      "Epoch 374/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.5051e-05 - mean_squared_logarithmic_error: 2.5051e-05The average loss for epoch 373 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.5040e-05 - mean_squared_logarithmic_error: 2.5040e-05 - val_loss: 1.2922e-04 - val_mean_squared_logarithmic_error: 1.2922e-04\n",
      "\n",
      "Epoch 00374: Learning rate is 0.0001.\n",
      "Epoch 375/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5048e-05 - mean_squared_logarithmic_error: 2.5048e-05The average loss for epoch 374 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 985us/step - loss: 2.5102e-05 - mean_squared_logarithmic_error: 2.5102e-05 - val_loss: 1.2886e-04 - val_mean_squared_logarithmic_error: 1.2886e-04\n",
      "\n",
      "Epoch 00375: Learning rate is 0.0001.\n",
      "Epoch 376/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.5052e-05 - mean_squared_logarithmic_error: 2.5052e-05The average loss for epoch 375 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4965e-05 - mean_squared_logarithmic_error: 2.4965e-05 - val_loss: 1.2924e-04 - val_mean_squared_logarithmic_error: 1.2924e-04\n",
      "\n",
      "Epoch 00376: Learning rate is 0.0001.\n",
      "Epoch 377/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.4985e-05 - mean_squared_logarithmic_error: 2.4985e-05The average loss for epoch 376 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.4997e-05 - mean_squared_logarithmic_error: 2.4997e-05 - val_loss: 1.2894e-04 - val_mean_squared_logarithmic_error: 1.2894e-04\n",
      "\n",
      "Epoch 00377: Learning rate is 0.0001.\n",
      "Epoch 378/800\n",
      "627/653 [===========================>..] - ETA: 0s - loss: 2.5098e-05 - mean_squared_logarithmic_error: 2.5098e-05The average loss for epoch 377 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 985us/step - loss: 2.5031e-05 - mean_squared_logarithmic_error: 2.5031e-05 - val_loss: 1.2896e-04 - val_mean_squared_logarithmic_error: 1.2896e-04\n",
      "\n",
      "Epoch 00378: Learning rate is 0.0001.\n",
      "Epoch 379/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4911e-05 - mean_squared_logarithmic_error: 2.4911e-05The average loss for epoch 378 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 978us/step - loss: 2.4984e-05 - mean_squared_logarithmic_error: 2.4984e-05 - val_loss: 1.2868e-04 - val_mean_squared_logarithmic_error: 1.2868e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00379: Learning rate is 0.0001.\n",
      "Epoch 380/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.5057e-05 - mean_squared_logarithmic_error: 2.5057e-05The average loss for epoch 379 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 970us/step - loss: 2.5024e-05 - mean_squared_logarithmic_error: 2.5024e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00380: Learning rate is 0.0001.\n",
      "Epoch 381/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.5142e-05 - mean_squared_logarithmic_error: 2.5142e-05The average loss for epoch 380 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.5046e-05 - mean_squared_logarithmic_error: 2.5046e-05 - val_loss: 1.2898e-04 - val_mean_squared_logarithmic_error: 1.2898e-04\n",
      "\n",
      "Epoch 00381: Learning rate is 0.0001.\n",
      "Epoch 382/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4967e-05 - mean_squared_logarithmic_error: 2.4967e-05The average loss for epoch 381 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 970us/step - loss: 2.4957e-05 - mean_squared_logarithmic_error: 2.4957e-05 - val_loss: 1.3005e-04 - val_mean_squared_logarithmic_error: 1.3005e-04\n",
      "\n",
      "Epoch 00382: Learning rate is 0.0001.\n",
      "Epoch 383/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.4961e-05 - mean_squared_logarithmic_error: 2.4961e-05The average loss for epoch 382 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.5040e-05 - mean_squared_logarithmic_error: 2.5040e-05 - val_loss: 1.2857e-04 - val_mean_squared_logarithmic_error: 1.2857e-04\n",
      "\n",
      "Epoch 00383: Learning rate is 0.0001.\n",
      "Epoch 384/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.5009e-05 - mean_squared_logarithmic_error: 2.5009e-05The average loss for epoch 383 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.4967e-05 - mean_squared_logarithmic_error: 2.4967e-05 - val_loss: 1.2894e-04 - val_mean_squared_logarithmic_error: 1.2894e-04\n",
      "\n",
      "Epoch 00384: Learning rate is 0.0001.\n",
      "Epoch 385/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.4950e-05 - mean_squared_logarithmic_error: 2.4950e-05The average loss for epoch 384 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 998us/step - loss: 2.4988e-05 - mean_squared_logarithmic_error: 2.4988e-05 - val_loss: 1.2913e-04 - val_mean_squared_logarithmic_error: 1.2913e-04\n",
      "\n",
      "Epoch 00385: Learning rate is 0.0001.\n",
      "Epoch 386/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.4989e-05 - mean_squared_logarithmic_error: 2.4989e-05The average loss for epoch 385 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.4970e-05 - mean_squared_logarithmic_error: 2.4970e-05 - val_loss: 1.2923e-04 - val_mean_squared_logarithmic_error: 1.2923e-04\n",
      "\n",
      "Epoch 00386: Learning rate is 0.0001.\n",
      "Epoch 387/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.5090e-05 - mean_squared_logarithmic_error: 2.5090e-05The average loss for epoch 386 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5004e-05 - mean_squared_logarithmic_error: 2.5004e-05 - val_loss: 1.2895e-04 - val_mean_squared_logarithmic_error: 1.2895e-04\n",
      "\n",
      "Epoch 00387: Learning rate is 0.0001.\n",
      "Epoch 388/800\n",
      "605/653 [==========================>...] - ETA: 0s - loss: 2.4885e-05 - mean_squared_logarithmic_error: 2.4885e-05The average loss for epoch 387 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5020e-05 - mean_squared_logarithmic_error: 2.5020e-05 - val_loss: 1.2946e-04 - val_mean_squared_logarithmic_error: 1.2946e-04\n",
      "\n",
      "Epoch 00388: Learning rate is 0.0001.\n",
      "Epoch 389/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4930e-05 - mean_squared_logarithmic_error: 2.4930e-05The average loss for epoch 388 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4929e-05 - mean_squared_logarithmic_error: 2.4929e-05 - val_loss: 1.2843e-04 - val_mean_squared_logarithmic_error: 1.2843e-04\n",
      "\n",
      "Epoch 00389: Learning rate is 0.0001.\n",
      "Epoch 390/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.4967e-05 - mean_squared_logarithmic_error: 2.4967e-05The average loss for epoch 389 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5017e-05 - mean_squared_logarithmic_error: 2.5017e-05 - val_loss: 1.2884e-04 - val_mean_squared_logarithmic_error: 1.2884e-04\n",
      "\n",
      "Epoch 00390: Learning rate is 0.0001.\n",
      "Epoch 391/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.5046e-05 - mean_squared_logarithmic_error: 2.5046e-05The average loss for epoch 390 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5048e-05 - mean_squared_logarithmic_error: 2.5048e-05 - val_loss: 1.2957e-04 - val_mean_squared_logarithmic_error: 1.2957e-04\n",
      "\n",
      "Epoch 00391: Learning rate is 0.0001.\n",
      "Epoch 392/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4885e-05 - mean_squared_logarithmic_error: 2.4885e-05The average loss for epoch 391 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5039e-05 - mean_squared_logarithmic_error: 2.5039e-05 - val_loss: 1.2926e-04 - val_mean_squared_logarithmic_error: 1.2926e-04\n",
      "\n",
      "Epoch 00392: Learning rate is 0.0001.\n",
      "Epoch 393/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.4945e-05 - mean_squared_logarithmic_error: 2.4945e-05The average loss for epoch 392 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4945e-05 - mean_squared_logarithmic_error: 2.4945e-05 - val_loss: 1.2837e-04 - val_mean_squared_logarithmic_error: 1.2837e-04\n",
      "\n",
      "Epoch 00393: Learning rate is 0.0001.\n",
      "Epoch 394/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.5150e-05 - mean_squared_logarithmic_error: 2.5150e-05The average loss for epoch 393 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4972e-05 - mean_squared_logarithmic_error: 2.4972e-05 - val_loss: 1.2982e-04 - val_mean_squared_logarithmic_error: 1.2982e-04\n",
      "\n",
      "Epoch 00394: Learning rate is 0.0001.\n",
      "Epoch 395/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.4874e-05 - mean_squared_logarithmic_error: 2.4874e-05The average loss for epoch 394 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5005e-05 - mean_squared_logarithmic_error: 2.5005e-05 - val_loss: 1.2990e-04 - val_mean_squared_logarithmic_error: 1.2990e-04\n",
      "\n",
      "Epoch 00395: Learning rate is 0.0001.\n",
      "Epoch 396/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 2.4941e-05 - mean_squared_logarithmic_error: 2.4941e-05The average loss for epoch 395 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4919e-05 - mean_squared_logarithmic_error: 2.4919e-05 - val_loss: 1.3020e-04 - val_mean_squared_logarithmic_error: 1.3020e-04\n",
      "\n",
      "Epoch 00396: Learning rate is 0.0001.\n",
      "Epoch 397/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4760e-05 - mean_squared_logarithmic_error: 2.4760e-05The average loss for epoch 396 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4965e-05 - mean_squared_logarithmic_error: 2.4965e-05 - val_loss: 1.2929e-04 - val_mean_squared_logarithmic_error: 1.2929e-04\n",
      "\n",
      "Epoch 00397: Learning rate is 0.0001.\n",
      "Epoch 398/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.5122e-05 - mean_squared_logarithmic_error: 2.5122e-05The average loss for epoch 397 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4932e-05 - mean_squared_logarithmic_error: 2.4932e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00398: Learning rate is 0.0001.\n",
      "Epoch 399/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.5086e-05 - mean_squared_logarithmic_error: 2.5086e-05The average loss for epoch 398 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.5022e-05 - mean_squared_logarithmic_error: 2.5022e-05 - val_loss: 1.2958e-04 - val_mean_squared_logarithmic_error: 1.2958e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00399: Learning rate is 0.0001.\n",
      "Epoch 400/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.4922e-05 - mean_squared_logarithmic_error: 2.4922e-05The average loss for epoch 399 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4927e-05 - mean_squared_logarithmic_error: 2.4927e-05 - val_loss: 1.3000e-04 - val_mean_squared_logarithmic_error: 1.3000e-04\n",
      "\n",
      "Epoch 00400: Learning rate is 0.0000.\n",
      "Epoch 401/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4332e-05 - mean_squared_logarithmic_error: 2.4332e-05The average loss for epoch 400 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4274e-05 - mean_squared_logarithmic_error: 2.4274e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00401: Learning rate is 0.0000.\n",
      "Epoch 402/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.4222e-05 - mean_squared_logarithmic_error: 2.4222e-05The average loss for epoch 401 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4192e-05 - mean_squared_logarithmic_error: 2.4192e-05 - val_loss: 1.2945e-04 - val_mean_squared_logarithmic_error: 1.2945e-04\n",
      "\n",
      "Epoch 00402: Learning rate is 0.0000.\n",
      "Epoch 403/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4172e-05 - mean_squared_logarithmic_error: 2.4172e-05The average loss for epoch 402 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4184e-05 - mean_squared_logarithmic_error: 2.4184e-05 - val_loss: 1.2940e-04 - val_mean_squared_logarithmic_error: 1.2940e-04\n",
      "\n",
      "Epoch 00403: Learning rate is 0.0000.\n",
      "Epoch 404/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4161e-05 - mean_squared_logarithmic_error: 2.4161e-05The average loss for epoch 403 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4170e-05 - mean_squared_logarithmic_error: 2.4170e-05 - val_loss: 1.2933e-04 - val_mean_squared_logarithmic_error: 1.2933e-04\n",
      "\n",
      "Epoch 00404: Learning rate is 0.0000.\n",
      "Epoch 405/800\n",
      "605/653 [==========================>...] - ETA: 0s - loss: 2.4080e-05 - mean_squared_logarithmic_error: 2.4080e-05The average loss for epoch 404 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4167e-05 - mean_squared_logarithmic_error: 2.4167e-05 - val_loss: 1.2936e-04 - val_mean_squared_logarithmic_error: 1.2936e-04\n",
      "\n",
      "Epoch 00405: Learning rate is 0.0000.\n",
      "Epoch 406/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4028e-05 - mean_squared_logarithmic_error: 2.4028e-05The average loss for epoch 405 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4149e-05 - mean_squared_logarithmic_error: 2.4149e-05 - val_loss: 1.2941e-04 - val_mean_squared_logarithmic_error: 1.2941e-04\n",
      "\n",
      "Epoch 00406: Learning rate is 0.0000.\n",
      "Epoch 407/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4239e-05 - mean_squared_logarithmic_error: 2.4239e-05The average loss for epoch 406 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4153e-05 - mean_squared_logarithmic_error: 2.4153e-05 - val_loss: 1.2940e-04 - val_mean_squared_logarithmic_error: 1.2940e-04\n",
      "\n",
      "Epoch 00407: Learning rate is 0.0000.\n",
      "Epoch 408/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.4208e-05 - mean_squared_logarithmic_error: 2.4208e-05The average loss for epoch 407 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4152e-05 - mean_squared_logarithmic_error: 2.4152e-05 - val_loss: 1.2930e-04 - val_mean_squared_logarithmic_error: 1.2930e-04\n",
      "\n",
      "Epoch 00408: Learning rate is 0.0000.\n",
      "Epoch 409/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.3889e-05 - mean_squared_logarithmic_error: 2.3889e-05The average loss for epoch 408 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4142e-05 - mean_squared_logarithmic_error: 2.4142e-05 - val_loss: 1.2938e-04 - val_mean_squared_logarithmic_error: 1.2938e-04\n",
      "\n",
      "Epoch 00409: Learning rate is 0.0000.\n",
      "Epoch 410/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05The average loss for epoch 409 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4140e-05 - mean_squared_logarithmic_error: 2.4140e-05 - val_loss: 1.2945e-04 - val_mean_squared_logarithmic_error: 1.2945e-04\n",
      "\n",
      "Epoch 00410: Learning rate is 0.0000.\n",
      "Epoch 411/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.4071e-05 - mean_squared_logarithmic_error: 2.4071e-05The average loss for epoch 410 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4137e-05 - mean_squared_logarithmic_error: 2.4137e-05 - val_loss: 1.2939e-04 - val_mean_squared_logarithmic_error: 1.2939e-04\n",
      "\n",
      "Epoch 00411: Learning rate is 0.0000.\n",
      "Epoch 412/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.4192e-05 - mean_squared_logarithmic_error: 2.4192e-05The average loss for epoch 411 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4138e-05 - mean_squared_logarithmic_error: 2.4138e-05 - val_loss: 1.2944e-04 - val_mean_squared_logarithmic_error: 1.2944e-04\n",
      "\n",
      "Epoch 00412: Learning rate is 0.0000.\n",
      "Epoch 413/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4201e-05 - mean_squared_logarithmic_error: 2.4201e-05The average loss for epoch 412 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4141e-05 - mean_squared_logarithmic_error: 2.4141e-05 - val_loss: 1.2934e-04 - val_mean_squared_logarithmic_error: 1.2934e-04\n",
      "\n",
      "Epoch 00413: Learning rate is 0.0000.\n",
      "Epoch 414/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4080e-05 - mean_squared_logarithmic_error: 2.4080e-05The average loss for epoch 413 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05 - val_loss: 1.2936e-04 - val_mean_squared_logarithmic_error: 1.2936e-04\n",
      "\n",
      "Epoch 00414: Learning rate is 0.0000.\n",
      "Epoch 415/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4281e-05 - mean_squared_logarithmic_error: 2.4281e-05The average loss for epoch 414 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4131e-05 - mean_squared_logarithmic_error: 2.4131e-05 - val_loss: 1.2938e-04 - val_mean_squared_logarithmic_error: 1.2938e-04\n",
      "\n",
      "Epoch 00415: Learning rate is 0.0000.\n",
      "Epoch 416/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4126e-05 - mean_squared_logarithmic_error: 2.4126e-05The average loss for epoch 415 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4132e-05 - mean_squared_logarithmic_error: 2.4132e-05 - val_loss: 1.2929e-04 - val_mean_squared_logarithmic_error: 1.2929e-04\n",
      "\n",
      "Epoch 00416: Learning rate is 0.0000.\n",
      "Epoch 417/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.4113e-05 - mean_squared_logarithmic_error: 2.4113e-05The average loss for epoch 416 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05 - val_loss: 1.2940e-04 - val_mean_squared_logarithmic_error: 1.2940e-04\n",
      "\n",
      "Epoch 00417: Learning rate is 0.0000.\n",
      "Epoch 418/800\n",
      "606/653 [==========================>...] - ETA: 0s - loss: 2.4326e-05 - mean_squared_logarithmic_error: 2.4326e-05The average loss for epoch 417 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4128e-05 - mean_squared_logarithmic_error: 2.4128e-05 - val_loss: 1.2957e-04 - val_mean_squared_logarithmic_error: 1.2957e-04\n",
      "\n",
      "Epoch 00418: Learning rate is 0.0000.\n",
      "Epoch 419/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4117e-05 - mean_squared_logarithmic_error: 2.4117e-05The average loss for epoch 418 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4131e-05 - mean_squared_logarithmic_error: 2.4131e-05 - val_loss: 1.2946e-04 - val_mean_squared_logarithmic_error: 1.2946e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00419: Learning rate is 0.0000.\n",
      "Epoch 420/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.3928e-05 - mean_squared_logarithmic_error: 2.3928e-05The average loss for epoch 419 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4131e-05 - mean_squared_logarithmic_error: 2.4131e-05 - val_loss: 1.2948e-04 - val_mean_squared_logarithmic_error: 1.2948e-04\n",
      "\n",
      "Epoch 00420: Learning rate is 0.0000.\n",
      "Epoch 421/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05The average loss for epoch 420 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4123e-05 - mean_squared_logarithmic_error: 2.4123e-05 - val_loss: 1.2940e-04 - val_mean_squared_logarithmic_error: 1.2940e-04\n",
      "\n",
      "Epoch 00421: Learning rate is 0.0000.\n",
      "Epoch 422/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.4143e-05 - mean_squared_logarithmic_error: 2.4143e-05The average loss for epoch 421 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05 - val_loss: 1.2947e-04 - val_mean_squared_logarithmic_error: 1.2947e-04\n",
      "\n",
      "Epoch 00422: Learning rate is 0.0000.\n",
      "Epoch 423/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 422 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4117e-05 - mean_squared_logarithmic_error: 2.4117e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00423: Learning rate is 0.0000.\n",
      "Epoch 424/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.4230e-05 - mean_squared_logarithmic_error: 2.4230e-05The average loss for epoch 423 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4120e-05 - mean_squared_logarithmic_error: 2.4120e-05 - val_loss: 1.2954e-04 - val_mean_squared_logarithmic_error: 1.2954e-04\n",
      "\n",
      "Epoch 00424: Learning rate is 0.0000.\n",
      "Epoch 425/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4183e-05 - mean_squared_logarithmic_error: 2.4183e-05The average loss for epoch 424 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4118e-05 - mean_squared_logarithmic_error: 2.4118e-05 - val_loss: 1.2958e-04 - val_mean_squared_logarithmic_error: 1.2958e-04\n",
      "\n",
      "Epoch 00425: Learning rate is 0.0000.\n",
      "Epoch 426/800\n",
      "618/653 [===========================>..] - ETA: 0s - loss: 2.4067e-05 - mean_squared_logarithmic_error: 2.4067e-05The average loss for epoch 425 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4123e-05 - mean_squared_logarithmic_error: 2.4123e-05 - val_loss: 1.2942e-04 - val_mean_squared_logarithmic_error: 1.2942e-04\n",
      "\n",
      "Epoch 00426: Learning rate is 0.0000.\n",
      "Epoch 427/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4008e-05 - mean_squared_logarithmic_error: 2.4008e-05The average loss for epoch 426 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4117e-05 - mean_squared_logarithmic_error: 2.4117e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00427: Learning rate is 0.0000.\n",
      "Epoch 428/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4150e-05 - mean_squared_logarithmic_error: 2.4150e-05The average loss for epoch 427 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4114e-05 - mean_squared_logarithmic_error: 2.4114e-05 - val_loss: 1.2939e-04 - val_mean_squared_logarithmic_error: 1.2939e-04\n",
      "\n",
      "Epoch 00428: Learning rate is 0.0000.\n",
      "Epoch 429/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4150e-05 - mean_squared_logarithmic_error: 2.4150e-05The average loss for epoch 428 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4121e-05 - mean_squared_logarithmic_error: 2.4121e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00429: Learning rate is 0.0000.\n",
      "Epoch 430/800\n",
      "606/653 [==========================>...] - ETA: 0s - loss: 2.4352e-05 - mean_squared_logarithmic_error: 2.4352e-05The average loss for epoch 429 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4118e-05 - mean_squared_logarithmic_error: 2.4118e-05 - val_loss: 1.2940e-04 - val_mean_squared_logarithmic_error: 1.2940e-04\n",
      "\n",
      "Epoch 00430: Learning rate is 0.0000.\n",
      "Epoch 431/800\n",
      "606/653 [==========================>...] - ETA: 0s - loss: 2.4136e-05 - mean_squared_logarithmic_error: 2.4136e-05The average loss for epoch 430 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4120e-05 - mean_squared_logarithmic_error: 2.4120e-05 - val_loss: 1.2939e-04 - val_mean_squared_logarithmic_error: 1.2939e-04\n",
      "\n",
      "Epoch 00431: Learning rate is 0.0000.\n",
      "Epoch 432/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.4123e-05 - mean_squared_logarithmic_error: 2.4123e-05The average loss for epoch 431 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00432: Learning rate is 0.0000.\n",
      "Epoch 433/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4132e-05 - mean_squared_logarithmic_error: 2.4132e-05The average loss for epoch 432 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4114e-05 - mean_squared_logarithmic_error: 2.4114e-05 - val_loss: 1.2955e-04 - val_mean_squared_logarithmic_error: 1.2955e-04\n",
      "\n",
      "Epoch 00433: Learning rate is 0.0000.\n",
      "Epoch 434/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.4019e-05 - mean_squared_logarithmic_error: 2.4019e-05The average loss for epoch 433 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4112e-05 - mean_squared_logarithmic_error: 2.4112e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00434: Learning rate is 0.0000.\n",
      "Epoch 435/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4235e-05 - mean_squared_logarithmic_error: 2.4235e-05The average loss for epoch 434 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4109e-05 - mean_squared_logarithmic_error: 2.4109e-05 - val_loss: 1.2946e-04 - val_mean_squared_logarithmic_error: 1.2946e-04\n",
      "\n",
      "Epoch 00435: Learning rate is 0.0000.\n",
      "Epoch 436/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.3954e-05 - mean_squared_logarithmic_error: 2.3954e-05The average loss for epoch 435 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4111e-05 - mean_squared_logarithmic_error: 2.4111e-05 - val_loss: 1.2952e-04 - val_mean_squared_logarithmic_error: 1.2952e-04\n",
      "\n",
      "Epoch 00436: Learning rate is 0.0000.\n",
      "Epoch 437/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.4147e-05 - mean_squared_logarithmic_error: 2.4147e-05The average loss for epoch 436 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4108e-05 - mean_squared_logarithmic_error: 2.4108e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00437: Learning rate is 0.0000.\n",
      "Epoch 438/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.3971e-05 - mean_squared_logarithmic_error: 2.3971e-05The average loss for epoch 437 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4115e-05 - mean_squared_logarithmic_error: 2.4115e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00438: Learning rate is 0.0000.\n",
      "Epoch 439/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.4164e-05 - mean_squared_logarithmic_error: 2.4164e-05The average loss for epoch 438 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4115e-05 - mean_squared_logarithmic_error: 2.4115e-05 - val_loss: 1.2958e-04 - val_mean_squared_logarithmic_error: 1.2958e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00439: Learning rate is 0.0000.\n",
      "Epoch 440/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4149e-05 - mean_squared_logarithmic_error: 2.4149e-05The average loss for epoch 439 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05 - val_loss: 1.2943e-04 - val_mean_squared_logarithmic_error: 1.2943e-04\n",
      "\n",
      "Epoch 00440: Learning rate is 0.0000.\n",
      "Epoch 441/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4075e-05 - mean_squared_logarithmic_error: 2.4075e-05The average loss for epoch 440 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4105e-05 - mean_squared_logarithmic_error: 2.4105e-05 - val_loss: 1.2943e-04 - val_mean_squared_logarithmic_error: 1.2943e-04\n",
      "\n",
      "Epoch 00441: Learning rate is 0.0000.\n",
      "Epoch 442/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4088e-05 - mean_squared_logarithmic_error: 2.4088e-05The average loss for epoch 441 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4104e-05 - mean_squared_logarithmic_error: 2.4104e-05 - val_loss: 1.2957e-04 - val_mean_squared_logarithmic_error: 1.2957e-04\n",
      "\n",
      "Epoch 00442: Learning rate is 0.0000.\n",
      "Epoch 443/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.3947e-05 - mean_squared_logarithmic_error: 2.3947e-05The average loss for epoch 442 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00443: Learning rate is 0.0000.\n",
      "Epoch 444/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.3739e-05 - mean_squared_logarithmic_error: 2.3739e-05The average loss for epoch 443 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4111e-05 - mean_squared_logarithmic_error: 2.4111e-05 - val_loss: 1.2964e-04 - val_mean_squared_logarithmic_error: 1.2964e-04\n",
      "\n",
      "Epoch 00444: Learning rate is 0.0000.\n",
      "Epoch 445/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05The average loss for epoch 444 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4101e-05 - mean_squared_logarithmic_error: 2.4101e-05 - val_loss: 1.2949e-04 - val_mean_squared_logarithmic_error: 1.2949e-04\n",
      "\n",
      "Epoch 00445: Learning rate is 0.0000.\n",
      "Epoch 446/800\n",
      "605/653 [==========================>...] - ETA: 0s - loss: 2.4056e-05 - mean_squared_logarithmic_error: 2.4056e-05The average loss for epoch 445 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4091e-05 - mean_squared_logarithmic_error: 2.4091e-05 - val_loss: 1.2960e-04 - val_mean_squared_logarithmic_error: 1.2960e-04\n",
      "\n",
      "Epoch 00446: Learning rate is 0.0000.\n",
      "Epoch 447/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3965e-05 - mean_squared_logarithmic_error: 2.3965e-05The average loss for epoch 446 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05 - val_loss: 1.2953e-04 - val_mean_squared_logarithmic_error: 1.2953e-04\n",
      "\n",
      "Epoch 00447: Learning rate is 0.0000.\n",
      "Epoch 448/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.4116e-05 - mean_squared_logarithmic_error: 2.4116e-05The average loss for epoch 447 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4104e-05 - mean_squared_logarithmic_error: 2.4104e-05 - val_loss: 1.2954e-04 - val_mean_squared_logarithmic_error: 1.2954e-04\n",
      "\n",
      "Epoch 00448: Learning rate is 0.0000.\n",
      "Epoch 449/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.4094e-05 - mean_squared_logarithmic_error: 2.4094e-05The average loss for epoch 448 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4097e-05 - mean_squared_logarithmic_error: 2.4097e-05 - val_loss: 1.2954e-04 - val_mean_squared_logarithmic_error: 1.2954e-04\n",
      "\n",
      "Epoch 00449: Learning rate is 0.0000.\n",
      "Epoch 450/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.4105e-05 - mean_squared_logarithmic_error: 2.4105e-05The average loss for epoch 449 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05 - val_loss: 1.2956e-04 - val_mean_squared_logarithmic_error: 1.2956e-04\n",
      "\n",
      "Epoch 00450: Learning rate is 0.0000.\n",
      "Epoch 451/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.4100e-05 - mean_squared_logarithmic_error: 2.4100e-05The average loss for epoch 450 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4096e-05 - mean_squared_logarithmic_error: 2.4096e-05 - val_loss: 1.2963e-04 - val_mean_squared_logarithmic_error: 1.2963e-04\n",
      "\n",
      "Epoch 00451: Learning rate is 0.0000.\n",
      "Epoch 452/800\n",
      "606/653 [==========================>...] - ETA: 0s - loss: 2.4334e-05 - mean_squared_logarithmic_error: 2.4334e-05The average loss for epoch 451 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4108e-05 - mean_squared_logarithmic_error: 2.4108e-05 - val_loss: 1.2952e-04 - val_mean_squared_logarithmic_error: 1.2952e-04\n",
      "\n",
      "Epoch 00452: Learning rate is 0.0000.\n",
      "Epoch 453/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.4110e-05 - mean_squared_logarithmic_error: 2.4110e-05The average loss for epoch 452 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05 - val_loss: 1.2950e-04 - val_mean_squared_logarithmic_error: 1.2950e-04\n",
      "\n",
      "Epoch 00453: Learning rate is 0.0000.\n",
      "Epoch 454/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4074e-05 - mean_squared_logarithmic_error: 2.4074e-05The average loss for epoch 453 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05 - val_loss: 1.2969e-04 - val_mean_squared_logarithmic_error: 1.2969e-04\n",
      "\n",
      "Epoch 00454: Learning rate is 0.0000.\n",
      "Epoch 455/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05The average loss for epoch 454 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05 - val_loss: 1.2952e-04 - val_mean_squared_logarithmic_error: 1.2952e-04\n",
      "\n",
      "Epoch 00455: Learning rate is 0.0000.\n",
      "Epoch 456/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.4192e-05 - mean_squared_logarithmic_error: 2.4192e-05The average loss for epoch 455 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4101e-05 - mean_squared_logarithmic_error: 2.4101e-05 - val_loss: 1.2964e-04 - val_mean_squared_logarithmic_error: 1.2964e-04\n",
      "\n",
      "Epoch 00456: Learning rate is 0.0000.\n",
      "Epoch 457/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.4174e-05 - mean_squared_logarithmic_error: 2.4174e-05The average loss for epoch 456 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4106e-05 - mean_squared_logarithmic_error: 2.4106e-05 - val_loss: 1.2952e-04 - val_mean_squared_logarithmic_error: 1.2952e-04\n",
      "\n",
      "Epoch 00457: Learning rate is 0.0000.\n",
      "Epoch 458/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05The average loss for epoch 457 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4116e-05 - mean_squared_logarithmic_error: 2.4116e-05 - val_loss: 1.2962e-04 - val_mean_squared_logarithmic_error: 1.2962e-04\n",
      "\n",
      "Epoch 00458: Learning rate is 0.0000.\n",
      "Epoch 459/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.4230e-05 - mean_squared_logarithmic_error: 2.4230e-05The average loss for epoch 458 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05 - val_loss: 1.2966e-04 - val_mean_squared_logarithmic_error: 1.2966e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00459: Learning rate is 0.0000.\n",
      "Epoch 460/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.4133e-05 - mean_squared_logarithmic_error: 2.4133e-05The average loss for epoch 459 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4105e-05 - mean_squared_logarithmic_error: 2.4105e-05 - val_loss: 1.2945e-04 - val_mean_squared_logarithmic_error: 1.2945e-04\n",
      "\n",
      "Epoch 00460: Learning rate is 0.0000.\n",
      "Epoch 461/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.4078e-05 - mean_squared_logarithmic_error: 2.4078e-05The average loss for epoch 460 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4104e-05 - mean_squared_logarithmic_error: 2.4104e-05 - val_loss: 1.2962e-04 - val_mean_squared_logarithmic_error: 1.2962e-04\n",
      "\n",
      "Epoch 00461: Learning rate is 0.0000.\n",
      "Epoch 462/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4261e-05 - mean_squared_logarithmic_error: 2.4261e-05The average loss for epoch 461 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4084e-05 - mean_squared_logarithmic_error: 2.4084e-05 - val_loss: 1.2966e-04 - val_mean_squared_logarithmic_error: 1.2966e-04\n",
      "\n",
      "Epoch 00462: Learning rate is 0.0000.\n",
      "Epoch 463/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4051e-05 - mean_squared_logarithmic_error: 2.4051e-05The average loss for epoch 462 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4099e-05 - mean_squared_logarithmic_error: 2.4099e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00463: Learning rate is 0.0000.\n",
      "Epoch 464/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4002e-05 - mean_squared_logarithmic_error: 2.4002e-05The average loss for epoch 463 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4087e-05 - mean_squared_logarithmic_error: 2.4087e-05 - val_loss: 1.2964e-04 - val_mean_squared_logarithmic_error: 1.2964e-04\n",
      "\n",
      "Epoch 00464: Learning rate is 0.0000.\n",
      "Epoch 465/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4182e-05 - mean_squared_logarithmic_error: 2.4182e-05The average loss for epoch 464 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4106e-05 - mean_squared_logarithmic_error: 2.4106e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00465: Learning rate is 0.0000.\n",
      "Epoch 466/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.4133e-05 - mean_squared_logarithmic_error: 2.4133e-05 ETA: 0s - loss: 2.4207e-05 - mean_squared_logarithmic_error: The average loss for epoch 465 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4104e-05 - mean_squared_logarithmic_error: 2.4104e-05 - val_loss: 1.2950e-04 - val_mean_squared_logarithmic_error: 1.2950e-04\n",
      "\n",
      "Epoch 00466: Learning rate is 0.0000.\n",
      "Epoch 467/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4177e-05 - mean_squared_logarithmic_error: 2.4177e-05The average loss for epoch 466 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4102e-05 - mean_squared_logarithmic_error: 2.4102e-05 - val_loss: 1.2959e-04 - val_mean_squared_logarithmic_error: 1.2959e-04\n",
      "\n",
      "Epoch 00467: Learning rate is 0.0000.\n",
      "Epoch 468/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05The average loss for epoch 467 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4092e-05 - mean_squared_logarithmic_error: 2.4092e-05 - val_loss: 1.2948e-04 - val_mean_squared_logarithmic_error: 1.2948e-04\n",
      "\n",
      "Epoch 00468: Learning rate is 0.0000.\n",
      "Epoch 469/800\n",
      "605/653 [==========================>...] - ETA: 0s - loss: 2.4105e-05 - mean_squared_logarithmic_error: 2.4104e-05The average loss for epoch 468 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4097e-05 - mean_squared_logarithmic_error: 2.4097e-05 - val_loss: 1.2969e-04 - val_mean_squared_logarithmic_error: 1.2969e-04\n",
      "\n",
      "Epoch 00469: Learning rate is 0.0000.\n",
      "Epoch 470/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.4127e-05 - mean_squared_logarithmic_error: 2.4127e-05The average loss for epoch 469 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4093e-05 - mean_squared_logarithmic_error: 2.4093e-05 - val_loss: 1.2951e-04 - val_mean_squared_logarithmic_error: 1.2951e-04\n",
      "\n",
      "Epoch 00470: Learning rate is 0.0000.\n",
      "Epoch 471/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4168e-05 - mean_squared_logarithmic_error: 2.4168e-05The average loss for epoch 470 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4094e-05 - mean_squared_logarithmic_error: 2.4094e-05 - val_loss: 1.2971e-04 - val_mean_squared_logarithmic_error: 1.2971e-04\n",
      "\n",
      "Epoch 00471: Learning rate is 0.0000.\n",
      "Epoch 472/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.4007e-05 - mean_squared_logarithmic_error: 2.4007e-05The average loss for epoch 471 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4085e-05 - mean_squared_logarithmic_error: 2.4085e-05 - val_loss: 1.2950e-04 - val_mean_squared_logarithmic_error: 1.2950e-04\n",
      "\n",
      "Epoch 00472: Learning rate is 0.0000.\n",
      "Epoch 473/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4111e-05 - mean_squared_logarithmic_error: 2.4111e-05The average loss for epoch 472 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4085e-05 - mean_squared_logarithmic_error: 2.4085e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00473: Learning rate is 0.0000.\n",
      "Epoch 474/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.4114e-05 - mean_squared_logarithmic_error: 2.4114e-05The average loss for epoch 473 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4085e-05 - mean_squared_logarithmic_error: 2.4085e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00474: Learning rate is 0.0000.\n",
      "Epoch 475/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.4011e-05 - mean_squared_logarithmic_error: 2.4011e-05The average loss for epoch 474 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4092e-05 - mean_squared_logarithmic_error: 2.4092e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00475: Learning rate is 0.0000.\n",
      "Epoch 476/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4118e-05 - mean_squared_logarithmic_error: 2.4118e-05The average loss for epoch 475 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4083e-05 - mean_squared_logarithmic_error: 2.4083e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00476: Learning rate is 0.0000.\n",
      "Epoch 477/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.4093e-05 - mean_squared_logarithmic_error: 2.4093e-05The average loss for epoch 476 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4093e-05 - mean_squared_logarithmic_error: 2.4093e-05 - val_loss: 1.3001e-04 - val_mean_squared_logarithmic_error: 1.3001e-04\n",
      "\n",
      "Epoch 00477: Learning rate is 0.0000.\n",
      "Epoch 478/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.4186e-05 - mean_squared_logarithmic_error: 2.4186e-05The average loss for epoch 477 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4089e-05 - mean_squared_logarithmic_error: 2.4089e-05 - val_loss: 1.2960e-04 - val_mean_squared_logarithmic_error: 1.2960e-04\n",
      "\n",
      "Epoch 00478: Learning rate is 0.0000.\n",
      "Epoch 479/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/653 [==========================>...] - ETA: 0s - loss: 2.4223e-05 - mean_squared_logarithmic_error: 2.4223e-05The average loss for epoch 478 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4087e-05 - mean_squared_logarithmic_error: 2.4087e-05 - val_loss: 1.2962e-04 - val_mean_squared_logarithmic_error: 1.2962e-04\n",
      "\n",
      "Epoch 00479: Learning rate is 0.0000.\n",
      "Epoch 480/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4218e-05 - mean_squared_logarithmic_error: 2.4218e-05The average loss for epoch 479 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4081e-05 - mean_squared_logarithmic_error: 2.4081e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00480: Learning rate is 0.0000.\n",
      "Epoch 481/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4064e-05 - mean_squared_logarithmic_error: 2.4064e-05The average loss for epoch 480 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4088e-05 - mean_squared_logarithmic_error: 2.4088e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00481: Learning rate is 0.0000.\n",
      "Epoch 482/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.4131e-05 - mean_squared_logarithmic_error: 2.4131e-05The average loss for epoch 481 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05 - val_loss: 1.2962e-04 - val_mean_squared_logarithmic_error: 1.2962e-04\n",
      "\n",
      "Epoch 00482: Learning rate is 0.0000.\n",
      "Epoch 483/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.4220e-05 - mean_squared_logarithmic_error: 2.4220e-05The average loss for epoch 482 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4078e-05 - mean_squared_logarithmic_error: 2.4078e-05 - val_loss: 1.2965e-04 - val_mean_squared_logarithmic_error: 1.2965e-04\n",
      "\n",
      "Epoch 00483: Learning rate is 0.0000.\n",
      "Epoch 484/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05The average loss for epoch 483 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4091e-05 - mean_squared_logarithmic_error: 2.4091e-05 - val_loss: 1.2964e-04 - val_mean_squared_logarithmic_error: 1.2964e-04\n",
      "\n",
      "Epoch 00484: Learning rate is 0.0000.\n",
      "Epoch 485/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4059e-05 - mean_squared_logarithmic_error: 2.4059e-05The average loss for epoch 484 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4083e-05 - mean_squared_logarithmic_error: 2.4083e-05 - val_loss: 1.2969e-04 - val_mean_squared_logarithmic_error: 1.2969e-04\n",
      "\n",
      "Epoch 00485: Learning rate is 0.0000.\n",
      "Epoch 486/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.4106e-05 - mean_squared_logarithmic_error: 2.4106e-05The average loss for epoch 485 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05 - val_loss: 1.2970e-04 - val_mean_squared_logarithmic_error: 1.2970e-04\n",
      "\n",
      "Epoch 00486: Learning rate is 0.0000.\n",
      "Epoch 487/800\n",
      "608/653 [==========================>...] - ETA: 0s - loss: 2.3914e-05 - mean_squared_logarithmic_error: 2.3914e-05The average loss for epoch 486 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05 - val_loss: 1.2965e-04 - val_mean_squared_logarithmic_error: 1.2965e-04\n",
      "\n",
      "Epoch 00487: Learning rate is 0.0000.\n",
      "Epoch 488/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.4100e-05 - mean_squared_logarithmic_error: 2.4100e-05The average loss for epoch 487 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4083e-05 - mean_squared_logarithmic_error: 2.4083e-05 - val_loss: 1.2970e-04 - val_mean_squared_logarithmic_error: 1.2970e-04\n",
      "\n",
      "Epoch 00488: Learning rate is 0.0000.\n",
      "Epoch 489/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4227e-05 - mean_squared_logarithmic_error: 2.4227e-05The average loss for epoch 488 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4090e-05 - mean_squared_logarithmic_error: 2.4090e-05 - val_loss: 1.2952e-04 - val_mean_squared_logarithmic_error: 1.2952e-04\n",
      "\n",
      "Epoch 00489: Learning rate is 0.0000.\n",
      "Epoch 490/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4201e-05 - mean_squared_logarithmic_error: 2.4201e-05The average loss for epoch 489 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4086e-05 - mean_squared_logarithmic_error: 2.4086e-05 - val_loss: 1.2961e-04 - val_mean_squared_logarithmic_error: 1.2961e-04\n",
      "\n",
      "Epoch 00490: Learning rate is 0.0000.\n",
      "Epoch 491/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4198e-05 - mean_squared_logarithmic_error: 2.4198e-05The average loss for epoch 490 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4083e-05 - mean_squared_logarithmic_error: 2.4083e-05 - val_loss: 1.2979e-04 - val_mean_squared_logarithmic_error: 1.2979e-04\n",
      "\n",
      "Epoch 00491: Learning rate is 0.0000.\n",
      "Epoch 492/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 491 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4073e-05 - mean_squared_logarithmic_error: 2.4073e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00492: Learning rate is 0.0000.\n",
      "Epoch 493/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.3974e-05 - mean_squared_logarithmic_error: 2.3974e-05The average loss for epoch 492 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4082e-05 - mean_squared_logarithmic_error: 2.4082e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00493: Learning rate is 0.0000.\n",
      "Epoch 494/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.4081e-05 - mean_squared_logarithmic_error: 2.4081e-05The average loss for epoch 493 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4085e-05 - mean_squared_logarithmic_error: 2.4085e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00494: Learning rate is 0.0000.\n",
      "Epoch 495/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.4076e-05 - mean_squared_logarithmic_error: 2.4076e-05The average loss for epoch 494 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4083e-05 - mean_squared_logarithmic_error: 2.4083e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00495: Learning rate is 0.0000.\n",
      "Epoch 496/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.4072e-05 - mean_squared_logarithmic_error: 2.4072e-05The average loss for epoch 495 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4072e-05 - mean_squared_logarithmic_error: 2.4072e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00496: Learning rate is 0.0000.\n",
      "Epoch 497/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4181e-05 - mean_squared_logarithmic_error: 2.4181e-05The average loss for epoch 496 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4089e-05 - mean_squared_logarithmic_error: 2.4089e-05 - val_loss: 1.2966e-04 - val_mean_squared_logarithmic_error: 1.2966e-04\n",
      "\n",
      "Epoch 00497: Learning rate is 0.0000.\n",
      "Epoch 498/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4088e-05 - mean_squared_logarithmic_error: 2.4088e-05The average loss for epoch 497 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4088e-05 - mean_squared_logarithmic_error: 2.4088e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00498: Learning rate is 0.0000.\n",
      "Epoch 499/800\n",
      "615/653 [===========================>..] - ETA: 0s - loss: 2.4168e-05 - mean_squared_logarithmic_error: 2.4168e-05The average loss for epoch 498 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4094e-05 - mean_squared_logarithmic_error: 2.4094e-05 - val_loss: 1.2985e-04 - val_mean_squared_logarithmic_error: 1.2985e-04\n",
      "\n",
      "Epoch 00499: Learning rate is 0.0000.\n",
      "Epoch 500/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.4135e-05 - mean_squared_logarithmic_error: 2.4135e-05The average loss for epoch 499 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.4082e-05 - mean_squared_logarithmic_error: 2.4082e-05 - val_loss: 1.2969e-04 - val_mean_squared_logarithmic_error: 1.2969e-04\n",
      "\n",
      "Epoch 00500: Learning rate is 0.0000.\n",
      "Epoch 501/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.4171e-05 - mean_squared_logarithmic_error: 2.4171e-05The average loss for epoch 500 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1000us/step - loss: 2.4001e-05 - mean_squared_logarithmic_error: 2.4001e-05 - val_loss: 1.2967e-04 - val_mean_squared_logarithmic_error: 1.2967e-04\n",
      "\n",
      "Epoch 00501: Learning rate is 0.0000.\n",
      "Epoch 502/800\n",
      "612/653 [===========================>..] - ETA: 0s - loss: 2.4002e-05 - mean_squared_logarithmic_error: 2.4002e-05The average loss for epoch 501 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3995e-05 - mean_squared_logarithmic_error: 2.3995e-05 - val_loss: 1.2969e-04 - val_mean_squared_logarithmic_error: 1.2969e-04\n",
      "\n",
      "Epoch 00502: Learning rate is 0.0000.\n",
      "Epoch 503/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4017e-05 - mean_squared_logarithmic_error: 2.4017e-05The average loss for epoch 502 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3991e-05 - mean_squared_logarithmic_error: 2.3991e-05 - val_loss: 1.2970e-04 - val_mean_squared_logarithmic_error: 1.2970e-04\n",
      "\n",
      "Epoch 00503: Learning rate is 0.0000.\n",
      "Epoch 504/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4192e-05 - mean_squared_logarithmic_error: 2.4192e-05The average loss for epoch 503 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3992e-05 - mean_squared_logarithmic_error: 2.3992e-05 - val_loss: 1.2970e-04 - val_mean_squared_logarithmic_error: 1.2970e-04\n",
      "\n",
      "Epoch 00504: Learning rate is 0.0000.\n",
      "Epoch 505/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05The average loss for epoch 504 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 982us/step - loss: 2.3990e-05 - mean_squared_logarithmic_error: 2.3990e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00505: Learning rate is 0.0000.\n",
      "Epoch 506/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4007e-05 - mean_squared_logarithmic_error: 2.4007e-05The average loss for epoch 505 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 978us/step - loss: 2.3991e-05 - mean_squared_logarithmic_error: 2.3991e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00506: Learning rate is 0.0000.\n",
      "Epoch 507/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.4081e-05 - mean_squared_logarithmic_error: 2.4081e-05The average loss for epoch 506 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3992e-05 - mean_squared_logarithmic_error: 2.3992e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00507: Learning rate is 0.0000.\n",
      "Epoch 508/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05The average loss for epoch 507 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 978us/step - loss: 2.3991e-05 - mean_squared_logarithmic_error: 2.3991e-05 - val_loss: 1.2971e-04 - val_mean_squared_logarithmic_error: 1.2971e-04\n",
      "\n",
      "Epoch 00508: Learning rate is 0.0000.\n",
      "Epoch 509/800\n",
      "625/653 [===========================>..] - ETA: 0s - loss: 2.4004e-05 - mean_squared_logarithmic_error: 2.4004e-05The average loss for epoch 508 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 989us/step - loss: 2.3990e-05 - mean_squared_logarithmic_error: 2.3990e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00509: Learning rate is 0.0000.\n",
      "Epoch 510/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.3912e-05 - mean_squared_logarithmic_error: 2.3912e-05The average loss for epoch 509 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 975us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00510: Learning rate is 0.0000.\n",
      "Epoch 511/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4063e-05 - mean_squared_logarithmic_error: 2.4063e-05The average loss for epoch 510 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3991e-05 - mean_squared_logarithmic_error: 2.3991e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00511: Learning rate is 0.0000.\n",
      "Epoch 512/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.4045e-05 - mean_squared_logarithmic_error: 2.4045e-05The average loss for epoch 511 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 993us/step - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00512: Learning rate is 0.0000.\n",
      "Epoch 513/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.3831e-05 - mean_squared_logarithmic_error: 2.3831e-05The average loss for epoch 512 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00513: Learning rate is 0.0000.\n",
      "Epoch 514/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.4002e-05 - mean_squared_logarithmic_error: 2.4002e-05The average loss for epoch 513 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 990us/step - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00514: Learning rate is 0.0000.\n",
      "Epoch 515/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.4010e-05 - mean_squared_logarithmic_error: 2.4010e-05The average loss for epoch 514 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 974us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00515: Learning rate is 0.0000.\n",
      "Epoch 516/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.4003e-05 - mean_squared_logarithmic_error: 2.4003e-05The average loss for epoch 515 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00516: Learning rate is 0.0000.\n",
      "Epoch 517/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 516 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00517: Learning rate is 0.0000.\n",
      "Epoch 518/800\n",
      "609/653 [==========================>...] - ETA: 0s - loss: 2.4082e-05 - mean_squared_logarithmic_error: 2.4082e-05The average loss for epoch 517 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00518: Learning rate is 0.0000.\n",
      "Epoch 519/800\n",
      "618/653 [===========================>..] - ETA: 0s - loss: 2.3957e-05 - mean_squared_logarithmic_error: 2.3957e-05The average loss for epoch 518 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 995us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00519: Learning rate is 0.0000.\n",
      "Epoch 520/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4055e-05 - mean_squared_logarithmic_error: 2.4055e-05The average loss for epoch 519 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 980us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00520: Learning rate is 0.0000.\n",
      "Epoch 521/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4062e-05 - mean_squared_logarithmic_error: 2.4062e-05The average loss for epoch 520 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 967us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00521: Learning rate is 0.0000.\n",
      "Epoch 522/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.3859e-05 - mean_squared_logarithmic_error: 2.3859e-05The average loss for epoch 521 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 969us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00522: Learning rate is 0.0000.\n",
      "Epoch 523/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.4066e-05 - mean_squared_logarithmic_error: 2.4066e-05The average loss for epoch 522 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 967us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2971e-04 - val_mean_squared_logarithmic_error: 1.2971e-04\n",
      "\n",
      "Epoch 00523: Learning rate is 0.0000.\n",
      "Epoch 524/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3943e-05 - mean_squared_logarithmic_error: 2.3943e-05The average loss for epoch 523 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 974us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00524: Learning rate is 0.0000.\n",
      "Epoch 525/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.4054e-05 - mean_squared_logarithmic_error: 2.4054e-05The average loss for epoch 524 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 971us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00525: Learning rate is 0.0000.\n",
      "Epoch 526/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 525 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00526: Learning rate is 0.0000.\n",
      "Epoch 527/800\n",
      "611/653 [===========================>..] - ETA: 0s - loss: 2.4000e-05 - mean_squared_logarithmic_error: 2.4000e-05The average loss for epoch 526 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00527: Learning rate is 0.0000.\n",
      "Epoch 528/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.3921e-05 - mean_squared_logarithmic_error: 2.3921e-05The average loss for epoch 527 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 963us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00528: Learning rate is 0.0000.\n",
      "Epoch 529/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 528 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00529: Learning rate is 0.0000.\n",
      "Epoch 530/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.3998e-05 - mean_squared_logarithmic_error: 2.3998e-05The average loss for epoch 529 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2972e-04 - val_mean_squared_logarithmic_error: 1.2972e-04\n",
      "\n",
      "Epoch 00530: Learning rate is 0.0000.\n",
      "Epoch 531/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.4018e-05 - mean_squared_logarithmic_error: 2.4018e-05The average loss for epoch 530 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00531: Learning rate is 0.0000.\n",
      "Epoch 532/800\n",
      "614/653 [===========================>..] - ETA: 0s - loss: 2.4163e-05 - mean_squared_logarithmic_error: 2.4163e-05The average loss for epoch 531 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1000us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00532: Learning rate is 0.0000.\n",
      "Epoch 533/800\n",
      "637/653 [============================>.] - ETA: 0s - loss: 2.3998e-05 - mean_squared_logarithmic_error: 2.3998e-05The average loss for epoch 532 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 970us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00533: Learning rate is 0.0000.\n",
      "Epoch 534/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4021e-05 - mean_squared_logarithmic_error: 2.4021e-05The average loss for epoch 533 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00534: Learning rate is 0.0000.\n",
      "Epoch 535/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 2.3906e-05 - mean_squared_logarithmic_error: 2.3906e-05The average loss for epoch 534 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 989us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00535: Learning rate is 0.0000.\n",
      "Epoch 536/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.3907e-05 - mean_squared_logarithmic_error: 2.3907e-05The average loss for epoch 535 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 979us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00536: Learning rate is 0.0000.\n",
      "Epoch 537/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3854e-05 - mean_squared_logarithmic_error: 2.3854e-05The average loss for epoch 536 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 975us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00537: Learning rate is 0.0000.\n",
      "Epoch 538/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3871e-05 - mean_squared_logarithmic_error: 2.3871e-05The average loss for epoch 537 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 973us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00538: Learning rate is 0.0000.\n",
      "Epoch 539/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.3981e-05 - mean_squared_logarithmic_error: 2.3981e-05The average loss for epoch 538 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 967us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00539: Learning rate is 0.0000.\n",
      "Epoch 540/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.3966e-05 - mean_squared_logarithmic_error: 2.3966e-05The average loss for epoch 539 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 967us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00540: Learning rate is 0.0000.\n",
      "Epoch 541/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.3929e-05 - mean_squared_logarithmic_error: 2.3929e-05The average loss for epoch 540 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 970us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00541: Learning rate is 0.0000.\n",
      "Epoch 542/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.3970e-05 - mean_squared_logarithmic_error: 2.3970e-05The average loss for epoch 541 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 966us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00542: Learning rate is 0.0000.\n",
      "Epoch 543/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.3787e-05 - mean_squared_logarithmic_error: 2.3787e-05The average loss for epoch 542 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 998us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00543: Learning rate is 0.0000.\n",
      "Epoch 544/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.4050e-05 - mean_squared_logarithmic_error: 2.4050e-05The average loss for epoch 543 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 981us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00544: Learning rate is 0.0000.\n",
      "Epoch 545/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4130e-05 - mean_squared_logarithmic_error: 2.4130e-05The average loss for epoch 544 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.3988e-05 - mean_squared_logarithmic_error: 2.3988e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00545: Learning rate is 0.0000.\n",
      "Epoch 546/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.4176e-05 - mean_squared_logarithmic_error: 2.4176e-05The average loss for epoch 545 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 928us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00546: Learning rate is 0.0000.\n",
      "Epoch 547/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 ETA: 0s - loss: 2.4749e-05 - mean_squared_logarithmic_errThe average loss for epoch 546 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 946us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00547: Learning rate is 0.0000.\n",
      "Epoch 548/800\n",
      "605/653 [==========================>...] - ETA: 0s - loss: 2.3956e-05 - mean_squared_logarithmic_error: 2.3956e-05The average loss for epoch 547 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00548: Learning rate is 0.0000.\n",
      "Epoch 549/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05The average loss for epoch 548 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00549: Learning rate is 0.0000.\n",
      "Epoch 550/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3995e-05 - mean_squared_logarithmic_error: 2.3995e-05The average loss for epoch 549 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00550: Learning rate is 0.0000.\n",
      "Epoch 551/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3995e-05 - mean_squared_logarithmic_error: 2.3995e-05The average loss for epoch 550 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00551: Learning rate is 0.0000.\n",
      "Epoch 552/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.3953e-05 - mean_squared_logarithmic_error: 2.3953e-05The average loss for epoch 551 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00552: Learning rate is 0.0000.\n",
      "Epoch 553/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3994e-05 - mean_squared_logarithmic_error: 2.3994e-05The average loss for epoch 552 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00553: Learning rate is 0.0000.\n",
      "Epoch 554/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3865e-05 - mean_squared_logarithmic_error: 2.3865e-05The average loss for epoch 553 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00554: Learning rate is 0.0000.\n",
      "Epoch 555/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4010e-05 - mean_squared_logarithmic_error: 2.4010e-05The average loss for epoch 554 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00555: Learning rate is 0.0000.\n",
      "Epoch 556/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4078e-05 - mean_squared_logarithmic_error: 2.4078e-05The average loss for epoch 555 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00556: Learning rate is 0.0000.\n",
      "Epoch 557/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4038e-05 - mean_squared_logarithmic_error: 2.4038e-05The average loss for epoch 556 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00557: Learning rate is 0.0000.\n",
      "Epoch 558/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629/653 [===========================>..] - ETA: 0s - loss: 2.4025e-05 - mean_squared_logarithmic_error: 2.4025e-05The average loss for epoch 557 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00558: Learning rate is 0.0000.\n",
      "Epoch 559/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.4003e-05 - mean_squared_logarithmic_error: 2.4003e-05The average loss for epoch 558 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00559: Learning rate is 0.0000.\n",
      "Epoch 560/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4099e-05 - mean_squared_logarithmic_error: 2.4099e-05The average loss for epoch 559 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00560: Learning rate is 0.0000.\n",
      "Epoch 561/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.4032e-05 - mean_squared_logarithmic_error: 2.4032e-05The average loss for epoch 560 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 960us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00561: Learning rate is 0.0000.\n",
      "Epoch 562/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.4013e-05 - mean_squared_logarithmic_error: 2.4013e-05The average loss for epoch 561 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 986us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00562: Learning rate is 0.0000.\n",
      "Epoch 563/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.4033e-05 - mean_squared_logarithmic_error: 2.4033e-05The average loss for epoch 562 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 962us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2980e-04 - val_mean_squared_logarithmic_error: 1.2980e-04\n",
      "\n",
      "Epoch 00563: Learning rate is 0.0000.\n",
      "Epoch 564/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.4005e-05 - mean_squared_logarithmic_error: 2.4005e-05The average loss for epoch 563 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 980us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00564: Learning rate is 0.0000.\n",
      "Epoch 565/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.3978e-05 - mean_squared_logarithmic_error: 2.3978e-05The average loss for epoch 564 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 973us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00565: Learning rate is 0.0000.\n",
      "Epoch 566/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.3994e-05 - mean_squared_logarithmic_error: 2.3994e-05The average loss for epoch 565 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 975us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00566: Learning rate is 0.0000.\n",
      "Epoch 567/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.4135e-05 - mean_squared_logarithmic_error: 2.4135e-05The average loss for epoch 566 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 982us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00567: Learning rate is 0.0000.\n",
      "Epoch 568/800\n",
      "610/653 [===========================>..] - ETA: 0s - loss: 2.3959e-05 - mean_squared_logarithmic_error: 2.3959e-05The average loss for epoch 567 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00568: Learning rate is 0.0000.\n",
      "Epoch 569/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.3762e-05 - mean_squared_logarithmic_error: 2.3762e-05The average loss for epoch 568 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 983us/step - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00569: Learning rate is 0.0000.\n",
      "Epoch 570/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.3862e-05 - mean_squared_logarithmic_error: 2.3862e-05The average loss for epoch 569 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2973e-04 - val_mean_squared_logarithmic_error: 1.2973e-04\n",
      "\n",
      "Epoch 00570: Learning rate is 0.0000.\n",
      "Epoch 571/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 570 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 942us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00571: Learning rate is 0.0000.\n",
      "Epoch 572/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3959e-05 - mean_squared_logarithmic_error: 2.3959e-05The average loss for epoch 571 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00572: Learning rate is 0.0000.\n",
      "Epoch 573/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4122e-05 - mean_squared_logarithmic_error: 2.4122e-05The average loss for epoch 572 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00573: Learning rate is 0.0000.\n",
      "Epoch 574/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3939e-05 - mean_squared_logarithmic_error: 2.3939e-05The average loss for epoch 573 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 942us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00574: Learning rate is 0.0000.\n",
      "Epoch 575/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.3958e-05 - mean_squared_logarithmic_error: 2.3958e-05The average loss for epoch 574 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00575: Learning rate is 0.0000.\n",
      "Epoch 576/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4096e-05 - mean_squared_logarithmic_error: 2.4096e-05The average loss for epoch 575 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 942us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00576: Learning rate is 0.0000.\n",
      "Epoch 577/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3851e-05 - mean_squared_logarithmic_error: 2.3851e-05The average loss for epoch 576 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00577: Learning rate is 0.0000.\n",
      "Epoch 578/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4228e-05 - mean_squared_logarithmic_error: 2.4228e-05The average loss for epoch 577 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00578: Learning rate is 0.0000.\n",
      "Epoch 579/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3997e-05 - mean_squared_logarithmic_error: 2.3997e-05The average loss for epoch 578 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 960us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00579: Learning rate is 0.0000.\n",
      "Epoch 580/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4124e-05 - mean_squared_logarithmic_error: 2.4124e-05The average loss for epoch 579 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00580: Learning rate is 0.0000.\n",
      "Epoch 581/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3880e-05 - mean_squared_logarithmic_error: 2.3880e-05The average loss for epoch 580 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00581: Learning rate is 0.0000.\n",
      "Epoch 582/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3940e-05 - mean_squared_logarithmic_error: 2.3940e-05The average loss for epoch 581 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00582: Learning rate is 0.0000.\n",
      "Epoch 583/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.4010e-05 - mean_squared_logarithmic_error: 2.4010e-05The average loss for epoch 582 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00583: Learning rate is 0.0000.\n",
      "Epoch 584/800\n",
      "628/653 [===========================>..] - ETA: 0s - loss: 2.3884e-05 - mean_squared_logarithmic_error: 2.3884e-05The average loss for epoch 583 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00584: Learning rate is 0.0000.\n",
      "Epoch 585/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3978e-05 - mean_squared_logarithmic_error: 2.3978e-05The average loss for epoch 584 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00585: Learning rate is 0.0000.\n",
      "Epoch 586/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4071e-05 - mean_squared_logarithmic_error: 2.4071e-05The average loss for epoch 585 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00586: Learning rate is 0.0000.\n",
      "Epoch 587/800\n",
      "593/653 [==========================>...] - ETA: 0s - loss: 2.4109e-05 - mean_squared_logarithmic_error: 2.4109e-05The average loss for epoch 586 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00587: Learning rate is 0.0000.\n",
      "Epoch 588/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3846e-05 - mean_squared_logarithmic_error: 2.3846e-05The average loss for epoch 587 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00588: Learning rate is 0.0000.\n",
      "Epoch 589/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05The average loss for epoch 588 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 943us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2974e-04 - val_mean_squared_logarithmic_error: 1.2974e-04\n",
      "\n",
      "Epoch 00589: Learning rate is 0.0000.\n",
      "Epoch 590/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4191e-05 - mean_squared_logarithmic_error: 2.4191e-05The average loss for epoch 589 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00590: Learning rate is 0.0000.\n",
      "Epoch 591/800\n",
      "639/653 [============================>.] - ETA: 0s - loss: 2.4084e-05 - mean_squared_logarithmic_error: 2.4084e-05The average loss for epoch 590 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 964us/step - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00591: Learning rate is 0.0000.\n",
      "Epoch 592/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.3996e-05 - mean_squared_logarithmic_error: 2.3996e-05The average loss for epoch 591 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00592: Learning rate is 0.0000.\n",
      "Epoch 593/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05The average loss for epoch 592 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 944us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2975e-04 - val_mean_squared_logarithmic_error: 1.2975e-04\n",
      "\n",
      "Epoch 00593: Learning rate is 0.0000.\n",
      "Epoch 594/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3977e-05 - mean_squared_logarithmic_error: 2.3977e-05The average loss for epoch 593 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3981e-05 - mean_squared_logarithmic_error: 2.3981e-05 - val_loss: 1.2980e-04 - val_mean_squared_logarithmic_error: 1.2980e-04\n",
      "\n",
      "Epoch 00594: Learning rate is 0.0000.\n",
      "Epoch 595/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3818e-05 - mean_squared_logarithmic_error: 2.3818e-05The average loss for epoch 594 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00595: Learning rate is 0.0000.\n",
      "Epoch 596/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 2.3936e-05 - mean_squared_logarithmic_error: 2.3936e-05The average loss for epoch 595 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 997us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00596: Learning rate is 0.0000.\n",
      "Epoch 597/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3981e-05 - mean_squared_logarithmic_error: 2.3981e-05The average loss for epoch 596 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 954us/step - loss: 2.3983e-05 - mean_squared_logarithmic_error: 2.3983e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00597: Learning rate is 0.0000.\n",
      "Epoch 598/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.3960e-05 - mean_squared_logarithmic_error: 2.3960e-05The average loss for epoch 597 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00598: Learning rate is 0.0000.\n",
      "Epoch 599/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4085e-05 - mean_squared_logarithmic_error: 2.4085e-05The average loss for epoch 598 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00599: Learning rate is 0.0000.\n",
      "Epoch 600/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05The average loss for epoch 599 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00600: Learning rate is 0.0000.\n",
      "Epoch 601/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05The average loss for epoch 600 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2976e-04 - val_mean_squared_logarithmic_error: 1.2976e-04\n",
      "\n",
      "Epoch 00601: Learning rate is 0.0000.\n",
      "Epoch 602/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.3956e-05 - mean_squared_logarithmic_error: 2.3956e-05The average loss for epoch 601 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00602: Learning rate is 0.0000.\n",
      "Epoch 603/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3791e-05 - mean_squared_logarithmic_error: 2.3791e-05The average loss for epoch 602 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00603: Learning rate is 0.0000.\n",
      "Epoch 604/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 603 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 999us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00604: Learning rate is 0.0000.\n",
      "Epoch 605/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.3756e-05 - mean_squared_logarithmic_error: 2.3756e-05The average loss for epoch 604 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00605: Learning rate is 0.0000.\n",
      "Epoch 606/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.3942e-05 - mean_squared_logarithmic_error: 2.3942e-05The average loss for epoch 605 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 987us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00606: Learning rate is 0.0000.\n",
      "Epoch 607/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.3977e-05 - mean_squared_logarithmic_error: 2.3977e-05The average loss for epoch 606 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 956us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00607: Learning rate is 0.0000.\n",
      "Epoch 608/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 607 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 944us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00608: Learning rate is 0.0000.\n",
      "Epoch 609/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4000e-05 - mean_squared_logarithmic_error: 2.4000e-05The average loss for epoch 608 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00609: Learning rate is 0.0000.\n",
      "Epoch 610/800\n",
      "593/653 [==========================>...] - ETA: 0s - loss: 2.3891e-05 - mean_squared_logarithmic_error: 2.3891e-05The average loss for epoch 609 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00610: Learning rate is 0.0000.\n",
      "Epoch 611/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4016e-05 - mean_squared_logarithmic_error: 2.4016e-05The average loss for epoch 610 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00611: Learning rate is 0.0000.\n",
      "Epoch 612/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.4009e-05 - mean_squared_logarithmic_error: 2.4009e-05The average loss for epoch 611 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00612: Learning rate is 0.0000.\n",
      "Epoch 613/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.4056e-05 - mean_squared_logarithmic_error: 2.4056e-05The average loss for epoch 612 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 961us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00613: Learning rate is 0.0000.\n",
      "Epoch 614/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.4017e-05 - mean_squared_logarithmic_error: 2.4017e-05The average loss for epoch 613 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00614: Learning rate is 0.0000.\n",
      "Epoch 615/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.3937e-05 - mean_squared_logarithmic_error: 2.3937e-05The average loss for epoch 614 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00615: Learning rate is 0.0000.\n",
      "Epoch 616/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 615 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00616: Learning rate is 0.0000.\n",
      "Epoch 617/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3970e-05 - mean_squared_logarithmic_error: 2.3970e-05The average loss for epoch 616 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00617: Learning rate is 0.0000.\n",
      "Epoch 618/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3920e-05 - mean_squared_logarithmic_error: 2.3920e-05The average loss for epoch 617 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00618: Learning rate is 0.0000.\n",
      "Epoch 619/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3938e-05 - mean_squared_logarithmic_error: 2.3938e-05The average loss for epoch 618 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00619: Learning rate is 0.0000.\n",
      "Epoch 620/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4009e-05 - mean_squared_logarithmic_error: 2.4009e-05The average loss for epoch 619 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00620: Learning rate is 0.0000.\n",
      "Epoch 621/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4067e-05 - mean_squared_logarithmic_error: 2.4067e-05The average loss for epoch 620 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00621: Learning rate is 0.0000.\n",
      "Epoch 622/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4286e-05 - mean_squared_logarithmic_error: 2.4286e-05The average loss for epoch 621 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00622: Learning rate is 0.0000.\n",
      "Epoch 623/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.3838e-05 - mean_squared_logarithmic_error: 2.3838e-05The average loss for epoch 622 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00623: Learning rate is 0.0000.\n",
      "Epoch 624/800\n",
      "649/653 [============================>.] - ETA: 0s - loss: 2.3994e-05 - mean_squared_logarithmic_error: 2.3994e-05The average loss for epoch 623 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00624: Learning rate is 0.0000.\n",
      "Epoch 625/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3723e-05 - mean_squared_logarithmic_error: 2.3723e-05The average loss for epoch 624 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00625: Learning rate is 0.0000.\n",
      "Epoch 626/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3888e-05 - mean_squared_logarithmic_error: 2.3888e-05The average loss for epoch 625 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00626: Learning rate is 0.0000.\n",
      "Epoch 627/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.3846e-05 - mean_squared_logarithmic_error: 2.3846e-05The average loss for epoch 626 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00627: Learning rate is 0.0000.\n",
      "Epoch 628/800\n",
      "624/653 [===========================>..] - ETA: 0s - loss: 2.4071e-05 - mean_squared_logarithmic_error: 2.4071e-05The average loss for epoch 627 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 980us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00628: Learning rate is 0.0000.\n",
      "Epoch 629/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3919e-05 - mean_squared_logarithmic_error: 2.3919e-05The average loss for epoch 628 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00629: Learning rate is 0.0000.\n",
      "Epoch 630/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3907e-05 - mean_squared_logarithmic_error: 2.3907e-05The average loss for epoch 629 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00630: Learning rate is 0.0000.\n",
      "Epoch 631/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.3936e-05 - mean_squared_logarithmic_error: 2.3936e-05The average loss for epoch 630 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 965us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00631: Learning rate is 0.0000.\n",
      "Epoch 632/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 631 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00632: Learning rate is 0.0000.\n",
      "Epoch 633/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.3992e-05 - mean_squared_logarithmic_error: 2.3992e-05The average loss for epoch 632 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 965us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00633: Learning rate is 0.0000.\n",
      "Epoch 634/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3971e-05 - mean_squared_logarithmic_error: 2.3971e-05The average loss for epoch 633 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00634: Learning rate is 0.0000.\n",
      "Epoch 635/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3977e-05 - mean_squared_logarithmic_error: 2.3977e-05The average loss for epoch 634 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 947us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00635: Learning rate is 0.0000.\n",
      "Epoch 636/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3975e-05 - mean_squared_logarithmic_error: 2.3975e-05The average loss for epoch 635 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00636: Learning rate is 0.0000.\n",
      "Epoch 637/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05The average loss for epoch 636 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 958us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00637: Learning rate is 0.0000.\n",
      "Epoch 638/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 637 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00638: Learning rate is 0.0000.\n",
      "Epoch 639/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3964e-05 - mean_squared_logarithmic_error: 2.3964e-05The average loss for epoch 638 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 944us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00639: Learning rate is 0.0000.\n",
      "Epoch 640/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3917e-05 - mean_squared_logarithmic_error: 2.3917e-05The average loss for epoch 639 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00640: Learning rate is 0.0000.\n",
      "Epoch 641/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3944e-05 - mean_squared_logarithmic_error: 2.3944e-05The average loss for epoch 640 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00641: Learning rate is 0.0000.\n",
      "Epoch 642/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4078e-05 - mean_squared_logarithmic_error: 2.4078e-05The average loss for epoch 641 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00642: Learning rate is 0.0000.\n",
      "Epoch 643/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.3916e-05 - mean_squared_logarithmic_error: 2.3916e-05The average loss for epoch 642 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00643: Learning rate is 0.0000.\n",
      "Epoch 644/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3963e-05 - mean_squared_logarithmic_error: 2.3963e-05The average loss for epoch 643 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00644: Learning rate is 0.0000.\n",
      "Epoch 645/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 644 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00645: Learning rate is 0.0000.\n",
      "Epoch 646/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 645 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00646: Learning rate is 0.0000.\n",
      "Epoch 647/800\n",
      "623/653 [===========================>..] - ETA: 0s - loss: 2.3971e-05 - mean_squared_logarithmic_error: 2.3971e-05The average loss for epoch 646 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 983us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00647: Learning rate is 0.0000.\n",
      "Epoch 648/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3904e-05 - mean_squared_logarithmic_error: 2.3904e-05The average loss for epoch 647 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00648: Learning rate is 0.0000.\n",
      "Epoch 649/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4040e-05 - mean_squared_logarithmic_error: 2.4040e-05The average loss for epoch 648 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00649: Learning rate is 0.0000.\n",
      "Epoch 650/800\n",
      "642/653 [============================>.] - ETA: 0s - loss: 2.4052e-05 - mean_squared_logarithmic_error: 2.4052e-05The average loss for epoch 649 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 965us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00650: Learning rate is 0.0000.\n",
      "Epoch 651/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4129e-05 - mean_squared_logarithmic_error: 2.4129e-05The average loss for epoch 650 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00651: Learning rate is 0.0000.\n",
      "Epoch 652/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3819e-05 - mean_squared_logarithmic_error: 2.3819e-05The average loss for epoch 651 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00652: Learning rate is 0.0000.\n",
      "Epoch 653/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.3916e-05 - mean_squared_logarithmic_error: 2.3916e-05The average loss for epoch 652 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00653: Learning rate is 0.0000.\n",
      "Epoch 654/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3968e-05 - mean_squared_logarithmic_error: 2.3968e-05The average loss for epoch 653 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00654: Learning rate is 0.0000.\n",
      "Epoch 655/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.3906e-05 - mean_squared_logarithmic_error: 2.3906e-05The average loss for epoch 654 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 988us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00655: Learning rate is 0.0000.\n",
      "Epoch 656/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4134e-05 - mean_squared_logarithmic_error: 2.4134e-05The average loss for epoch 655 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00656: Learning rate is 0.0000.\n",
      "Epoch 657/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05The average loss for epoch 656 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00657: Learning rate is 0.0000.\n",
      "Epoch 658/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4059e-05 - mean_squared_logarithmic_error: 2.4059e-05The average loss for epoch 657 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00658: Learning rate is 0.0000.\n",
      "Epoch 659/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4031e-05 - mean_squared_logarithmic_error: 2.4031e-05The average loss for epoch 658 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00659: Learning rate is 0.0000.\n",
      "Epoch 660/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4029e-05 - mean_squared_logarithmic_error: 2.4029e-05The average loss for epoch 659 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00660: Learning rate is 0.0000.\n",
      "Epoch 661/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4042e-05 - mean_squared_logarithmic_error: 2.4042e-05The average loss for epoch 660 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 930us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00661: Learning rate is 0.0000.\n",
      "Epoch 662/800\n",
      "616/653 [===========================>..] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 661 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 996us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00662: Learning rate is 0.0000.\n",
      "Epoch 663/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3883e-05 - mean_squared_logarithmic_error: 2.3883e-05The average loss for epoch 662 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 975us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00663: Learning rate is 0.0000.\n",
      "Epoch 664/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3968e-05 - mean_squared_logarithmic_error: 2.3968e-05The average loss for epoch 663 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00664: Learning rate is 0.0000.\n",
      "Epoch 665/800\n",
      "641/653 [============================>.] - ETA: 0s - loss: 2.4005e-05 - mean_squared_logarithmic_error: 2.4005e-05The average loss for epoch 664 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 963us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00665: Learning rate is 0.0000.\n",
      "Epoch 666/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.4045e-05 - mean_squared_logarithmic_error: 2.4045e-05The average loss for epoch 665 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00666: Learning rate is 0.0000.\n",
      "Epoch 667/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4037e-05 - mean_squared_logarithmic_error: 2.4037e-05The average loss for epoch 666 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 979us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00667: Learning rate is 0.0000.\n",
      "Epoch 668/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.4060e-05 - mean_squared_logarithmic_error: 2.4060e-05The average loss for epoch 667 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 978us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00668: Learning rate is 0.0000.\n",
      "Epoch 669/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4066e-05 - mean_squared_logarithmic_error: 2.4066e-05The average loss for epoch 668 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 977us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00669: Learning rate is 0.0000.\n",
      "Epoch 670/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.4059e-05 - mean_squared_logarithmic_error: 2.4059e-05The average loss for epoch 669 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00670: Learning rate is 0.0000.\n",
      "Epoch 671/800\n",
      "619/653 [===========================>..] - ETA: 0s - loss: 2.3884e-05 - mean_squared_logarithmic_error: 2.3884e-05The average loss for epoch 670 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 994us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00671: Learning rate is 0.0000.\n",
      "Epoch 672/800\n",
      "633/653 [============================>.] - ETA: 0s - loss: 2.4110e-05 - mean_squared_logarithmic_error: 2.4110e-05The average loss for epoch 671 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 974us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00672: Learning rate is 0.0000.\n",
      "Epoch 673/800\n",
      "618/653 [===========================>..] - ETA: 0s - loss: 2.3994e-05 - mean_squared_logarithmic_error: 2.3994e-05The average loss for epoch 672 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 993us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00673: Learning rate is 0.0000.\n",
      "Epoch 674/800\n",
      "630/653 [===========================>..] - ETA: 0s - loss: 2.3933e-05 - mean_squared_logarithmic_error: 2.3933e-05The average loss for epoch 673 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 980us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00674: Learning rate is 0.0000.\n",
      "Epoch 675/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4056e-05 - mean_squared_logarithmic_error: 2.4056e-05The average loss for epoch 674 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00675: Learning rate is 0.0000.\n",
      "Epoch 676/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 675 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00676: Learning rate is 0.0000.\n",
      "Epoch 677/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3974e-05 - mean_squared_logarithmic_error: 2.3974e-05The average loss for epoch 676 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 951us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00677: Learning rate is 0.0000.\n",
      "Epoch 678/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 677 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00678: Learning rate is 0.0000.\n",
      "Epoch 679/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4010e-05 - mean_squared_logarithmic_error: 2.4010e-05The average loss for epoch 678 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00679: Learning rate is 0.0000.\n",
      "Epoch 680/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4120e-05 - mean_squared_logarithmic_error: 2.4120e-05The average loss for epoch 679 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00680: Learning rate is 0.0000.\n",
      "Epoch 681/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.3936e-05 - mean_squared_logarithmic_error: 2.3936e-05The average loss for epoch 680 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00681: Learning rate is 0.0000.\n",
      "Epoch 682/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4023e-05 - mean_squared_logarithmic_error: 2.4023e-05The average loss for epoch 681 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00682: Learning rate is 0.0000.\n",
      "Epoch 683/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3883e-05 - mean_squared_logarithmic_error: 2.3883e-05The average loss for epoch 682 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 932us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00683: Learning rate is 0.0000.\n",
      "Epoch 684/800\n",
      "604/653 [==========================>...] - ETA: 0s - loss: 2.4187e-05 - mean_squared_logarithmic_error: 2.4187e-05The average loss for epoch 683 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00684: Learning rate is 0.0000.\n",
      "Epoch 685/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4026e-05 - mean_squared_logarithmic_error: 2.4026e-05The average loss for epoch 684 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00685: Learning rate is 0.0000.\n",
      "Epoch 686/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 685 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00686: Learning rate is 0.0000.\n",
      "Epoch 687/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3964e-05 - mean_squared_logarithmic_error: 2.3964e-05The average loss for epoch 686 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00687: Learning rate is 0.0000.\n",
      "Epoch 688/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3956e-05 - mean_squared_logarithmic_error: 2.3956e-05The average loss for epoch 687 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00688: Learning rate is 0.0000.\n",
      "Epoch 689/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4061e-05 - mean_squared_logarithmic_error: 2.4061e-05The average loss for epoch 688 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2977e-04 - val_mean_squared_logarithmic_error: 1.2977e-04\n",
      "\n",
      "Epoch 00689: Learning rate is 0.0000.\n",
      "Epoch 690/800\n",
      "593/653 [==========================>...] - ETA: 0s - loss: 2.4112e-05 - mean_squared_logarithmic_error: 2.4112e-05The average loss for epoch 689 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 942us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00690: Learning rate is 0.0000.\n",
      "Epoch 691/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.3851e-05 - mean_squared_logarithmic_error: 2.3851e-05The average loss for epoch 690 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 953us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00691: Learning rate is 0.0000.\n",
      "Epoch 692/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3986e-05 - mean_squared_logarithmic_error: 2.3986e-05The average loss for epoch 691 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00692: Learning rate is 0.0000.\n",
      "Epoch 693/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3975e-05 - mean_squared_logarithmic_error: 2.3975e-05The average loss for epoch 692 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00693: Learning rate is 0.0000.\n",
      "Epoch 694/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4054e-05 - mean_squared_logarithmic_error: 2.4054e-05The average loss for epoch 693 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00694: Learning rate is 0.0000.\n",
      "Epoch 695/800\n",
      "643/653 [============================>.] - ETA: 0s - loss: 2.3965e-05 - mean_squared_logarithmic_error: 2.3965e-05The average loss for epoch 694 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 959us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00695: Learning rate is 0.0000.\n",
      "Epoch 696/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4048e-05 - mean_squared_logarithmic_error: 2.4048e-05The average loss for epoch 695 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00696: Learning rate is 0.0000.\n",
      "Epoch 697/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4167e-05 - mean_squared_logarithmic_error: 2.4167e-05The average loss for epoch 696 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00697: Learning rate is 0.0000.\n",
      "Epoch 698/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4067e-05 - mean_squared_logarithmic_error: 2.4067e-05The average loss for epoch 697 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00698: Learning rate is 0.0000.\n",
      "Epoch 699/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.4091e-05 - mean_squared_logarithmic_error: 2.4091e-05The average loss for epoch 698 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 928us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00699: Learning rate is 0.0000.\n",
      "Epoch 700/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4005e-05 - mean_squared_logarithmic_error: 2.4005e-05The average loss for epoch 699 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00700: Learning rate is 0.0000.\n",
      "Epoch 701/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3881e-05 - mean_squared_logarithmic_error: 2.3881e-05The average loss for epoch 700 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 931us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00701: Learning rate is 0.0000.\n",
      "Epoch 702/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4041e-05 - mean_squared_logarithmic_error: 2.4041e-05The average loss for epoch 701 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00702: Learning rate is 0.0000.\n",
      "Epoch 703/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.3953e-05 - mean_squared_logarithmic_error: 2.3953e-05The average loss for epoch 702 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 985us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00703: Learning rate is 0.0000.\n",
      "Epoch 704/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4142e-05 - mean_squared_logarithmic_error: 2.4142e-05The average loss for epoch 703 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00704: Learning rate is 0.0000.\n",
      "Epoch 705/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 704 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 946us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00705: Learning rate is 0.0000.\n",
      "Epoch 706/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3961e-05 - mean_squared_logarithmic_error: 2.3961e-05The average loss for epoch 705 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00706: Learning rate is 0.0000.\n",
      "Epoch 707/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 706 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00707: Learning rate is 0.0000.\n",
      "Epoch 708/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3992e-05 - mean_squared_logarithmic_error: 2.3992e-05The average loss for epoch 707 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00708: Learning rate is 0.0000.\n",
      "Epoch 709/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.3948e-05 - mean_squared_logarithmic_error: 2.3948e-05The average loss for epoch 708 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 973us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00709: Learning rate is 0.0000.\n",
      "Epoch 710/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.4022e-05 - mean_squared_logarithmic_error: 2.4022e-05The average loss for epoch 709 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 954us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00710: Learning rate is 0.0000.\n",
      "Epoch 711/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.3947e-05 - mean_squared_logarithmic_error: 2.3947e-05The average loss for epoch 710 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 954us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00711: Learning rate is 0.0000.\n",
      "Epoch 712/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3833e-05 - mean_squared_logarithmic_error: 2.3833e-05The average loss for epoch 711 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00712: Learning rate is 0.0000.\n",
      "Epoch 713/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3951e-05 - mean_squared_logarithmic_error: 2.3951e-05The average loss for epoch 712 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00713: Learning rate is 0.0000.\n",
      "Epoch 714/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.3913e-05 - mean_squared_logarithmic_error: 2.3913e-05The average loss for epoch 713 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00714: Learning rate is 0.0000.\n",
      "Epoch 715/800\n",
      "629/653 [===========================>..] - ETA: 0s - loss: 2.3730e-05 - mean_squared_logarithmic_error: 2.3730e-05The average loss for epoch 714 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 982us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00715: Learning rate is 0.0000.\n",
      "Epoch 716/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.3953e-05 - mean_squared_logarithmic_error: 2.3953e-05The average loss for epoch 715 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 976us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00716: Learning rate is 0.0000.\n",
      "Epoch 717/800\n",
      "634/653 [============================>.] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 716 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 972us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00717: Learning rate is 0.0000.\n",
      "Epoch 718/800\n",
      "631/653 [===========================>..] - ETA: 0s - loss: 2.3762e-05 - mean_squared_logarithmic_error: 2.3762e-05The average loss for epoch 717 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 979us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00718: Learning rate is 0.0000.\n",
      "Epoch 719/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.3884e-05 - mean_squared_logarithmic_error: 2.3884e-05The average loss for epoch 718 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 991us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00719: Learning rate is 0.0000.\n",
      "Epoch 720/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.4020e-05 - mean_squared_logarithmic_error: 2.4020e-05The average loss for epoch 719 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 973us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00720: Learning rate is 0.0000.\n",
      "Epoch 721/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4009e-05 - mean_squared_logarithmic_error: 2.4009e-05The average loss for epoch 720 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 962us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00721: Learning rate is 0.0000.\n",
      "Epoch 722/800\n",
      "602/653 [==========================>...] - ETA: 0s - loss: 2.3889e-05 - mean_squared_logarithmic_error: 2.3889e-05The average loss for epoch 721 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00722: Learning rate is 0.0000.\n",
      "Epoch 723/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3987e-05 - mean_squared_logarithmic_error: 2.3987e-05The average loss for epoch 722 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00723: Learning rate is 0.0000.\n",
      "Epoch 724/800\n",
      "632/653 [============================>.] - ETA: 0s - loss: 2.4183e-05 - mean_squared_logarithmic_error: 2.4183e-05The average loss for epoch 723 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 984us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00724: Learning rate is 0.0000.\n",
      "Epoch 725/800\n",
      "635/653 [============================>.] - ETA: 0s - loss: 2.3917e-05 - mean_squared_logarithmic_error: 2.3917e-05The average loss for epoch 724 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 973us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00725: Learning rate is 0.0000.\n",
      "Epoch 726/800\n",
      "645/653 [============================>.] - ETA: 0s - loss: 2.4038e-05 - mean_squared_logarithmic_error: 2.4038e-05The average loss for epoch 725 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 954us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00726: Learning rate is 0.0000.\n",
      "Epoch 727/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 726 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 946us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00727: Learning rate is 0.0000.\n",
      "Epoch 728/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3993e-05 - mean_squared_logarithmic_error: 2.3993e-05The average loss for epoch 727 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00728: Learning rate is 0.0000.\n",
      "Epoch 729/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.3947e-05 - mean_squared_logarithmic_error: 2.3947e-05The average loss for epoch 728 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00729: Learning rate is 0.0000.\n",
      "Epoch 730/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4023e-05 - mean_squared_logarithmic_error: 2.4023e-05The average loss for epoch 729 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00730: Learning rate is 0.0000.\n",
      "Epoch 731/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4090e-05 - mean_squared_logarithmic_error: 2.4090e-05The average loss for epoch 730 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00731: Learning rate is 0.0000.\n",
      "Epoch 732/800\n",
      "593/653 [==========================>...] - ETA: 0s - loss: 2.3964e-05 - mean_squared_logarithmic_error: 2.3964e-05The average loss for epoch 731 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00732: Learning rate is 0.0000.\n",
      "Epoch 733/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.3936e-05 - mean_squared_logarithmic_error: 2.3936e-05The average loss for epoch 732 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 953us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00733: Learning rate is 0.0000.\n",
      "Epoch 734/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 733 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00734: Learning rate is 0.0000.\n",
      "Epoch 735/800\n",
      "647/653 [============================>.] - ETA: 0s - loss: 2.3970e-05 - mean_squared_logarithmic_error: 2.3970e-05The average loss for epoch 734 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00735: Learning rate is 0.0000.\n",
      "Epoch 736/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3974e-05 - mean_squared_logarithmic_error: 2.3974e-05The average loss for epoch 735 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00736: Learning rate is 0.0000.\n",
      "Epoch 737/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4123e-05 - mean_squared_logarithmic_error: 2.4123e-05The average loss for epoch 736 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00737: Learning rate is 0.0000.\n",
      "Epoch 738/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.4168e-05 - mean_squared_logarithmic_error: 2.4168e-05The average loss for epoch 737 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 926us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00738: Learning rate is 0.0000.\n",
      "Epoch 739/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 2.4006e-05 - mean_squared_logarithmic_error: 2.4006e-05The average loss for epoch 738 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 993us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00739: Learning rate is 0.0000.\n",
      "Epoch 740/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4035e-05 - mean_squared_logarithmic_error: 2.4035e-05The average loss for epoch 739 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00740: Learning rate is 0.0000.\n",
      "Epoch 741/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3799e-05 - mean_squared_logarithmic_error: 2.3799e-05 ETA: 0s - loss: 2.4041e-05 - mean_squared_logarithmic_error: 2.40The average loss for epoch 740 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00741: Learning rate is 0.0000.\n",
      "Epoch 742/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.3854e-05 - mean_squared_logarithmic_error: 2.3854e-05The average loss for epoch 741 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 929us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00742: Learning rate is 0.0000.\n",
      "Epoch 743/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.4018e-05 - mean_squared_logarithmic_error: 2.4018e-05The average loss for epoch 742 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 955us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00743: Learning rate is 0.0000.\n",
      "Epoch 744/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4135e-05 - mean_squared_logarithmic_error: 2.4135e-05The average loss for epoch 743 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 942us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00744: Learning rate is 0.0000.\n",
      "Epoch 745/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.4191e-05 - mean_squared_logarithmic_error: 2.4191e-05The average loss for epoch 744 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00745: Learning rate is 0.0000.\n",
      "Epoch 746/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.4017e-05 - mean_squared_logarithmic_error: 2.4017e-05The average loss for epoch 745 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00746: Learning rate is 0.0000.\n",
      "Epoch 747/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.4163e-05 - mean_squared_logarithmic_error: 2.4163e-05The average loss for epoch 746 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 931us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00747: Learning rate is 0.0000.\n",
      "Epoch 748/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3842e-05 - mean_squared_logarithmic_error: 2.3842e-05The average loss for epoch 747 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00748: Learning rate is 0.0000.\n",
      "Epoch 749/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3969e-05 - mean_squared_logarithmic_error: 2.3969e-05The average loss for epoch 748 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 950us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00749: Learning rate is 0.0000.\n",
      "Epoch 750/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.3828e-05 - mean_squared_logarithmic_error: 2.3828e-05The average loss for epoch 749 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00750: Learning rate is 0.0000.\n",
      "Epoch 751/800\n",
      "607/653 [==========================>...] - ETA: 0s - loss: 2.3973e-05 - mean_squared_logarithmic_error: 2.3973e-05The average loss for epoch 750 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00751: Learning rate is 0.0000.\n",
      "Epoch 752/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3896e-05 - mean_squared_logarithmic_error: 2.3896e-05The average loss for epoch 751 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00752: Learning rate is 0.0000.\n",
      "Epoch 753/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3899e-05 - mean_squared_logarithmic_error: 2.3899e-05The average loss for epoch 752 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00753: Learning rate is 0.0000.\n",
      "Epoch 754/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3853e-05 - mean_squared_logarithmic_error: 2.3853e-05The average loss for epoch 753 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00754: Learning rate is 0.0000.\n",
      "Epoch 755/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3980e-05 - mean_squared_logarithmic_error: 2.3980e-05The average loss for epoch 754 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 946us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00755: Learning rate is 0.0000.\n",
      "Epoch 756/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.4017e-05 - mean_squared_logarithmic_error: 2.4017e-05The average loss for epoch 755 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00756: Learning rate is 0.0000.\n",
      "Epoch 757/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4138e-05 - mean_squared_logarithmic_error: 2.4138e-05The average loss for epoch 756 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00757: Learning rate is 0.0000.\n",
      "Epoch 758/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4081e-05 - mean_squared_logarithmic_error: 2.4081e-05The average loss for epoch 757 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00758: Learning rate is 0.0000.\n",
      "Epoch 759/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3859e-05 - mean_squared_logarithmic_error: 2.3859e-05The average loss for epoch 758 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 936us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00759: Learning rate is 0.0000.\n",
      "Epoch 760/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.3921e-05 - mean_squared_logarithmic_error: 2.3921e-05The average loss for epoch 759 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00760: Learning rate is 0.0000.\n",
      "Epoch 761/800\n",
      "646/653 [============================>.] - ETA: 0s - loss: 2.3907e-05 - mean_squared_logarithmic_error: 2.3907e-05The average loss for epoch 760 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 957us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00761: Learning rate is 0.0000.\n",
      "Epoch 762/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3968e-05 - mean_squared_logarithmic_error: 2.3968e-05The average loss for epoch 761 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 945us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00762: Learning rate is 0.0000.\n",
      "Epoch 763/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4188e-05 - mean_squared_logarithmic_error: 2.4188e-05The average loss for epoch 762 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00763: Learning rate is 0.0000.\n",
      "Epoch 764/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.4091e-05 - mean_squared_logarithmic_error: 2.4091e-05The average loss for epoch 763 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 932us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00764: Learning rate is 0.0000.\n",
      "Epoch 765/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3971e-05 - mean_squared_logarithmic_error: 2.3971e-05The average loss for epoch 764 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 944us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00765: Learning rate is 0.0000.\n",
      "Epoch 766/800\n",
      "651/653 [============================>.] - ETA: 0s - loss: 2.3969e-05 - mean_squared_logarithmic_error: 2.3969e-05The average loss for epoch 765 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00766: Learning rate is 0.0000.\n",
      "Epoch 767/800\n",
      "621/653 [===========================>..] - ETA: 0s - loss: 2.4030e-05 - mean_squared_logarithmic_error: 2.4030e-05The average loss for epoch 766 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 985us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00767: Learning rate is 0.0000.\n",
      "Epoch 768/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3982e-05 - mean_squared_logarithmic_error: 2.3982e-05The average loss for epoch 767 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00768: Learning rate is 0.0000.\n",
      "Epoch 769/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 768 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 948us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00769: Learning rate is 0.0000.\n",
      "Epoch 770/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.4030e-05 - mean_squared_logarithmic_error: 2.4030e-05The average loss for epoch 769 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00770: Learning rate is 0.0000.\n",
      "Epoch 771/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 770 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00771: Learning rate is 0.0000.\n",
      "Epoch 772/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4015e-05 - mean_squared_logarithmic_error: 2.4015e-05The average loss for epoch 771 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 939us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00772: Learning rate is 0.0000.\n",
      "Epoch 773/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.4010e-05 - mean_squared_logarithmic_error: 2.4010e-05The average loss for epoch 772 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00773: Learning rate is 0.0000.\n",
      "Epoch 774/800\n",
      "595/653 [==========================>...] - ETA: 0s - loss: 2.4009e-05 - mean_squared_logarithmic_error: 2.4009e-05The average loss for epoch 773 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 941us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00774: Learning rate is 0.0000.\n",
      "Epoch 775/800\n",
      "640/653 [============================>.] - ETA: 0s - loss: 2.4008e-05 - mean_squared_logarithmic_error: 2.4008e-05The average loss for epoch 774 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 965us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00775: Learning rate is 0.0000.\n",
      "Epoch 776/800\n",
      "594/653 [==========================>...] - ETA: 0s - loss: 2.3941e-05 - mean_squared_logarithmic_error: 2.3941e-05The average loss for epoch 775 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 940us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00776: Learning rate is 0.0000.\n",
      "Epoch 777/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3942e-05 - mean_squared_logarithmic_error: 2.3942e-05The average loss for epoch 776 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00777: Learning rate is 0.0000.\n",
      "Epoch 778/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3984e-05 - mean_squared_logarithmic_error: 2.3984e-05The average loss for epoch 777 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 932us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00778: Learning rate is 0.0000.\n",
      "Epoch 779/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3989e-05 - mean_squared_logarithmic_error: 2.3989e-05The average loss for epoch 778 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00779: Learning rate is 0.0000.\n",
      "Epoch 780/800\n",
      "603/653 [==========================>...] - ETA: 0s - loss: 2.3592e-05 - mean_squared_logarithmic_error: 2.3592e-05The average loss for epoch 779 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 931us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00780: Learning rate is 0.0000.\n",
      "Epoch 781/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3827e-05 - mean_squared_logarithmic_error: 2.3827e-05The average loss for epoch 780 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 934us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00781: Learning rate is 0.0000.\n",
      "Epoch 782/800\n",
      "650/653 [============================>.] - ETA: 0s - loss: 2.3961e-05 - mean_squared_logarithmic_error: 2.3961e-05The average loss for epoch 781 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 949us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00782: Learning rate is 0.0000.\n",
      "Epoch 783/800\n",
      "644/653 [============================>.] - ETA: 0s - loss: 2.4003e-05 - mean_squared_logarithmic_error: 2.4003e-05The average loss for epoch 782 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 959us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00783: Learning rate is 0.0000.\n",
      "Epoch 784/800\n",
      "652/653 [============================>.] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 783 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 944us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00784: Learning rate is 0.0000.\n",
      "Epoch 785/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3963e-05 - mean_squared_logarithmic_error: 2.3963e-05The average loss for epoch 784 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 933us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00785: Learning rate is 0.0000.\n",
      "Epoch 786/800\n",
      "653/653 [==============================] - ETA: 0s - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05The average loss for epoch 785 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 946us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00786: Learning rate is 0.0000.\n",
      "Epoch 787/800\n",
      "596/653 [==========================>...] - ETA: 0s - loss: 2.4058e-05 - mean_squared_logarithmic_error: 2.4058e-05The average loss for epoch 786 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 938us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00787: Learning rate is 0.0000.\n",
      "Epoch 788/800\n",
      "600/653 [==========================>...] - ETA: 0s - loss: 2.3948e-05 - mean_squared_logarithmic_error: 2.3948e-05The average loss for epoch 787 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 931us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00788: Learning rate is 0.0000.\n",
      "Epoch 789/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3985e-05 - mean_squared_logarithmic_error: 2.3985e-05The average loss for epoch 788 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00789: Learning rate is 0.0000.\n",
      "Epoch 790/800\n",
      "601/653 [==========================>...] - ETA: 0s - loss: 2.3872e-05 - mean_squared_logarithmic_error: 2.3872e-05The average loss for epoch 789 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 935us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00790: Learning rate is 0.0000.\n",
      "Epoch 791/800\n",
      "638/653 [============================>.] - ETA: 0s - loss: 2.3932e-05 - mean_squared_logarithmic_error: 2.3932e-05The average loss for epoch 790 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 968us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00791: Learning rate is 0.0000.\n",
      "Epoch 792/800\n",
      "599/653 [==========================>...] - ETA: 0s - loss: 2.3997e-05 - mean_squared_logarithmic_error: 2.3997e-05The average loss for epoch 791 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00792: Learning rate is 0.0000.\n",
      "Epoch 793/800\n",
      "648/653 [============================>.] - ETA: 0s - loss: 2.4001e-05 - mean_squared_logarithmic_error: 2.4001e-05 ETA: 0s - loss: 2.4049e-05 - mean_squared_logarithmic_error: 2.4049e-The average loss for epoch 792 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 951us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00793: Learning rate is 0.0000.\n",
      "Epoch 794/800\n",
      "636/653 [============================>.] - ETA: 0s - loss: 2.3942e-05 - mean_squared_logarithmic_error: 2.3942e-05The average loss for epoch 793 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1ms/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00794: Learning rate is 0.0000.\n",
      "Epoch 795/800\n",
      "597/653 [==========================>...] - ETA: 0s - loss: 2.4155e-05 - mean_squared_logarithmic_error: 2.4155e-05The average loss for epoch 794 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00795: Learning rate is 0.0000.\n",
      "Epoch 796/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648/653 [============================>.] - ETA: 0s - loss: 2.3969e-05 - mean_squared_logarithmic_error: 2.3969e-05The average loss for epoch 795 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 952us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00796: Learning rate is 0.0000.\n",
      "Epoch 797/800\n",
      "617/653 [===========================>..] - ETA: 0s - loss: 2.4103e-05 - mean_squared_logarithmic_error: 2.4103e-05The average loss for epoch 796 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 991us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00797: Learning rate is 0.0000.\n",
      "Epoch 798/800\n",
      "620/653 [===========================>..] - ETA: 0s - loss: 2.4099e-05 - mean_squared_logarithmic_error: 2.4099e-05The average loss for epoch 797 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 1000us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00798: Learning rate is 0.0000.\n",
      "Epoch 799/800\n",
      "613/653 [===========================>..] - ETA: 0s - loss: 2.3969e-05 - mean_squared_logarithmic_error: 2.3969e-05The average loss for epoch 798 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 995us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n",
      "\n",
      "Epoch 00799: Learning rate is 0.0000.\n",
      "Epoch 800/800\n",
      "598/653 [==========================>...] - ETA: 0s - loss: 2.3869e-05 - mean_squared_logarithmic_error: 2.3869e-05The average loss for epoch 799 is    0.00 and MSE is    0.00.\n",
      "653/653 [==============================] - 1s 937us/step - loss: 2.3972e-05 - mean_squared_logarithmic_error: 2.3972e-05 - val_loss: 1.2978e-04 - val_mean_squared_logarithmic_error: 1.2978e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16b91a130>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keff_model.fit(X_train,y_train.T, epochs=800, batch_size=44, verbose=1,\n",
    "               shuffle=True,callbacks=[LossAndErrorPrintingCallback(),\n",
    "        CustomLearningRateScheduler(lr_schedule)],validation_data=(X_val,y_val.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015707737"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = keff_model.predict(X_test)\n",
    "metric = tf.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
    "metric.update_state(np.array(y_predicted*normConst),np.array(y_test.T*normConst))\n",
    "metric.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8204, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating dataset\n",
    "a = (y_predicted-y_test.T)\n",
    "print(a.shape)\n",
    "# Creating histogram\n",
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "ax.hist(a[:,2],bins=70,alpha=0.75,label=\"EOL\",color='r')\n",
    "ax.hist(a[:,1],bins=70,alpha=0.25,label=\"MOL\",color='b')\n",
    "ax.hist(a[:,0],bins=70,alpha=0.25,label=\"BOL\",color='g')\n",
    "plt.xlabel(\"$Δk_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.title(\"SFR $Δk_{\\infty}$, flux based serialization\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"SFR_incompleteData_fluxShaped_2.png\")\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    " \n",
    "\n",
    "n_bins = 500\n",
    " \n",
    "\n",
    "# Creating histogram\n",
    "fig, axs = plt.subplots(figsize =(10, 7))\n",
    " \n",
    "axs.hist(y_test.T[:,0], bins = n_bins,label=\"BOL actual\")\n",
    "axs.hist(y_predicted[:,0], bins = n_bins,label=\"BOL predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.title(\"SFR $k_{\\infty}$, shaped serialization\")\n",
    "plt.savefig(\"kinfPredDist_fluxShaped_narrow_2.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "\n",
    "plt.show()\n",
    "# Show plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03901901842931891\n",
      "0.03961941320533842\n",
      "0.04023739559658851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.std(a[:,2]))\n",
    "print(np.std(a[:,1]))\n",
    "print(np.std(a[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
