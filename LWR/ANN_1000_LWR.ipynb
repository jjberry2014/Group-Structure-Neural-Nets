{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/Volumes/data/LosAlamosSummer')\n",
    "sys.path.insert(0, '/Volumes/data/LosAlamosSummer/DrOsborneCode')\n",
    "\n",
    "import Utilities as ut\n",
    "import importlib\n",
    "import model as mod\n",
    "import predict_with_uncertainty as pu\n",
    "import custom as cus\n",
    "importlib.reload(ut)\n",
    "importlib.reload(mod)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(cus)\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading In Data\n",
      "Finished Loading Data\n"
     ]
    }
   ],
   "source": [
    "#datapath = '/Users/jessiejo/data/VBUDS/GroupStructurePaper/NeuralNetworks/All_Libraries/NewDataSetFull1.mat'\n",
    "datapath='/Volumes/data/LosAlamosSummer/LWR/DATA/LWR_data_7.mat'\n",
    "print('Loading In Data')\n",
    "kinfBOL,kinfMOL,kinfEOL,GS=ut.LoadData(datapath,1)\n",
    "#MakeGroupDensity(X, nDecades)\n",
    "Nfeatures = 1000;\n",
    "allData= ut.ProcessData(datapath, 1,1000,0,0,1)\n",
    "# allData: (100,000x1,000) y_direct: (100,000x3)\n",
    "print('Finished Loading Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load Data loads raw data from the .mat file\n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]\n",
    "\n",
    "ProcessData is the serialization maker \n",
    "Inputs\n",
    "datapath: Path to .mat file containing the data [string]\n",
    "Percent of data to be used: in most cases full data set will be used but good for analysis [double](0-1)\n",
    "ndecades: Number of decades wanted in equal lethargy serialization. Number is ignored if custom serialization inputted [int]\n",
    "mode: equal lethargy mode (0) or custom serialization mode (1) [boolean]\n",
    "input serial: a custom serialization regime (ignored if mode is not 1) [numpy array]\n",
    "BU: whether the data contains burnup; 1=burnup, 0=no burnup [bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999,)\n",
      "49999\n",
      "(49999, 3)\n",
      "0.25062617612169336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install -q -U keras-tuner\n",
    "print(kinfBOL.shape)\n",
    "print(len(kinfBOL))\n",
    "kinf=np.array(np.zeros((len(kinfBOL),3)))\n",
    "kinf[:,0]=kinfBOL\n",
    "kinf[:,1]=kinfMOL\n",
    "kinf[:,2]=kinfEOL#np.concatenate((kinfBOL,kinfMOL,kinfEOL),axis=0)\n",
    "print(kinf.shape)\n",
    "print(np.std(kinfBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9999)\n",
      "(3, 4999)\n",
      "(49999, 3)\n",
      "(3, 34999)\n"
     ]
    }
   ],
   "source": [
    "Nsamples,Ndecades = allData.shape\n",
    "vldF=.1\n",
    "testF=.2\n",
    "normConst=1#np.linalg.norm(kinf)\n",
    "y_norm=np.array(kinf/normConst)\n",
    "\n",
    "X, X_test, y, y_test, vldF_corr = ut.makeFractions(Nsamples, vldF, testF, allData, y_norm, 1)\n",
    "\n",
    "\n",
    "NtrainingSamples = int(Nsamples*(1 - testF))\n",
    "tranValSplit=int(NtrainingSamples*(1-vldF_corr))\n",
    "X_train=X[:tranValSplit,:]\n",
    "y_train=y[:,:tranValSplit]\n",
    "X_val=X[tranValSplit+1:,:]\n",
    "y_val=y[:,tranValSplit+1:]\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_norm.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makeFractions splits the data into appropriatly sized sets\n",
    "Nsamples is the number of samples of the data set\n",
    "vldF is the validation fraction\n",
    "testF is the test fraction\n",
    "allData is the set of serialzed group structures\n",
    "y_norm is the kinfs that correspond to the serialized group structures (normalized or otherwise)\n",
    "BU (the last input) is a boolean determining whether the data contains burnup [Boolean] (used in the same manner as previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 1053)              1053000   \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 96)                101184    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 291       \n",
      "=================================================================\n",
      "Total params: 1,154,475\n",
      "Trainable params: 1,154,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=192;\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(1053, activation='relu', name='hidden_1', input_dim=999),\n",
    "    layers.Dense(96, activation='relu',  name='hidden_2'),\n",
    "    layers.Dense(3, activation='linear',name='output')])\n",
    "model.compile(loss=\"mean_squared_logarithmic_error\",metrics=\"mean_squared_logarithmic_error\")\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.1709 - val_loss: 0.1452\n",
      "Epoch 2/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.1022 - val_loss: 0.0745\n",
      "Epoch 3/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0446 - val_loss: 0.0430\n",
      "Epoch 4/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0354\n",
      "Epoch 5/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0265 - val_loss: 0.0351\n",
      "Epoch 6/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0231 - val_loss: 0.0315\n",
      "Epoch 7/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0208 - val_loss: 0.0308\n",
      "Epoch 8/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0189 - val_loss: 0.0288\n",
      "Epoch 9/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0186 - val_loss: 0.0319\n",
      "Epoch 10/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0170 - val_loss: 0.0304\n",
      "Epoch 11/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0159 - val_loss: 0.0276\n",
      "Epoch 12/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0148 - val_loss: 0.0265\n",
      "Epoch 13/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0141 - val_loss: 0.0274\n",
      "Epoch 14/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0138 - val_loss: 0.0261\n",
      "Epoch 15/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0128 - val_loss: 0.0276\n",
      "Epoch 16/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0127 - val_loss: 0.0256\n",
      "Epoch 17/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0120 - val_loss: 0.0248\n",
      "Epoch 18/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0117 - val_loss: 0.0261\n",
      "Epoch 19/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0114 - val_loss: 0.0243\n",
      "Epoch 20/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0109 - val_loss: 0.0255\n",
      "Epoch 21/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0112 - val_loss: 0.0249\n",
      "Epoch 22/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0100 - val_loss: 0.0254\n",
      "Epoch 23/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0099 - val_loss: 0.0228\n",
      "Epoch 24/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0099 - val_loss: 0.0240\n",
      "Epoch 25/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0098 - val_loss: 0.0232\n",
      "Epoch 26/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0098 - val_loss: 0.0223\n",
      "Epoch 27/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0102 - val_loss: 0.0250\n",
      "Epoch 28/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0092 - val_loss: 0.0270\n",
      "Epoch 29/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0092 - val_loss: 0.0231\n",
      "Epoch 30/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0088 - val_loss: 0.0221\n",
      "Epoch 31/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0085 - val_loss: 0.0221\n",
      "Epoch 32/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0088 - val_loss: 0.0223\n",
      "Epoch 33/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0084 - val_loss: 0.0225\n",
      "Epoch 34/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0083 - val_loss: 0.0225\n",
      "Epoch 35/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0084 - val_loss: 0.0222\n",
      "Epoch 36/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0080 - val_loss: 0.0216\n",
      "Epoch 37/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0080 - val_loss: 0.0222\n",
      "Epoch 38/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0079 - val_loss: 0.0220\n",
      "Epoch 39/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0075 - val_loss: 0.0219\n",
      "Epoch 40/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0077 - val_loss: 0.0213\n",
      "Epoch 41/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0081 - val_loss: 0.0225\n",
      "Epoch 42/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0075 - val_loss: 0.0213\n",
      "Epoch 43/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0071 - val_loss: 0.0205\n",
      "Epoch 44/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0067 - val_loss: 0.0210\n",
      "Epoch 45/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0069 - val_loss: 0.0205\n",
      "Epoch 46/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0070 - val_loss: 0.0203\n",
      "Epoch 47/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0075 - val_loss: 0.0216\n",
      "Epoch 48/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0068 - val_loss: 0.0209\n",
      "Epoch 49/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0067 - val_loss: 0.0203\n",
      "Epoch 50/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0069 - val_loss: 0.0204\n",
      "Epoch 51/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0066 - val_loss: 0.0214\n",
      "Epoch 52/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0071 - val_loss: 0.0202\n",
      "Epoch 53/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0064 - val_loss: 0.0202\n",
      "Epoch 54/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0063 - val_loss: 0.0197\n",
      "Epoch 55/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0199\n",
      "Epoch 56/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0062 - val_loss: 0.0200\n",
      "Epoch 57/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0066 - val_loss: 0.0201\n",
      "Epoch 58/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0063 - val_loss: 0.0199\n",
      "Epoch 59/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0063 - val_loss: 0.0197\n",
      "Epoch 60/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0064 - val_loss: 0.0202\n",
      "Epoch 61/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0061 - val_loss: 0.0200\n",
      "Epoch 62/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0059 - val_loss: 0.0199\n",
      "Epoch 63/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0196\n",
      "Epoch 64/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0064 - val_loss: 0.0192\n",
      "Epoch 65/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0056 - val_loss: 0.0196\n",
      "Epoch 66/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0059 - val_loss: 0.0193\n",
      "Epoch 67/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0057 - val_loss: 0.0197\n",
      "Epoch 68/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0061 - val_loss: 0.0191\n",
      "Epoch 69/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0055 - val_loss: 0.0195\n",
      "Epoch 70/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0058 - val_loss: 0.0194\n",
      "Epoch 71/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0056 - val_loss: 0.0193\n",
      "Epoch 72/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0059 - val_loss: 0.0191\n",
      "Epoch 73/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0058 - val_loss: 0.0208\n",
      "Epoch 74/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0058 - val_loss: 0.0199\n",
      "Epoch 75/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0055 - val_loss: 0.0197\n",
      "Epoch 76/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0054 - val_loss: 0.0195\n",
      "Epoch 77/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0187\n",
      "Epoch 78/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0052 - val_loss: 0.0191\n",
      "Epoch 79/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0051 - val_loss: 0.0186\n",
      "Epoch 80/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0188\n",
      "Epoch 81/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0184\n",
      "Epoch 82/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0188\n",
      "Epoch 83/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0192\n",
      "Epoch 84/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0055 - val_loss: 0.0187\n",
      "Epoch 85/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0052 - val_loss: 0.0187\n",
      "Epoch 86/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0053 - val_loss: 0.0194\n",
      "Epoch 87/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0052 - val_loss: 0.0187\n",
      "Epoch 88/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0050 - val_loss: 0.0188\n",
      "Epoch 89/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0052 - val_loss: 0.0185\n",
      "Epoch 90/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0047 - val_loss: 0.0184\n",
      "Epoch 91/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0047 - val_loss: 0.0187\n",
      "Epoch 92/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0184\n",
      "Epoch 93/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0048 - val_loss: 0.0183\n",
      "Epoch 94/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0184\n",
      "Epoch 95/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0047 - val_loss: 0.0182\n",
      "Epoch 96/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0049 - val_loss: 0.0196\n",
      "Epoch 97/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0049 - val_loss: 0.0180\n",
      "Epoch 98/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0048 - val_loss: 0.0181\n",
      "Epoch 99/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0046 - val_loss: 0.0181\n",
      "Epoch 100/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0050 - val_loss: 0.0183\n",
      "Epoch 101/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0046 - val_loss: 0.0183\n",
      "Epoch 102/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0181\n",
      "Epoch 103/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0047 - val_loss: 0.0182\n",
      "Epoch 104/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0048 - val_loss: 0.0183\n",
      "Epoch 105/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0045 - val_loss: 0.0181\n",
      "Epoch 106/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0183\n",
      "Epoch 107/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0183\n",
      "Epoch 108/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0188\n",
      "Epoch 109/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0046 - val_loss: 0.0188\n",
      "Epoch 110/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0048 - val_loss: 0.0178\n",
      "Epoch 111/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0190\n",
      "Epoch 112/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0049 - val_loss: 0.0179\n",
      "Epoch 113/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0044 - val_loss: 0.0181\n",
      "Epoch 114/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0043 - val_loss: 0.0181\n",
      "Epoch 115/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0040 - val_loss: 0.0180\n",
      "Epoch 116/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0043 - val_loss: 0.0179\n",
      "Epoch 117/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0043 - val_loss: 0.0181\n",
      "Epoch 118/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0048 - val_loss: 0.0176\n",
      "Epoch 119/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0180\n",
      "Epoch 120/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0043 - val_loss: 0.0181\n",
      "Epoch 121/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0045 - val_loss: 0.0192\n",
      "Epoch 122/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0045 - val_loss: 0.0177\n",
      "Epoch 123/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0176\n",
      "Epoch 124/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0043 - val_loss: 0.0176\n",
      "Epoch 125/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0039 - val_loss: 0.0176\n",
      "Epoch 126/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "Epoch 127/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0046 - val_loss: 0.0175\n",
      "Epoch 128/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0174\n",
      "Epoch 129/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0176\n",
      "Epoch 130/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0179\n",
      "Epoch 131/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0042 - val_loss: 0.0175\n",
      "Epoch 132/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0040 - val_loss: 0.0177\n",
      "Epoch 133/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0043 - val_loss: 0.0177\n",
      "Epoch 134/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0177\n",
      "Epoch 135/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0185\n",
      "Epoch 136/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0174\n",
      "Epoch 137/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0184\n",
      "Epoch 138/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0179\n",
      "Epoch 139/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0041 - val_loss: 0.0175\n",
      "Epoch 140/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0172\n",
      "Epoch 141/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0173\n",
      "Epoch 142/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0173\n",
      "Epoch 143/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0036 - val_loss: 0.0176\n",
      "Epoch 144/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0174\n",
      "Epoch 145/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0171\n",
      "Epoch 146/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0046 - val_loss: 0.0179\n",
      "Epoch 147/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0173\n",
      "Epoch 148/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0036 - val_loss: 0.0174\n",
      "Epoch 149/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0037 - val_loss: 0.0173\n",
      "Epoch 150/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0174\n",
      "Epoch 151/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0170\n",
      "Epoch 152/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0036 - val_loss: 0.0172\n",
      "Epoch 153/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0035 - val_loss: 0.0174\n",
      "Epoch 154/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0173\n",
      "Epoch 155/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0170\n",
      "Epoch 156/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "Epoch 157/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0169\n",
      "Epoch 158/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0037 - val_loss: 0.0170\n",
      "Epoch 159/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0036 - val_loss: 0.0170\n",
      "Epoch 160/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0037 - val_loss: 0.0174\n",
      "Epoch 162/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0169\n",
      "Epoch 163/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0039 - val_loss: 0.0169\n",
      "Epoch 164/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0035 - val_loss: 0.0169\n",
      "Epoch 165/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0034 - val_loss: 0.0173\n",
      "Epoch 166/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0034 - val_loss: 0.0169\n",
      "Epoch 167/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0173\n",
      "Epoch 168/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0169\n",
      "Epoch 169/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0170\n",
      "Epoch 170/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0039 - val_loss: 0.0171\n",
      "Epoch 171/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0169\n",
      "Epoch 172/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0035 - val_loss: 0.0174\n",
      "Epoch 173/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0038 - val_loss: 0.0167\n",
      "Epoch 174/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0169\n",
      "Epoch 175/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0168\n",
      "Epoch 176/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0169\n",
      "Epoch 177/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0173\n",
      "Epoch 178/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0038 - val_loss: 0.0171\n",
      "Epoch 179/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0172\n",
      "Epoch 180/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0171\n",
      "Epoch 181/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0172\n",
      "Epoch 182/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0167\n",
      "Epoch 183/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0169\n",
      "Epoch 184/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0167\n",
      "Epoch 185/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0167\n",
      "Epoch 186/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0032 - val_loss: 0.0168\n",
      "Epoch 187/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0168\n",
      "Epoch 188/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0166\n",
      "Epoch 189/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0167\n",
      "Epoch 190/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0166\n",
      "Epoch 191/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0030 - val_loss: 0.0166\n",
      "Epoch 192/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0031 - val_loss: 0.0167\n",
      "Epoch 193/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0167\n",
      "Epoch 194/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0035 - val_loss: 0.0168\n",
      "Epoch 195/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0165\n",
      "Epoch 196/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0168\n",
      "Epoch 197/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0035 - val_loss: 0.0170\n",
      "Epoch 198/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0167\n",
      "Epoch 199/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0032 - val_loss: 0.0166\n",
      "Epoch 200/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0034 - val_loss: 0.0165\n",
      "Epoch 201/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0168\n",
      "Epoch 202/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0167\n",
      "Epoch 203/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0166\n",
      "Epoch 204/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0165\n",
      "Epoch 205/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0164\n",
      "Epoch 206/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0033 - val_loss: 0.0168\n",
      "Epoch 207/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0031 - val_loss: 0.0164\n",
      "Epoch 208/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0029 - val_loss: 0.0165\n",
      "Epoch 209/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0165\n",
      "Epoch 210/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0165\n",
      "Epoch 211/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0171\n",
      "Epoch 212/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0165\n",
      "Epoch 213/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0164\n",
      "Epoch 214/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0164\n",
      "Epoch 215/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0168\n",
      "Epoch 216/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0164\n",
      "Epoch 217/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0164\n",
      "Epoch 218/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0030 - val_loss: 0.0166\n",
      "Epoch 219/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0163\n",
      "Epoch 220/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0165\n",
      "Epoch 221/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0166\n",
      "Epoch 222/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0166\n",
      "Epoch 223/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0169\n",
      "Epoch 224/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0163\n",
      "Epoch 225/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0168\n",
      "Epoch 226/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0165\n",
      "Epoch 227/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0032 - val_loss: 0.0169\n",
      "Epoch 228/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0165\n",
      "Epoch 229/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0171\n",
      "Epoch 230/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0163\n",
      "Epoch 231/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0162\n",
      "Epoch 232/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0031 - val_loss: 0.0163\n",
      "Epoch 233/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0033 - val_loss: 0.0165\n",
      "Epoch 234/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0162\n",
      "Epoch 235/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0161\n",
      "Epoch 236/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0162\n",
      "Epoch 237/800\n",
      "183/183 [==============================] - 2s 11ms/step - loss: 0.0028 - val_loss: 0.0163\n",
      "Epoch 238/800\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.0027 - val_loss: 0.0163\n",
      "Epoch 239/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0026 - val_loss: 0.0164\n",
      "Epoch 240/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0163\n",
      "Epoch 241/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0163\n",
      "Epoch 242/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0165\n",
      "Epoch 243/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0029 - val_loss: 0.0164\n",
      "Epoch 244/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0162\n",
      "Epoch 245/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0164\n",
      "Epoch 246/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0028 - val_loss: 0.0162\n",
      "Epoch 247/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0030 - val_loss: 0.0171\n",
      "Epoch 248/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0164\n",
      "Epoch 249/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0027 - val_loss: 0.0162\n",
      "Epoch 250/800\n",
      "179/183 [============================>.] - ETA: 0s - loss: 0.0027Restoring model weights from the end of the best epoch.\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0027 - val_loss: 0.0163\n",
      "Epoch 00250: early stopping\n",
      "Epoch 1/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0021 - val_loss: 0.0160\n",
      "Epoch 2/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0017 - val_loss: 0.0160\n",
      "Epoch 3/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 4/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0013 - val_loss: 0.0160\n",
      "Epoch 5/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0012 - val_loss: 0.0160\n",
      "Epoch 6/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0011 - val_loss: 0.0161\n",
      "Epoch 7/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0010 - val_loss: 0.0161\n",
      "Epoch 8/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 9.7041e-04 - val_loss: 0.0161\n",
      "Epoch 9/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 9.2250e-04 - val_loss: 0.0161\n",
      "Epoch 10/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 8.8837e-04 - val_loss: 0.0161\n",
      "Epoch 11/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 8.5297e-04 - val_loss: 0.0161\n",
      "Epoch 12/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 8.4341e-04 - val_loss: 0.0161\n",
      "Epoch 13/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 8.1293e-04 - val_loss: 0.0161\n",
      "Epoch 14/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 7.8308e-04 - val_loss: 0.0161\n",
      "Epoch 15/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 7.7376e-04 - val_loss: 0.0161\n",
      "Epoch 16/800\n",
      "180/183 [============================>.] - ETA: 0s - loss: 7.6177e-04Restoring model weights from the end of the best epoch.\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 7.6212e-04 - val_loss: 0.0161\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/800\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 2/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 3/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 4/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 5/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 6/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 7/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 8/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0160\n",
      "Epoch 9/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0160\n",
      "Epoch 10/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0014 - val_loss: 0.0160\n",
      "Epoch 11/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0160\n",
      "Epoch 12/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0160\n",
      "Epoch 13/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0160\n",
      "Epoch 14/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0013 - val_loss: 0.0160\n",
      "Epoch 15/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0013 - val_loss: 0.0160\n",
      "Epoch 16/800\n",
      "178/183 [============================>.] - ETA: 0s - loss: 0.0013Restoring model weights from the end of the best epoch.\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0013 - val_loss: 0.0160\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/800\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 2/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 3/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 4/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 5/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 6/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 7/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 8/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 9/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 10/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 11/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 12/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 13/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 14/800\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 15/800\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0016 - val_loss: 0.0160\n",
      "Epoch 16/800\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0015Restoring model weights from the end of the best epoch.\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0015 - val_loss: 0.0160\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17925a7f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-4))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-5))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())\n",
    "model.compile(loss='MAE',optimizer=tf.keras.optimizers.Adam(1e-6))\n",
    "model.fit(X_train,y_train.T, epochs=800, batch_size=batch_size, verbose=1,validation_data=(X_val,y_val.T), callbacks=cus.callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013994994\n",
      "[[1.3717239  1.0839608  0.88836324]\n",
      " [1.3663667  1.0805475  0.88662547]\n",
      " [0.8926562  0.63980085 0.4955833 ]\n",
      " ...\n",
      " [1.3654922  1.0743272  0.8750871 ]\n",
      " [1.372577   1.0814513  0.870959  ]\n",
      " [0.8136295  0.6143204  0.48565847]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "metric = tf.keras.metrics.MeanSquaredError(name=\"mean_average_error\", dtype=None)\n",
    "metric.update_state(np.array(y_predicted*normConst),np.array(y_test.T*normConst))\n",
    "print(metric.result().numpy())\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating dataset\n",
    "a = (y_predicted-y_test.T)\n",
    "print(a.shape)\n",
    "# Creating histogram\n",
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "ax.hist(a[:,2],bins=500,alpha=0.75,label=\"EOL\",color='r')\n",
    "ax.hist(a[:,1],bins=500,alpha=0.25,label=\"MOL\",color='b')\n",
    "ax.hist(a[:,0],bins=500,alpha=0.25,label=\"BOL\",color='g')\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "plt.title(\"LWR $k_{\\infty}$, eq leth serialization\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim([-0.5,0.5])\n",
    "plt.ylim([0,1000])\n",
    "\n",
    "plt.savefig(\"PICS/LWR_deltaK_eqleth_opti.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039587860333798894\n",
      "0.038042128720711874\n",
      "0.03438322788652094\n",
      "0.037337738980343906\n",
      "[0.03958786 0.03804213 0.03438323]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.std(a[:,0]))\n",
    "print(np.std(a[:,1]))\n",
    "print(np.std(a[:,2]))\n",
    "print(np.mean([np.std(a[:,0]),np.std(a[:,1]),np.std(a[:,2])]))\n",
    "print(np.std(a,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3717239 1.3663667 0.8926562 ... 1.3654922 1.372577  0.8136295]\n",
      "(9999, 3)\n",
      "(3, 9999)\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted[:,0])\n",
    "print(y_predicted.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    " \n",
    "\n",
    "n_bins = 1000\n",
    " \n",
    "\n",
    "# Creating histogram\n",
    "fig, axs = plt.subplots(figsize =(10, 7))\n",
    " \n",
    "axs.hist(y_test.T[:,0], bins = n_bins,label=\"BOL Ground Truth\")\n",
    "axs.hist(y_predicted[:,0], bins = n_bins,label=\"BOL predicted\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$k_{\\infty}$\", size=14)\n",
    "plt.ylabel(\"Count\", size=14)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.axvline(1.3815,color='r',linestyle='dashed',linewidth=2)\n",
    "\n",
    "plt.title(\"LWR $k_{\\infty}$, eq leth serialization\")\n",
    "plt.savefig(\"PICS/kinfPredDist_eqleth_opti.png\",bbox_inches =\"tight\",\n",
    "            pad_inches = 1,\n",
    "            transparent = False,\n",
    "            facecolor =\"w\",\n",
    "            edgecolor ='w',\n",
    "            orientation ='landscape')\n",
    "plt.show()\n",
    "# Show plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01676\n",
      "49999\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.logical_and(kinfBOL > 1.3815-0.001,kinfBOL < 1.3815+0.001))/50000)\n",
    "\n",
    "print(len(kinfBOL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5420"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a[:,0]<0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5741/9999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
